{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ky-kk_til_mt_mid_last_one_i_promise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7rPVLHJlne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec022d8c-68dd-471b-8a0e-da19eb19018c"
      },
      "source": [
        "# mount the drive onto here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc3JolYCJnWp"
      },
      "source": [
        "import os\n",
        "# select the source and target language code\n",
        "source_language = \"ky\"\n",
        "target_language = \"kk\" \n",
        "\n",
        "# this is for bilingual\n",
        "experiment_name = \"bilingual_baseline\" \n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = experiment_name\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/experiments/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/experiments/%s-%s-%s\" % (source_language, target_language, experiment_name)\n",
        "os.environ['data_path'] = \"/content/drive/My Drive/TIL Corpus/bitext\"\n",
        "os.environ['dev_path'] = \"/content/drive/My Drive/TIL Corpus/dev_set\"\n",
        "os.environ['test_bible'] = \"/content/drive/My Drive/TIL Corpus/test_set/bible\"\n",
        "#os.environ['test_ted'] = \"/content/drive/My Drive/TIL Corpus/test_set/ted_test\"\n",
        "#os.environ['test_wmt'] = \"/content/drive/My Drive/TIL Corpus/test_set/ted_wmt\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icWcAjWpKZJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2057832b-636d-495b-85e9-a82df0f1e3fd"
      },
      "source": [
        "# check if the drive link ia good\n",
        "!echo \"$gdrive_path\"\n",
        "!echo \"$data_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/experiments/ky-kk-bilingual_baseline\n",
            "/content/drive/My Drive/TIL Corpus/bitext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3fHDrfLK0ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afee2df1-fc42-4370-9cb7-f066c45a3c42"
      },
      "source": [
        "# collect all the bitext data files for a language pair given above\n",
        "!find \"$data_path\" -name \"$src-$tgt.*\" >> data.txt\n",
        "!find \"$data_path\" -name \"$tgt-$src.*\" >> data.txt\n",
        "\n",
        "# this prints how many files it has discovered\n",
        "! wc -l data.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq50OSN5WEw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc8c9af-93ed-421a-b747-815c1466c9ec"
      },
      "source": [
        " !cat data.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TIL Corpus/bitext/ted_talks/ky-kk/ky-kk.kk\n",
            "/content/drive/My Drive/TIL Corpus/bitext/ted_talks/ky-kk/ky-kk.ky\n",
            "/content/drive/My Drive/TIL Corpus/bitext/bible/kk-ky/kk-ky.ky\n",
            "/content/drive/My Drive/TIL Corpus/bitext/bible/kk-ky/kk-ky.kk\n",
            "/content/drive/My Drive/TIL Corpus/bitext/aligned-udhr/kk-ky/kk-ky.ky\n",
            "/content/drive/My Drive/TIL Corpus/bitext/aligned-udhr/kk-ky/kk-ky.kk\n",
            "/content/drive/My Drive/TIL Corpus/bitext/mozilla/kk-ky/kk-ky.ky\n",
            "/content/drive/My Drive/TIL Corpus/bitext/mozilla/kk-ky/kk-ky.kk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4ym_fDL5NH"
      },
      "source": [
        "# read the files and merge them\n",
        "paths = open('data.txt', 'r').readlines()\n",
        "source = []\n",
        "target = []\n",
        "for i in range(0, len(paths), 2):\n",
        "  \n",
        "  if paths[i].strip().endswith(source_language):\n",
        "    source += open(paths[i].strip(), 'r').readlines()\n",
        "    target += open(paths[i+1].strip(), 'r').readlines()\n",
        "  else:\n",
        "    source += open(paths[i+1].strip(), 'r').readlines()\n",
        "    target += open(paths[i].strip(), 'r').readlines()\n",
        "\n",
        "assert len(source) == len(target)\n",
        "\n",
        "!rm data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0456OY390bay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7313e68-c394-47f0-a6bd-15df24c1683c"
      },
      "source": [
        "# load the universal test sets from bible\n",
        "!find \"$test_bible\" -name \"$src-$tgt.*\" >> test_files.txt\n",
        "!find \"$test_bible\" -name \"$tgt-$src.*\" >> test_files.txt\n",
        "! wc -l test_files.txt\n",
        "\n",
        "paths = open('test_files.txt', 'r').readlines()\n",
        "test_source = []\n",
        "test_target = []\n",
        "for i in range(0, len(paths), 2):\n",
        "  \n",
        "  if paths[i].strip().endswith(source_language):\n",
        "    test_source += open(paths[i].strip(), 'r').readlines()\n",
        "    test_target += open(paths[i+1].strip(), 'r').readlines()\n",
        "  else:\n",
        "    test_source += open(paths[i+1].strip(), 'r').readlines()\n",
        "    test_target += open(paths[i].strip(), 'r').readlines()\n",
        "\n",
        "assert len(test_source) == len(test_target)\n",
        "!rm test_files.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 test_files.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJFLPXVLWX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bd1874-1918-4888-d37a-567cb7cc63ba"
      },
      "source": [
        "# load the universal test sets from ted\n",
        "!find \"$test_ted\" -name \"$src-$tgt.*\" >> test_files.txt\n",
        "!find \"$test_ted\" -name \"$tgt-$src.*\" >> test_files.txt\n",
        "! wc -l test_files.txt\n",
        "\n",
        "paths = open('test_files.txt', 'r').readlines()\n",
        "test_source2 = []\n",
        "test_target2 = []\n",
        "for i in range(0, len(paths), 2):\n",
        "  \n",
        "  if paths[i].strip().endswith(source_language):\n",
        "    test_source2 += open(paths[i].strip(), 'r').readlines()\n",
        "    test_target2 += open(paths[i+1].strip(), 'r').readlines()\n",
        "  else:\n",
        "    test_source2 += open(paths[i+1].strip(), 'r').readlines()\n",
        "    test_target2 += open(paths[i].strip(), 'r').readlines()\n",
        "\n",
        "assert len(test_source2) == len(test_target2)\n",
        "!rm test_files.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "find: ‘’: No such file or directory\n",
            "find: ‘’: No such file or directory\n",
            "0 test_files.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4tKsW4Vm7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a53e765-f78c-4041-8b96-f56e0575df79"
      },
      "source": [
        "# load the universal dev sets\n",
        "!find \"$dev_path\" -name \"dev.$src-$tgt.*\" >> dev_files.txt\n",
        "!find \"$dev_path\" -name \"dev.$tgt-$src.*\" >> dev_files.txt\n",
        "! wc -l dev_files.txt\n",
        "\n",
        "paths = open('dev_files.txt', 'r').readlines()\n",
        "dev_source = []\n",
        "dev_target = []\n",
        "for i in range(0, len(paths), 2):\n",
        "  \n",
        "  if paths[i].strip().endswith(source_language):\n",
        "    dev_source += open(paths[i].strip(), 'r').readlines()\n",
        "    dev_target += open(paths[i+1].strip(), 'r').readlines()\n",
        "  else:\n",
        "    dev_source += open(paths[i+1].strip(), 'r').readlines()\n",
        "    dev_target += open(paths[i].strip(), 'r').readlines()\n",
        "\n",
        "assert len(dev_source) == len(dev_target)\n",
        "\n",
        "!rm dev_files.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 dev_files.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h6gFuVBAvRW"
      },
      "source": [
        "# deduplicate the data\n",
        "clean_source = []\n",
        "clean_target = []\n",
        "\n",
        "merged_test = set([(test_source[i], test_target[i]) for i in range(0, len(test_source))])\n",
        "merged_test2 = set([(test_source2[i], test_target2[i]) for i in range(0, len(test_source2))])\n",
        "merged_dev = set([(dev_source[i], dev_target[i]) for i in range(0, len(dev_source))])\n",
        "\n",
        "for s, t in zip(source, target):\n",
        "  # IMPORTANT for uzbek only. Uncomment otherwise!\n",
        "  # _t = t.replace('ѓ', 'gʻ')\n",
        "  # _s = s.replace('ѓ', 'gʻ')\n",
        "\n",
        "  # if the sentences is a single letter (plus a newline), then discard it\n",
        "  if len(s) > 2 and len(t) > 2:\n",
        "    if (s, t) not in merged_test and (s, t) not in merged_test2 and (s, t) not in merged_dev:\n",
        "        clean_source.append(s)\n",
        "        clean_target.append(t)\n",
        "\n",
        "dev_source = []\n",
        "dev_target = []\n",
        "\n",
        "for (s, t) in merged_dev:\n",
        "  if len(s) > 2 and len(t) > 2:\n",
        "    if (s, t) not in merged_test and (s, t) not in merged_test2:\n",
        "        dev_source.append(s)\n",
        "        dev_target.append(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1tH2wi9d7hJ"
      },
      "source": [
        "clean_source += dev_source[:1700]\n",
        "clean_target += dev_target[:1700]\n",
        "\n",
        "dev_source = dev_source[1700:]\n",
        "dev_target = dev_target[1700:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsAVK--VA2vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6af91f-227a-4fa8-dfc0-d794ec7f758b"
      },
      "source": [
        "# check the new length of the data (before deduplication)\n",
        "print(f\"Train set: {len(clean_source)} sentences\")\n",
        "print(f\"Dev set: {len(dev_source)} sentences\")\n",
        "print(f\"Test set (bible): {len(test_source)} sentences\")\n",
        "print(f\"Test set (ted talks): {len(test_source2)} sentences\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set: 6486 sentences\n",
            "Dev set: 505 sentences\n",
            "Test set (bible): 1000 sentences\n",
            "Test set (ted talks): 0 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v5wV2KvpDsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf1caa6-9547-494e-b3ba-7d8b5147d2b6"
      },
      "source": [
        "# load the data into a pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(zip(clean_source, clean_target), columns=['source_sentence', 'target_sentence'])\n",
        "df_dev = pd.DataFrame(zip(dev_source, dev_target), columns=['source_sentence', 'target_sentence'])\n",
        "df_bible = pd.DataFrame(zip(test_source, test_target), columns=['source_sentence', 'target_sentence'])\n",
        "df_ted = pd.DataFrame(zip(test_source2, test_target2), columns=['source_sentence', 'target_sentence'])\n",
        "\n",
        "\n",
        "# drop duplicate translations\n",
        "df = df.drop_duplicates()\n",
        "print(f\"Train set (after dedup): {len(df)} sentences\")\n",
        "\n",
        "\n",
        "# drop conflicting translations\n",
        "df.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "print(f\"Train set (after double dedup): {len(df)} sentences\")\n",
        "\n",
        "\n",
        "df_dev = df_dev.drop_duplicates()\n",
        "df_bible = df_bible.drop_duplicates()\n",
        "df_ted = df_ted.drop_duplicates()\n",
        "\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(df.head(3))\n",
        "\n",
        "print(f\"Train set (after dedup): {len(df_pp)} sentences\")\n",
        "print(f\"Dev set: {len(df_dev)} sentences\")\n",
        "print(f\"Test set (bible): {len(df_bible)} sentences\")\n",
        "print(f\"Test set (ted talks): {len(df_ted)} sentences\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set (after dedup): 6478 sentences\n",
            "Train set (after double dedup): 6432 sentences\n",
            "                                     source_sentence                                    target_sentence\n",
            "0  Чындыгында, азыр аларга убакытымдын көпчүлүгүн...  Әсіресе қазіргі уақытта.Әртүрлі халықтардың ті...\n",
            "1                       Бул кербездик деп ойлошот.\\n                 Себебі оны маңызсыз деп санайды.\\n\n",
            "2  Санта Фе институтунда жана Москвада мыкты линг...  Әсіресе дарынды ресей лингвистері Мәскеудегі С...\n",
            "Train set (after dedup): 6432 sentences\n",
            "Dev set: 505 sentences\n",
            "Test set (bible): 1000 sentences\n",
            "Test set (ted talks): 0 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfvQer44oeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd340e9-f2ac-42db-d7ca-fb6f7cfd6473"
      },
      "source": [
        "# This section does the split between train/dev/test for the parallel corpora then saves them as separate files\n",
        "import csv\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_pp.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "  \n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_bible.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "\n",
        "with open(\"test2.\"+source_language, \"w\") as src_file, open(\"test2.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_ted.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "\n",
        "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*\n",
        "! head test2.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.kk <==\n",
            "Бұл қызметке өзіміз қабілеттіміз деп ойлайтындай еш негізіміз жоқ. Біздегі бар қабілеттілік өзімізден емес, Құдайдан.\n",
            "Жай, бос әңгімелерден аулақ жүр. Бұларды айтатындар Құдайдың адал жолынан одан бетер адасып кетеді.\n",
            "Керісінше, жас шарапты жаңа торсыққа құю керек. (Сонда екеуі де аман қалады.)\n",
            "— Мырза, қызметшім үйде сал болып қатты қиналып жатыр, — деген өтініш жасады.\n",
            "Ол аянда Анани есімді біреудің кіріп келіп, көзін ашу үшін үстіне қолын қойғанын көрді, — деді Иеміз.\n",
            "Иса қолын тигізіп бата берсін деп, бірнеше адам Оған нәрестелерін алып келді. Бірақ мұны көрген шәкірттер оларды жібермеді.\n",
            "Сонымен қатар, халық санын төмендету тенденциясы социалдық ықпалға тікелей байланысты.\n",
            "Бірақ жүзімшілер ұлын көргенде өзара ақылдасып: «Бұл — мұрагер ғой! Кәне, жүріңдер, оны өлтірейік, сонда мұрасы біздікі болады!» — деседі.※\n",
            "Олардың арам ойларын білген Иса:— Екі жүзділер, Мені азғырып сынамақсыңдар, ә? 19 Маған салыққа төлейтін бір күміс теңгені көрсетіңдерші! — деді. Олар Оған бір күміс теңге әкеліп берді. 20 Иса:\n",
            "Сол кезде Дамаскіде Исаның жолын қуушы Анани есімді бір шәкірті тұратын. Иеміз соған аян беріп:— Анани! — деді. Ол:— Тыңдап тұрмын, Ием! — деді.\n",
            "\n",
            "==> train.ky <==\n",
            "Колубуздан бир нерсе келип, муну же тигини өзүбүз кылдык дегендей эч акыбыз жок, себеби бизди ошого жөндөмдүү кылган – Кудай.\n",
            "Кудайды карабай, ыгы жок айтышуудан оолак бол. Анткендер Кудайдан ого бетер четтеп калышат.\n",
            "Ошондуктан жаңы шарапты жаңы чаначтарга куюш керек.\n",
            "Таксыр, кызматчым үйдө ооруп, ордунан тура албай, катуу кыйналып жатат, – деген өтүнүч менен кайрылды.\n",
            "Мен ага Анания аттуу бирөөнүн келип, көзү көрүп калсын үчүн колун койгонун көргөзүп аян бердим.\n",
            "Колун коюп, батасын берсин деп, Ыйсанын алдына балдарды да алып келишти. Шакирттери муну көрүп, алып келгендерди жемелеп, уруша башташты.\n",
            "Бирок узак мөөнөттөгү социалдык таасири - калктын санынын өсүшүн токтотуу.\n",
            "Ижарачылар болсо уулун көргөндө, өз ара акылдашып: «Бул мураскор, жүргүлө, өлтүрөлү, ошондо мурасы бизге калат», – дешет да,\n",
            "Ыйса алардын мындай жаман оюн билип:– Эй, эки жүздүүлөр, эмнеге Мени сынап жатасыңар? 19 Мага салыкка төлөнө турган бир күмүш теңгени көрсөткүлөчү! – деди.\n",
            "Дамаскта Ыйсанын Анания аттуу шакирти жашачу. Эгебиз ага аян берип, ошондо аны:– Анания! – деп атынан чакырды.Ал:– Угуп жатам, Эгем! – деди.\n",
            "==> dev.kk <==\n",
            "Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "— Соңғылар бір сағат қана жұмыс істесе, біз осындай ыстықта күні бойы жұмыс істедік. Дегенмен сіз бәрімізге бірдей ақы төледіңіз! — деп күңкілдейді.\n",
            "«Осы сөздерімді құлақтарыңа құйып алыңдар: көктен келген Билеушіні※ адамдардың қолына ұстап береді!»\n",
            "Бұдан былай осы дүниедегі өміріңнің қалған кезеңін ескі күнәкар болмыстарыңның құмарлықтарына сай емес, Құдайдың еркіне мойынсұнып өткізулерің керек.\n",
            "Иса оған былай деді:— Айтқаныма илан: көктегі Әкеге не осы таудың басында, не Иерусалимде сиынбайтын замандарың※ келе жатыр. 22 Сендер өздерің білмейтін Біреуге табынып келесіңдер, ал біз Кімге ғибадат етіп жүргенімізді білеміз, өйткені Құдай яһудилер арқылы адамзатты құтқармақ.※ 23 Нағыз сиынушылардың Құдайдың Рухына кенеліп, Әкеге шындап мойынсұнып сиынатын уақыты※ болады, ол тіпті жетіп те қалды: Әке Өзіне осылай мойынсұнып сиынушыларды қалайды. 24 Құдайдың Өзі — Рух. Сондықтан Оған сиынатындар Оның Рухына кенеліп, шындықпен сиынуға тиіс. — 25 Сонда әйел:\n",
            "Сурет файлын жүктеңіз\n",
            "Едәуір уақыт өткеннен кейін Пауыл Барнабаға: «Біз Иеміздің хабарын уағыздаған барлық қалалардағы бауырластарға барып, олардың жағдайларын көрейік!» — деді.\n",
            "\n",
            "==> dev.ky <==\n",
            "Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "«Мына бул акыркы келгендер бир эле саат иштешти. А сен болсо эртеден кечке маңдай терибизди төгүп, мээ кайнаткан ысыкта иштеген бизди тигилерге теңеп салдың».\n",
            "Бул сөздөрдү эсиңерге түйүп алгыла: Адам Уулунун кишилердин колуна түшөрүнө аз калды.\n",
            "Эми мындан ары бул жер үстүндөгү калган өмүрдү адамдык арам кумардын эмес, Кудай каалоосунун жетегинде жашап өтүш керек.\n",
            "Айтканыма ишен: асмандагы Атабызга бул тоонун башында жана Иерусалимде гана эмес, бардык жерде табына турган убакыт келет, – деп жооп берди Ыйса. –\n",
            "сүрөт файл жүктөп берүү\n",
            "Бир нече күн өткөндө, Пабыл Барнапка:– Эми кайтып барып, биз мурда Эгебиздин Жакшы Кабарын айткан шаарлардагы боордошторубуздун ал-акыбалы кандай болду экен, ошону көрүп келели, – деди.\n",
            "==> test.kk <==\n",
            "Жексенбі күні※ әлі қараңғы болған кезде мағдалалық Мәриям (жартастан үңгіп жасалған) қабірге келді. Сонда ол қабірдің аузын бекіткен дәу қақпақ тастың орнынан алынып тасталғанын көрді.※\n",
            "Содан Мәриям Шимон Петірге және Исаның жақсы көрген шәкіртіне※ жүгіріп барып, оларға: «Біреулер Мырзамызды қабірден алып кетіпті! Оны қай жерге қойғанын білмейміз!»※ — деді.\n",
            "Петір мен әлгі шәкірт дереу қаладан шығып, қабірге қарай беттеді.\n",
            "Екеуі бірге жүгіре жөнелді, бірақ сол шәкірт Петірден жылдамырақ жүгіріп, үңгірдегі қабірге алдымен жетті.\n",
            "Ол еңкейіп қарап, бәтес орауыштың※ жатқанын көрді, бірақ қабірдің ішіне кірмеді.\n",
            "Сол сәтте оның соңын ала Шимон Петір де келіп, қабірге кірді. Ол да бәтес орауыш пен Исаның беті бүркелген сүлгінің жатқанын көрді. Сүлгі бәтес орауышпен бірге емес, бүктелген күйі бөлек жатыр екен.\n",
            "Сонда бірінші болып келген әлгі шәкірт※ те қабірге кірді. Ол да соны көріп, Исаның тірілгеніне сенді.\n",
            "Сол кезде олар Исаның Киелі жазбаларға сай өлімнен қайта тірілуге тиіс екендігін※ әлі де түсінбейтін.\n",
            "Содан кейін екі шәкірт өздері тұрып жатқан үйге қайта оралды.\n",
            "Ал Мәриям үңгірдің алдында жылап тұрды. Жылап тұрған ол қабірдің ішіне еңкейіп қарағанда,\n",
            "\n",
            "==> test.ky <==\n",
            "Жуманын биринчи күнү таң азанда магдалалык Мариям көргө келип, анын оозун бекиткен таштын ордунан жылдырылып калганын көрдү.\n",
            "Ал Ысман-Петир менен Ыйсанын өзгөчө сүйгөн шакиртине жүгүрүп барып, аларга:– Эгебизди көрдөн алып кетишиптир! Кай жерге коюшканын билбейбиз – деди.\n",
            "Петир менен ошол шакирт дароо шаардан чыгып, көргө бет алышты.\n",
            "Экөө тең чуркаган бойдон жөнөп, бирок алиги шакирт Петирден ылдамыраак жүгүргөндүктөн, көргө биринчи жетти.\n",
            "Анан эңкейип кепиндин жатканын көрдү да, бирок ичине кирбеди.\n",
            "Ошол кезде анын артынан Ысман-Петир да жетип келип, түз эле көргө кирди. Ал да кепин менен Ыйсанын башын орогон сүлгүнүн жатканын көрдү. Бирок сүлгү кепин менен бирге эмес, оролуу боюнча бөлөк жаткан экен.\n",
            "Анан биринчи болуп келген алиги шакирт көргө кирди. Ал көрдү да ишенди.\n",
            "Ал кезде алар Ыйык Жазуулар боюнча Анын өлүмдөн кайра тирилүүгө тийиш экендигин алиге чейин түшүнүшө элек эле.\n",
            "Ушундан кийин шакирттери үйлөрүнө кайтышты.\n",
            "Мариям болсо үңкүрдүн жанында ыйлап турду. Ыйлап жатып, ал эңкейип, көрдүн ичин карады.\n",
            "==> test2.kk <==\n",
            "\n",
            "==> test2.ky <==\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-girJE5mHi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0e88e25-3841-4132-e98e-df6a5d1de230"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! pip install joeynmt ; cd joeynmt;\n",
        "!pip3 install torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 2822 (delta 5), reused 2 (delta 0), pack-reused 2804\u001b[K\n",
            "Receiving objects: 100% (2822/2822), 2.84 MiB | 2.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1928/1928), done.\n",
            "Collecting joeynmt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/28/9e2df8769162c911955015a381ea76f4a7248d0fe2169a4ed8b7b5193cd6/joeynmt-1.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt) (0.16.0)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/13/519c1264a134beab2be4bac8dd3e64948980a5ca7833b31cf0255b21f20a/pylint-2.6.0-py3-none-any.whl (325kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.6/dist-packages (from joeynmt) (2.4.1)\n",
            "Collecting numpy<1.19.0,>=1.14.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 81.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt) (51.3.3)\n",
            "Collecting torchtext==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/81/be2d72b1ea641afc74557574650a5b421134198de9f68f483ab10d515dca/torchtext-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 78.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt) (0.11.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt) (7.0.0)\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Collecting torch==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt) (3.2.2)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/14/47/43fdee732cda080367c7d2a1ee5aee3b31e49baf629029a386df458d2dd3/portalocker-2.2.0-py2.py3-none-any.whl\n",
            "Collecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/a8/5133f51967fb21e46ee50831c3f5dda49e976b7f915408d670b1603d41d6/astroid-2.4.2-py3-none-any.whl (213kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pylint->joeynmt) (0.10.2)\n",
            "Collecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/89/6888f573886e9dc0906ec98f1b15888de20919a142c355d7f57ebd977d36/isort-5.7.0-py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 61.4MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15->joeynmt) (1.32.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.1->joeynmt) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt) (1.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->joeynmt) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->joeynmt) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt) (2.8.1)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/c1/4cc3c0da2374963f59b2f57ef02e048cdc4f609cbc1184b4146d0812e5b5/typed_ast-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 53.3MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=67433 sha256=ca211cb7b9f3d5ceef449ec3d424c4f9dc933b1830428d41ed95014a686b2070\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.1 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: portalocker, sacrebleu, six, typed-ast, lazy-object-proxy, wrapt, astroid, isort, mccabe, pylint, numpy, torch, torchtext, subword-nmt, pyyaml, joeynmt\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.4.2 isort-5.7.0 joeynmt-1.2 lazy-object-proxy-1.4.3 mccabe-0.6.1 numpy-1.18.5 portalocker-2.2.0 pylint-2.6.0 pyyaml-5.4.1 sacrebleu-1.5.0 six-1.12.0 subword-nmt-0.3.7 torch-1.7.1 torchtext-0.8.1 typed-ast-1.4.2 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.18.5)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "Successfully installed torch-1.7.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5i0vjqE7Cnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e148f616-fcf8-44f5-d786-d0dc88f3ad15"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test2.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.$tgt > test2.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p \"$data_path\"\n",
        "! cp train.* \"$data_path\"\n",
        "! cp test.* \"$data_path\"\n",
        "! cp test2.* \"$data_path\"\n",
        "! cp dev.* \"$data_path\"\n",
        "! cp bpe.codes.4000 \"$data_path\"\n",
        "! ls \"$data_path\"\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp test2.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Test language Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.kk\t      test2.bpe.ky  test.bpe.kk  test.ky       train.kk\n",
            "dev.bpe.kk\tdev.ky\t      test2.kk\t    test.bpe.ky  train.bpe.kk  train.ky\n",
            "dev.bpe.ky\ttest2.bpe.kk  test2.ky\t    test.kk\t train.bpe.ky\n",
            "bpe.codes.4000\tdev.kk\t      test2.bpe.ky  test.bpe.kk  test.ky       train.kk\n",
            "dev.bpe.kk\tdev.ky\t      test2.kk\t    test.bpe.ky  train.bpe.kk  train.ky\n",
            "dev.bpe.ky\ttest2.bpe.kk  test2.ky\t    test.kk\t train.bpe.ky\n",
            "BPE Test language Sentences\n",
            "Себебі (@@ е@@ жел@@ де@@ ) Ж@@ үн@@ іс пайғамбардың Н@@ ине@@ в@@ и қалас@@ ының тұрғ@@ ындар@@ ына керемет бел@@ гі болып көрін@@ ген@@ ін@@ дей, көктен келген Билеуші де осы ұрпақ@@ қа керемет бел@@ гі болып көрін@@ етін болады.※\n",
            "Е@@ жел@@ гі О@@ ң@@ түс@@ ті@@ к Ар@@ а@@ би@@ я@@ ның патш@@ айым@@ ы※ қия@@ мет күні осы ұрпақ@@ тың адамдар@@ ымен бірге сот алдында тұрып, олардың қи@@ қар@@ лы@@ ғын әш@@ кереле@@ й@@ ді.※ Себебі патш@@ айым С@@ үле@@ й@@ мен@@ нің дан@@ алық сөздер@@ ін тыңда@@ у үшін жер@@ дің қи@@ ыр шет@@ інен келді. Ал мұн@@ да С@@ үле@@ й@@ мен пат@@ ша@@ дан да м@@ әр@@ те@@ бес@@ і зор Бір@@ еу бар@@ ! (@@ Алайда сендер Маған шынымен құлақ ас@@ пай@@ сыңдар@@ !@@ )\n",
            "Н@@ ине@@ в@@ и@@ дің тұрғ@@ ындар@@ ы да қия@@ мет кезінде осы ұрпақ@@ тың адамдар@@ ымен бірге сот алдында тұрып, олардың қи@@ қар@@ лы@@ ғын әш@@ кереле@@ й@@ ді: н@@ ине@@ в@@ ил@@ ікт@@ ер Ж@@ үн@@ іс уағызда@@ ғанда теріс жол@@ дарынан қайт@@ ты@@ .※ Ал мұн@@ да Ж@@ үн@@ іс пайғамбар@@ дан да м@@ әр@@ те@@ бес@@ і зор Бір@@ еу бар@@ ! (@@ Алайда сендер теріс жол@@ дар@@ ыңнан қай@@ ту@@ дан бас тар@@ тас@@ ыңдар@@ !@@ )@@ »\n",
            "Еш@@ кім ша@@ м жақ@@ қан соң оны жас@@ ыр@@ ын жерге немесе бір ы@@ ды@@ с@@ тың аст@@ ына қой@@ май@@ ды. Қайта, оны ша@@ м қой@@ ғ@@ ыш@@ қа қоя@@ ды, сонда ол үйге кір@@ ген@@ дерге жары@@ ғын түсір@@ еді.※\n",
            "Д@@ ен@@ енің «@@ шам@@ ы» — көз@@ .※ К@@ өз@@ ің кір@@ ші@@ к@@ сіз таза болған@@ да, өмір@@ ің түгел@@ дей нұр@@ ға кен@@ еле@@ ді. Ал егер аш@@ көз болса@@ ң, өмір@@ іңді қар@@ аң@@ ғ@@ ылық бас@@ ады.※\n",
            "Combined BPE Vocab\n",
            "э\n",
            "Й\n",
            "муг@@\n",
            "ц\n",
            "’\n",
            "Ш\n",
            "в\n",
            "М\n",
            "Ң\n",
            "Ыб@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybck4tim8aPz"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"  # change this to data/{name}/test2.bpe so that you can test it on Ted Talks\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 128\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.998] \n",
        "    scheduling: \"plateau\"           \n",
        "    patience: 7                     \n",
        "    learning_rate_factor: 0.5       \n",
        "    learning_rate_warmup: 4000     \n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003          \n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 8\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 3000                     \n",
        "    validation_freq: 500          \n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    fp16: True\n",
        "    max_output_length: 128\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             \n",
        "        embeddings:\n",
        "            embedding_dim: 256   \n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         \n",
        "        ff_size: 1024            \n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             \n",
        "        embeddings:\n",
        "            embedding_dim: 256    \n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         \n",
        "        ff_size: 1024           \n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlPz-83Neg98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9089e6b3-b35d-4837-e60d-1ac9ad3c4f34"
      },
      "source": [
        "# This may take a few minutes to install but it will speed up your training a lot!\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-_9lgyivt\n",
            "Created temporary directory: /tmp/pip-req-tracker-bhnzuovz\n",
            "Created requirements tracker '/tmp/pip-req-tracker-bhnzuovz'\n",
            "Created temporary directory: /tmp/pip-install-s3bscp3v\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-4o4jhj18\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-bhnzuovz'\n",
            "    Running setup.py (path:/tmp/pip-req-build-4o4jhj18/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-4o4jhj18/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-4o4jhj18/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-4o4jhj18 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-bhnzuovz'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-8zi99v3p\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-4o4jhj18/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-4o4jhj18/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-8zi99v3p/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.1+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-4o4jhj18/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:352: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-8zi99v3p/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-4o4jhj18\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-bhnzuovz'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKsFbsiiCAcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c19094-bfba-40ef-90e2-7ce297fd75bf"
      },
      "source": [
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2021-02-01 08:47:28,296 - INFO - joeynmt.training - Epoch 288, Step:     4300, Batch Loss:     1.410141, Tokens per Sec:    18502, Lr: 0.000300\n",
            "2021-02-01 08:47:32,510 - INFO - joeynmt.training - Epoch 288: total training loss 22.55\n",
            "2021-02-01 08:47:32,511 - INFO - joeynmt.training - EPOCH 289\n",
            "2021-02-01 08:47:45,017 - INFO - joeynmt.training - Epoch 289: total training loss 22.46\n",
            "2021-02-01 08:47:45,017 - INFO - joeynmt.training - EPOCH 290\n",
            "2021-02-01 08:47:57,667 - INFO - joeynmt.training - Epoch 290: total training loss 22.35\n",
            "2021-02-01 08:47:57,668 - INFO - joeynmt.training - EPOCH 291\n",
            "2021-02-01 08:48:10,086 - INFO - joeynmt.training - Epoch 291: total training loss 22.40\n",
            "2021-02-01 08:48:10,086 - INFO - joeynmt.training - EPOCH 292\n",
            "2021-02-01 08:48:22,577 - INFO - joeynmt.training - Epoch 292: total training loss 22.36\n",
            "2021-02-01 08:48:22,577 - INFO - joeynmt.training - EPOCH 293\n",
            "2021-02-01 08:48:34,910 - INFO - joeynmt.training - Epoch 293: total training loss 22.36\n",
            "2021-02-01 08:48:34,911 - INFO - joeynmt.training - EPOCH 294\n",
            "2021-02-01 08:48:47,591 - INFO - joeynmt.training - Epoch 294: total training loss 22.11\n",
            "2021-02-01 08:48:47,592 - INFO - joeynmt.training - EPOCH 295\n",
            "2021-02-01 08:48:51,707 - INFO - joeynmt.training - Epoch 295, Step:     4400, Batch Loss:     1.329290, Tokens per Sec:    18141, Lr: 0.000300\n",
            "2021-02-01 08:49:00,000 - INFO - joeynmt.training - Epoch 295: total training loss 22.15\n",
            "2021-02-01 08:49:00,001 - INFO - joeynmt.training - EPOCH 296\n",
            "2021-02-01 08:49:12,294 - INFO - joeynmt.training - Epoch 296: total training loss 22.09\n",
            "2021-02-01 08:49:12,294 - INFO - joeynmt.training - EPOCH 297\n",
            "2021-02-01 08:49:24,535 - INFO - joeynmt.training - Epoch 297: total training loss 21.97\n",
            "2021-02-01 08:49:24,535 - INFO - joeynmt.training - EPOCH 298\n",
            "2021-02-01 08:49:36,937 - INFO - joeynmt.training - Epoch 298: total training loss 22.02\n",
            "2021-02-01 08:49:36,937 - INFO - joeynmt.training - EPOCH 299\n",
            "2021-02-01 08:49:49,498 - INFO - joeynmt.training - Epoch 299: total training loss 21.91\n",
            "2021-02-01 08:49:49,498 - INFO - joeynmt.training - EPOCH 300\n",
            "2021-02-01 08:50:01,998 - INFO - joeynmt.training - Epoch 300: total training loss 21.75\n",
            "2021-02-01 08:50:01,998 - INFO - joeynmt.training - EPOCH 301\n",
            "2021-02-01 08:50:14,403 - INFO - joeynmt.training - Epoch 301, Step:     4500, Batch Loss:     1.414656, Tokens per Sec:    18433, Lr: 0.000300\n",
            "2021-02-01 08:50:42,799 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 08:50:42,800 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 08:50:42,800 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 08:50:42,801 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбасынан гөрі демалыс күні мәжілісханаларында※ барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 08:50:42,801 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 08:50:42,801 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 08:50:42,802 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 08:50:42,802 - INFO - joeynmt.training - \tHypothesis: Жаст түскен түскен кісі Исаны байлаудар деп ойлап қалды. Сол жерге түскен олар Исаны іздеп ойлап жатты.\n",
            "2021-02-01 08:50:42,802 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 08:50:42,803 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 08:50:42,803 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 08:50:42,803 - INFO - joeynmt.training - \tHypothesis: Сол сияқты, әлемдегі Құдайдың Рухы арқылы бас тартқандықтан бас періште келіп, оны көрдім. Тоқтының үстінде тұрған жалған тәңірлерге құлшылық етушілердің аты көрінді. Тәні мен оның тілі көрінісінде н-айуанды көрдім.\n",
            "2021-02-01 08:50:42,804 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 08:50:42,804 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 08:50:42,804 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 08:50:42,805 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте орнына орнайтын жердің барлық жағындағы барлық жағынан жайғасқан нұрдың орнына жайғасты.\n",
            "2021-02-01 08:50:42,805 - INFO - joeynmt.training - Validation result (greedy) at epoch 301, step     4500: bleu:   2.83, loss: 80897.0859, ppl:  56.0213, duration: 28.4018s\n",
            "2021-02-01 08:50:42,810 - INFO - joeynmt.training - Epoch 301: total training loss 21.88\n",
            "2021-02-01 08:50:42,811 - INFO - joeynmt.training - EPOCH 302\n",
            "2021-02-01 08:50:55,341 - INFO - joeynmt.training - Epoch 302: total training loss 21.75\n",
            "2021-02-01 08:50:55,341 - INFO - joeynmt.training - EPOCH 303\n",
            "2021-02-01 08:51:07,707 - INFO - joeynmt.training - Epoch 303: total training loss 21.64\n",
            "2021-02-01 08:51:07,708 - INFO - joeynmt.training - EPOCH 304\n",
            "2021-02-01 08:51:20,322 - INFO - joeynmt.training - Epoch 304: total training loss 21.38\n",
            "2021-02-01 08:51:20,323 - INFO - joeynmt.training - EPOCH 305\n",
            "2021-02-01 08:51:32,737 - INFO - joeynmt.training - Epoch 305: total training loss 21.53\n",
            "2021-02-01 08:51:32,737 - INFO - joeynmt.training - EPOCH 306\n",
            "2021-02-01 08:51:45,187 - INFO - joeynmt.training - Epoch 306: total training loss 21.57\n",
            "2021-02-01 08:51:45,188 - INFO - joeynmt.training - EPOCH 307\n",
            "2021-02-01 08:51:57,413 - INFO - joeynmt.training - Epoch 307: total training loss 21.41\n",
            "2021-02-01 08:51:57,413 - INFO - joeynmt.training - EPOCH 308\n",
            "2021-02-01 08:52:05,613 - INFO - joeynmt.training - Epoch 308, Step:     4600, Batch Loss:     1.346897, Tokens per Sec:    18635, Lr: 0.000300\n",
            "2021-02-01 08:52:09,712 - INFO - joeynmt.training - Epoch 308: total training loss 21.41\n",
            "2021-02-01 08:52:09,712 - INFO - joeynmt.training - EPOCH 309\n",
            "2021-02-01 08:52:21,851 - INFO - joeynmt.training - Epoch 309: total training loss 20.05\n",
            "2021-02-01 08:52:21,852 - INFO - joeynmt.training - EPOCH 310\n",
            "2021-02-01 08:52:34,300 - INFO - joeynmt.training - Epoch 310: total training loss 21.24\n",
            "2021-02-01 08:52:34,301 - INFO - joeynmt.training - EPOCH 311\n",
            "2021-02-01 08:52:46,826 - INFO - joeynmt.training - Epoch 311: total training loss 21.20\n",
            "2021-02-01 08:52:46,826 - INFO - joeynmt.training - EPOCH 312\n",
            "2021-02-01 08:52:59,144 - INFO - joeynmt.training - Epoch 312: total training loss 21.22\n",
            "2021-02-01 08:52:59,144 - INFO - joeynmt.training - EPOCH 313\n",
            "2021-02-01 08:53:11,678 - INFO - joeynmt.training - Epoch 313: total training loss 20.98\n",
            "2021-02-01 08:53:11,678 - INFO - joeynmt.training - EPOCH 314\n",
            "2021-02-01 08:53:23,929 - INFO - joeynmt.training - Epoch 314: total training loss 21.05\n",
            "2021-02-01 08:53:23,930 - INFO - joeynmt.training - EPOCH 315\n",
            "2021-02-01 08:53:28,808 - INFO - joeynmt.training - Epoch 315, Step:     4700, Batch Loss:     1.461801, Tokens per Sec:    18187, Lr: 0.000300\n",
            "2021-02-01 08:53:36,057 - INFO - joeynmt.training - Epoch 315: total training loss 19.63\n",
            "2021-02-01 08:53:36,058 - INFO - joeynmt.training - EPOCH 316\n",
            "2021-02-01 08:53:48,524 - INFO - joeynmt.training - Epoch 316: total training loss 20.94\n",
            "2021-02-01 08:53:48,525 - INFO - joeynmt.training - EPOCH 317\n",
            "2021-02-01 08:54:00,886 - INFO - joeynmt.training - Epoch 317: total training loss 20.90\n",
            "2021-02-01 08:54:00,886 - INFO - joeynmt.training - EPOCH 318\n",
            "2021-02-01 08:54:13,214 - INFO - joeynmt.training - Epoch 318: total training loss 20.75\n",
            "2021-02-01 08:54:13,214 - INFO - joeynmt.training - EPOCH 319\n",
            "2021-02-01 08:54:25,576 - INFO - joeynmt.training - Epoch 319: total training loss 20.81\n",
            "2021-02-01 08:54:25,577 - INFO - joeynmt.training - EPOCH 320\n",
            "2021-02-01 08:54:38,005 - INFO - joeynmt.training - Epoch 320: total training loss 20.74\n",
            "2021-02-01 08:54:38,005 - INFO - joeynmt.training - EPOCH 321\n",
            "2021-02-01 08:54:50,401 - INFO - joeynmt.training - Epoch 321: total training loss 20.70\n",
            "2021-02-01 08:54:50,401 - INFO - joeynmt.training - EPOCH 322\n",
            "2021-02-01 08:54:52,096 - INFO - joeynmt.training - Epoch 322, Step:     4800, Batch Loss:     1.403918, Tokens per Sec:    18912, Lr: 0.000300\n",
            "2021-02-01 08:55:02,717 - INFO - joeynmt.training - Epoch 322: total training loss 20.73\n",
            "2021-02-01 08:55:02,718 - INFO - joeynmt.training - EPOCH 323\n",
            "2021-02-01 08:55:14,888 - INFO - joeynmt.training - Epoch 323: total training loss 19.31\n",
            "2021-02-01 08:55:14,888 - INFO - joeynmt.training - EPOCH 324\n",
            "2021-02-01 08:55:27,181 - INFO - joeynmt.training - Epoch 324: total training loss 20.51\n",
            "2021-02-01 08:55:27,181 - INFO - joeynmt.training - EPOCH 325\n",
            "2021-02-01 08:55:39,687 - INFO - joeynmt.training - Epoch 325: total training loss 20.41\n",
            "2021-02-01 08:55:39,688 - INFO - joeynmt.training - EPOCH 326\n",
            "2021-02-01 08:55:52,469 - INFO - joeynmt.training - Epoch 326: total training loss 20.45\n",
            "2021-02-01 08:55:52,470 - INFO - joeynmt.training - EPOCH 327\n",
            "2021-02-01 08:56:05,095 - INFO - joeynmt.training - Epoch 327: total training loss 20.36\n",
            "2021-02-01 08:56:05,095 - INFO - joeynmt.training - EPOCH 328\n",
            "2021-02-01 08:56:15,905 - INFO - joeynmt.training - Epoch 328, Step:     4900, Batch Loss:     1.469136, Tokens per Sec:    18376, Lr: 0.000300\n",
            "2021-02-01 08:56:17,628 - INFO - joeynmt.training - Epoch 328: total training loss 20.37\n",
            "2021-02-01 08:56:17,628 - INFO - joeynmt.training - EPOCH 329\n",
            "2021-02-01 08:56:30,192 - INFO - joeynmt.training - Epoch 329: total training loss 20.25\n",
            "2021-02-01 08:56:30,192 - INFO - joeynmt.training - EPOCH 330\n",
            "2021-02-01 08:56:42,759 - INFO - joeynmt.training - Epoch 330: total training loss 20.24\n",
            "2021-02-01 08:56:42,759 - INFO - joeynmt.training - EPOCH 331\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 08:56:55,333 - INFO - joeynmt.training - Epoch 331: total training loss 20.14\n",
            "2021-02-01 08:56:55,333 - INFO - joeynmt.training - EPOCH 332\n",
            "2021-02-01 08:57:07,918 - INFO - joeynmt.training - Epoch 332: total training loss 20.13\n",
            "2021-02-01 08:57:07,918 - INFO - joeynmt.training - EPOCH 333\n",
            "2021-02-01 08:57:20,444 - INFO - joeynmt.training - Epoch 333: total training loss 20.03\n",
            "2021-02-01 08:57:20,444 - INFO - joeynmt.training - EPOCH 334\n",
            "2021-02-01 08:57:32,960 - INFO - joeynmt.training - Epoch 334: total training loss 20.05\n",
            "2021-02-01 08:57:32,960 - INFO - joeynmt.training - EPOCH 335\n",
            "2021-02-01 08:57:39,677 - INFO - joeynmt.training - Epoch 335, Step:     5000, Batch Loss:     1.418969, Tokens per Sec:    18303, Lr: 0.000300\n",
            "2021-02-01 08:58:07,785 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбаларды оқуға тиіс; мәжілісханаларында※ мәжілісханаларында болған соң, мәжілісханаларға жіберіп бара жатты.※\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 08:58:07,786 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - \tHypothesis: Жасшылар түскен кісі өлтіргенше оты※ деп үміттенді.\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - \tHypothesis: Сол сияқты, әлемдегі Құдайдың Рухы арқылы бас тартқан осы періште де көрдім. Ол оны көргенде жалған тәңірге қарсы шыққан айуанның бар екенін көрдім. Олар қатты ашуланып, Құдайды қорлайтын тілдермен көрінетін айуанның тілі мен көрдім.\n",
            "2021-02-01 08:58:07,787 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 08:58:07,788 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 08:58:07,788 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 08:58:07,788 - INFO - joeynmt.training - \tHypothesis: Сонымен, Мәсіх Исамен бірге тірілмеген кезде сендер де көкке көтермедіңдер. Осылай Құдайдың оң жағындағы барлық жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 08:58:07,788 - INFO - joeynmt.training - Validation result (greedy) at epoch 335, step     5000: bleu:   3.01, loss: 82059.3203, ppl:  59.3569, duration: 28.1101s\n",
            "2021-02-01 08:58:13,750 - INFO - joeynmt.training - Epoch 335: total training loss 19.96\n",
            "2021-02-01 08:58:13,750 - INFO - joeynmt.training - EPOCH 336\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 08:58:26,342 - INFO - joeynmt.training - Epoch 336: total training loss 19.96\n",
            "2021-02-01 08:58:26,342 - INFO - joeynmt.training - EPOCH 337\n",
            "2021-02-01 08:58:38,917 - INFO - joeynmt.training - Epoch 337: total training loss 19.83\n",
            "2021-02-01 08:58:38,918 - INFO - joeynmt.training - EPOCH 338\n",
            "2021-02-01 08:58:51,628 - INFO - joeynmt.training - Epoch 338: total training loss 19.99\n",
            "2021-02-01 08:58:51,628 - INFO - joeynmt.training - EPOCH 339\n",
            "2021-02-01 08:59:04,210 - INFO - joeynmt.training - Epoch 339: total training loss 19.86\n",
            "2021-02-01 08:59:04,211 - INFO - joeynmt.training - EPOCH 340\n",
            "2021-02-01 08:59:16,661 - INFO - joeynmt.training - Epoch 340: total training loss 19.67\n",
            "2021-02-01 08:59:16,662 - INFO - joeynmt.training - EPOCH 341\n",
            "2021-02-01 08:59:29,249 - INFO - joeynmt.training - Epoch 341: total training loss 19.82\n",
            "2021-02-01 08:59:29,249 - INFO - joeynmt.training - EPOCH 342\n",
            "2021-02-01 08:59:31,751 - INFO - joeynmt.training - Epoch 342, Step:     5100, Batch Loss:     1.370085, Tokens per Sec:    17816, Lr: 0.000300\n",
            "2021-02-01 08:59:41,801 - INFO - joeynmt.training - Epoch 342: total training loss 19.64\n",
            "2021-02-01 08:59:41,802 - INFO - joeynmt.training - EPOCH 343\n",
            "2021-02-01 08:59:54,545 - INFO - joeynmt.training - Epoch 343: total training loss 19.64\n",
            "2021-02-01 08:59:54,545 - INFO - joeynmt.training - EPOCH 344\n",
            "2021-02-01 09:00:06,931 - INFO - joeynmt.training - Epoch 344: total training loss 19.58\n",
            "2021-02-01 09:00:06,931 - INFO - joeynmt.training - EPOCH 345\n",
            "2021-02-01 09:00:19,542 - INFO - joeynmt.training - Epoch 345: total training loss 19.59\n",
            "2021-02-01 09:00:19,543 - INFO - joeynmt.training - EPOCH 346\n",
            "2021-02-01 09:00:32,032 - INFO - joeynmt.training - Epoch 346: total training loss 19.51\n",
            "2021-02-01 09:00:32,032 - INFO - joeynmt.training - EPOCH 347\n",
            "2021-02-01 09:00:44,495 - INFO - joeynmt.training - Epoch 347: total training loss 19.48\n",
            "2021-02-01 09:00:44,495 - INFO - joeynmt.training - EPOCH 348\n",
            "2021-02-01 09:00:55,215 - INFO - joeynmt.training - Epoch 348, Step:     5200, Batch Loss:     1.336259, Tokens per Sec:    18156, Lr: 0.000300\n",
            "2021-02-01 09:00:57,116 - INFO - joeynmt.training - Epoch 348: total training loss 19.46\n",
            "2021-02-01 09:00:57,117 - INFO - joeynmt.training - EPOCH 349\n",
            "2021-02-01 09:01:09,790 - INFO - joeynmt.training - Epoch 349: total training loss 19.28\n",
            "2021-02-01 09:01:09,791 - INFO - joeynmt.training - EPOCH 350\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 09:01:22,363 - INFO - joeynmt.training - Epoch 350: total training loss 19.22\n",
            "2021-02-01 09:01:22,363 - INFO - joeynmt.training - EPOCH 351\n",
            "2021-02-01 09:01:34,910 - INFO - joeynmt.training - Epoch 351: total training loss 19.32\n",
            "2021-02-01 09:01:34,910 - INFO - joeynmt.training - EPOCH 352\n",
            "2021-02-01 09:01:47,609 - INFO - joeynmt.training - Epoch 352: total training loss 19.29\n",
            "2021-02-01 09:01:47,610 - INFO - joeynmt.training - EPOCH 353\n",
            "2021-02-01 09:01:59,998 - INFO - joeynmt.training - Epoch 353: total training loss 19.22\n",
            "2021-02-01 09:01:59,998 - INFO - joeynmt.training - EPOCH 354\n",
            "2021-02-01 09:02:12,543 - INFO - joeynmt.training - Epoch 354: total training loss 19.02\n",
            "2021-02-01 09:02:12,544 - INFO - joeynmt.training - EPOCH 355\n",
            "2021-02-01 09:02:19,012 - INFO - joeynmt.training - Epoch 355, Step:     5300, Batch Loss:     1.274901, Tokens per Sec:    17856, Lr: 0.000300\n",
            "2021-02-01 09:02:24,807 - INFO - joeynmt.training - Epoch 355: total training loss 19.11\n",
            "2021-02-01 09:02:24,807 - INFO - joeynmt.training - EPOCH 356\n",
            "2021-02-01 09:02:37,089 - INFO - joeynmt.training - Epoch 356: total training loss 19.00\n",
            "2021-02-01 09:02:37,089 - INFO - joeynmt.training - EPOCH 357\n",
            "2021-02-01 09:02:49,493 - INFO - joeynmt.training - Epoch 357: total training loss 19.07\n",
            "2021-02-01 09:02:49,493 - INFO - joeynmt.training - EPOCH 358\n",
            "2021-02-01 09:03:01,947 - INFO - joeynmt.training - Epoch 358: total training loss 18.97\n",
            "2021-02-01 09:03:01,948 - INFO - joeynmt.training - EPOCH 359\n",
            "2021-02-01 09:03:14,507 - INFO - joeynmt.training - Epoch 359: total training loss 18.78\n",
            "2021-02-01 09:03:14,507 - INFO - joeynmt.training - EPOCH 360\n",
            "2021-02-01 09:03:27,083 - INFO - joeynmt.training - Epoch 360: total training loss 18.90\n",
            "2021-02-01 09:03:27,083 - INFO - joeynmt.training - EPOCH 361\n",
            "2021-02-01 09:03:39,694 - INFO - joeynmt.training - Epoch 361: total training loss 18.72\n",
            "2021-02-01 09:03:39,694 - INFO - joeynmt.training - EPOCH 362\n",
            "2021-02-01 09:03:42,142 - INFO - joeynmt.training - Epoch 362, Step:     5400, Batch Loss:     1.224607, Tokens per Sec:    18233, Lr: 0.000300\n",
            "2021-02-01 09:03:52,162 - INFO - joeynmt.training - Epoch 362: total training loss 18.77\n",
            "2021-02-01 09:03:52,162 - INFO - joeynmt.training - EPOCH 363\n",
            "2021-02-01 09:04:04,574 - INFO - joeynmt.training - Epoch 363: total training loss 18.79\n",
            "2021-02-01 09:04:04,574 - INFO - joeynmt.training - EPOCH 364\n",
            "2021-02-01 09:04:16,808 - INFO - joeynmt.training - Epoch 364: total training loss 18.76\n",
            "2021-02-01 09:04:16,808 - INFO - joeynmt.training - EPOCH 365\n",
            "2021-02-01 09:04:29,269 - INFO - joeynmt.training - Epoch 365: total training loss 18.63\n",
            "2021-02-01 09:04:29,270 - INFO - joeynmt.training - EPOCH 366\n",
            "2021-02-01 09:04:41,732 - INFO - joeynmt.training - Epoch 366: total training loss 18.73\n",
            "2021-02-01 09:04:41,733 - INFO - joeynmt.training - EPOCH 367\n",
            "2021-02-01 09:04:54,294 - INFO - joeynmt.training - Epoch 367: total training loss 18.63\n",
            "2021-02-01 09:04:54,294 - INFO - joeynmt.training - EPOCH 368\n",
            "2021-02-01 09:05:04,942 - INFO - joeynmt.training - Epoch 368, Step:     5500, Batch Loss:     1.187179, Tokens per Sec:    18396, Lr: 0.000300\n",
            "2021-02-01 09:05:33,416 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:05:33,416 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбасында Дауысқа барып, мәжілісханаларында※ тәлім беретіндер мәжілісханаларға жіберіп келеді.\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - \tHypothesis: Жасшылар Исаны ұстап беруге тура келетіндер деп үміттенді.\n",
            "2021-02-01 09:05:33,417 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tHypothesis: Сол сияқты, әлемді бүкіл адамзаттың ескі күнәкар болмысының құнын өтеді. Тоқтының үстінде отырған Құдай оны көрдім. Олар қатты таңдаған Құдайды мадақтап жүр, жалған тәңірлерге табынған нәкелді. Олар қатты ашуланып, Құдайды мадақтайтын әйелдер мен көрдім.\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:05:33,418 - INFO - joeynmt.training - \tHypothesis: Мәсіх Исамен бірге болыңдар: Мәсіх көкте жер бетінде орналасқан жерді Құдайдың оң жағындағы барлық жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:05:33,419 - INFO - joeynmt.training - Validation result (greedy) at epoch 368, step     5500: bleu:   3.32, loss: 82826.1797, ppl:  61.6659, duration: 28.4757s\n",
            "2021-02-01 09:05:35,256 - INFO - joeynmt.training - Epoch 368: total training loss 18.57\n",
            "2021-02-01 09:05:35,257 - INFO - joeynmt.training - EPOCH 369\n",
            "2021-02-01 09:05:47,915 - INFO - joeynmt.training - Epoch 369: total training loss 18.49\n",
            "2021-02-01 09:05:47,915 - INFO - joeynmt.training - EPOCH 370\n",
            "2021-02-01 09:06:00,334 - INFO - joeynmt.training - Epoch 370: total training loss 18.40\n",
            "2021-02-01 09:06:00,334 - INFO - joeynmt.training - EPOCH 371\n",
            "2021-02-01 09:06:12,847 - INFO - joeynmt.training - Epoch 371: total training loss 18.49\n",
            "2021-02-01 09:06:12,847 - INFO - joeynmt.training - EPOCH 372\n",
            "2021-02-01 09:06:25,267 - INFO - joeynmt.training - Epoch 372: total training loss 18.43\n",
            "2021-02-01 09:06:25,267 - INFO - joeynmt.training - EPOCH 373\n",
            "2021-02-01 09:06:37,581 - INFO - joeynmt.training - Epoch 373: total training loss 18.43\n",
            "2021-02-01 09:06:37,581 - INFO - joeynmt.training - EPOCH 374\n",
            "2021-02-01 09:06:49,913 - INFO - joeynmt.training - Epoch 374: total training loss 18.40\n",
            "2021-02-01 09:06:49,913 - INFO - joeynmt.training - EPOCH 375\n",
            "2021-02-01 09:06:56,459 - INFO - joeynmt.training - Epoch 375, Step:     5600, Batch Loss:     1.191389, Tokens per Sec:    18495, Lr: 0.000300\n",
            "2021-02-01 09:07:02,068 - INFO - joeynmt.training - Epoch 375: total training loss 17.20\n",
            "2021-02-01 09:07:02,068 - INFO - joeynmt.training - EPOCH 376\n",
            "2021-02-01 09:07:14,351 - INFO - joeynmt.training - Epoch 376: total training loss 18.29\n",
            "2021-02-01 09:07:14,351 - INFO - joeynmt.training - EPOCH 377\n",
            "2021-02-01 09:07:26,607 - INFO - joeynmt.training - Epoch 377: total training loss 18.15\n",
            "2021-02-01 09:07:26,608 - INFO - joeynmt.training - EPOCH 378\n",
            "2021-02-01 09:07:38,969 - INFO - joeynmt.training - Epoch 378: total training loss 18.18\n",
            "2021-02-01 09:07:38,969 - INFO - joeynmt.training - EPOCH 379\n",
            "2021-02-01 09:07:51,154 - INFO - joeynmt.training - Epoch 379: total training loss 16.94\n",
            "2021-02-01 09:07:51,154 - INFO - joeynmt.training - EPOCH 380\n",
            "2021-02-01 09:08:03,461 - INFO - joeynmt.training - Epoch 380: total training loss 18.14\n",
            "2021-02-01 09:08:03,462 - INFO - joeynmt.training - EPOCH 381\n",
            "2021-02-01 09:08:15,875 - INFO - joeynmt.training - Epoch 381: total training loss 18.06\n",
            "2021-02-01 09:08:15,875 - INFO - joeynmt.training - EPOCH 382\n",
            "2021-02-01 09:08:19,974 - INFO - joeynmt.training - Epoch 382, Step:     5700, Batch Loss:     1.186143, Tokens per Sec:    18001, Lr: 0.000300\n",
            "2021-02-01 09:08:28,093 - INFO - joeynmt.training - Epoch 382: total training loss 17.96\n",
            "2021-02-01 09:08:28,094 - INFO - joeynmt.training - EPOCH 383\n",
            "2021-02-01 09:08:40,568 - INFO - joeynmt.training - Epoch 383: total training loss 17.88\n",
            "2021-02-01 09:08:40,568 - INFO - joeynmt.training - EPOCH 384\n",
            "2021-02-01 09:08:53,088 - INFO - joeynmt.training - Epoch 384: total training loss 17.93\n",
            "2021-02-01 09:08:53,088 - INFO - joeynmt.training - EPOCH 385\n",
            "2021-02-01 09:09:05,509 - INFO - joeynmt.training - Epoch 385: total training loss 17.94\n",
            "2021-02-01 09:09:05,509 - INFO - joeynmt.training - EPOCH 386\n",
            "2021-02-01 09:09:17,903 - INFO - joeynmt.training - Epoch 386: total training loss 17.93\n",
            "2021-02-01 09:09:17,903 - INFO - joeynmt.training - EPOCH 387\n",
            "2021-02-01 09:09:30,381 - INFO - joeynmt.training - Epoch 387: total training loss 17.70\n",
            "2021-02-01 09:09:30,382 - INFO - joeynmt.training - EPOCH 388\n",
            "2021-02-01 09:09:42,647 - INFO - joeynmt.training - Epoch 388, Step:     5800, Batch Loss:     1.147494, Tokens per Sec:    18290, Lr: 0.000300\n",
            "2021-02-01 09:09:42,844 - INFO - joeynmt.training - Epoch 388: total training loss 17.74\n",
            "2021-02-01 09:09:42,844 - INFO - joeynmt.training - EPOCH 389\n",
            "2021-02-01 09:09:55,356 - INFO - joeynmt.training - Epoch 389: total training loss 17.71\n",
            "2021-02-01 09:09:55,356 - INFO - joeynmt.training - EPOCH 390\n",
            "2021-02-01 09:10:07,445 - INFO - joeynmt.training - Epoch 390: total training loss 16.58\n",
            "2021-02-01 09:10:07,446 - INFO - joeynmt.training - EPOCH 391\n",
            "2021-02-01 09:10:19,495 - INFO - joeynmt.training - Epoch 391: total training loss 16.62\n",
            "2021-02-01 09:10:19,496 - INFO - joeynmt.training - EPOCH 392\n",
            "2021-02-01 09:10:31,688 - INFO - joeynmt.training - Epoch 392: total training loss 17.68\n",
            "2021-02-01 09:10:31,689 - INFO - joeynmt.training - EPOCH 393\n",
            "2021-02-01 09:10:43,932 - INFO - joeynmt.training - Epoch 393: total training loss 17.52\n",
            "2021-02-01 09:10:43,933 - INFO - joeynmt.training - EPOCH 394\n",
            "2021-02-01 09:10:56,397 - INFO - joeynmt.training - Epoch 394: total training loss 17.61\n",
            "2021-02-01 09:10:56,398 - INFO - joeynmt.training - EPOCH 395\n",
            "2021-02-01 09:11:06,194 - INFO - joeynmt.training - Epoch 395, Step:     5900, Batch Loss:     1.277389, Tokens per Sec:    18805, Lr: 0.000300\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:11:08,673 - INFO - joeynmt.training - Epoch 395: total training loss 17.54\n",
            "2021-02-01 09:11:08,674 - INFO - joeynmt.training - EPOCH 396\n",
            "2021-02-01 09:11:20,934 - INFO - joeynmt.training - Epoch 396: total training loss 17.52\n",
            "2021-02-01 09:11:20,934 - INFO - joeynmt.training - EPOCH 397\n",
            "2021-02-01 09:11:33,462 - INFO - joeynmt.training - Epoch 397: total training loss 17.38\n",
            "2021-02-01 09:11:33,463 - INFO - joeynmt.training - EPOCH 398\n",
            "2021-02-01 09:11:45,751 - INFO - joeynmt.training - Epoch 398: total training loss 17.42\n",
            "2021-02-01 09:11:45,751 - INFO - joeynmt.training - EPOCH 399\n",
            "2021-02-01 09:11:58,103 - INFO - joeynmt.training - Epoch 399: total training loss 17.44\n",
            "2021-02-01 09:11:58,104 - INFO - joeynmt.training - EPOCH 400\n",
            "2021-02-01 09:12:10,294 - INFO - joeynmt.training - Epoch 400: total training loss 17.35\n",
            "2021-02-01 09:12:10,294 - INFO - joeynmt.training - EPOCH 401\n",
            "2021-02-01 09:12:22,571 - INFO - joeynmt.training - Epoch 401: total training loss 17.35\n",
            "2021-02-01 09:12:22,571 - INFO - joeynmt.training - EPOCH 402\n",
            "2021-02-01 09:12:28,271 - INFO - joeynmt.training - Epoch 402, Step:     6000, Batch Loss:     1.167460, Tokens per Sec:    18773, Lr: 0.000300\n",
            "2021-02-01 09:12:57,700 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:12:57,700 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:12:57,700 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбасынан гөрі демалыс күні мәжілісханаларында※ барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - \tHypothesis: Жасшылар түскен оқты жерге түсті. Олар Оны байлаудың мәйітін деп үміттенді.\n",
            "2021-02-01 09:12:57,701 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл адамзаттың ескі күнәкар дүниеге келіп, оны бас тартты. Сол туралы Ізгі хабардың үстінде інде інде көрдім. Олар қатты дауыспен Құдайды көрмеген еген әйелдер қатты дауыстап Құдайды мадақтайтын көрдім.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте орнына орналасқан жерді де солайсыңдар. Оның оң жағындағы барлық жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:12:57,702 - INFO - joeynmt.training - Validation result (greedy) at epoch 402, step     6000: bleu:   3.82, loss: 83944.9375, ppl:  65.1964, duration: 29.4313s\n",
            "2021-02-01 09:13:04,214 - INFO - joeynmt.training - Epoch 402: total training loss 17.35\n",
            "2021-02-01 09:13:04,215 - INFO - joeynmt.training - EPOCH 403\n",
            "2021-02-01 09:13:16,681 - INFO - joeynmt.training - Epoch 403: total training loss 17.15\n",
            "2021-02-01 09:13:16,681 - INFO - joeynmt.training - EPOCH 404\n",
            "2021-02-01 09:13:28,921 - INFO - joeynmt.training - Epoch 404: total training loss 17.11\n",
            "2021-02-01 09:13:28,922 - INFO - joeynmt.training - EPOCH 405\n",
            "2021-02-01 09:13:41,186 - INFO - joeynmt.training - Epoch 405: total training loss 17.02\n",
            "2021-02-01 09:13:41,186 - INFO - joeynmt.training - EPOCH 406\n",
            "2021-02-01 09:13:53,485 - INFO - joeynmt.training - Epoch 406: total training loss 16.95\n",
            "2021-02-01 09:13:53,485 - INFO - joeynmt.training - EPOCH 407\n",
            "2021-02-01 09:14:05,863 - INFO - joeynmt.training - Epoch 407: total training loss 16.93\n",
            "2021-02-01 09:14:05,863 - INFO - joeynmt.training - EPOCH 408\n",
            "2021-02-01 09:14:18,328 - INFO - joeynmt.training - Epoch 408: total training loss 16.92\n",
            "2021-02-01 09:14:18,328 - INFO - joeynmt.training - EPOCH 409\n",
            "2021-02-01 09:14:20,001 - INFO - joeynmt.training - Epoch 409, Step:     6100, Batch Loss:     1.154286, Tokens per Sec:    19518, Lr: 0.000210\n",
            "2021-02-01 09:14:30,555 - INFO - joeynmt.training - Epoch 409: total training loss 16.94\n",
            "2021-02-01 09:14:30,556 - INFO - joeynmt.training - EPOCH 410\n",
            "2021-02-01 09:14:42,547 - INFO - joeynmt.training - Epoch 410: total training loss 15.80\n",
            "2021-02-01 09:14:42,547 - INFO - joeynmt.training - EPOCH 411\n",
            "2021-02-01 09:14:54,843 - INFO - joeynmt.training - Epoch 411: total training loss 16.90\n",
            "2021-02-01 09:14:54,843 - INFO - joeynmt.training - EPOCH 412\n",
            "2021-02-01 09:15:07,331 - INFO - joeynmt.training - Epoch 412: total training loss 16.70\n",
            "2021-02-01 09:15:07,332 - INFO - joeynmt.training - EPOCH 413\n",
            "2021-02-01 09:15:19,704 - INFO - joeynmt.training - Epoch 413: total training loss 16.69\n",
            "2021-02-01 09:15:19,705 - INFO - joeynmt.training - EPOCH 414\n",
            "2021-02-01 09:15:31,971 - INFO - joeynmt.training - Epoch 414: total training loss 16.82\n",
            "2021-02-01 09:15:31,972 - INFO - joeynmt.training - EPOCH 415\n",
            "2021-02-01 09:15:42,512 - INFO - joeynmt.training - Epoch 415, Step:     6200, Batch Loss:     1.094502, Tokens per Sec:    18272, Lr: 0.000210\n",
            "2021-02-01 09:15:44,455 - INFO - joeynmt.training - Epoch 415: total training loss 16.73\n",
            "2021-02-01 09:15:44,456 - INFO - joeynmt.training - EPOCH 416\n",
            "2021-02-01 09:15:56,823 - INFO - joeynmt.training - Epoch 416: total training loss 16.67\n",
            "2021-02-01 09:15:56,824 - INFO - joeynmt.training - EPOCH 417\n",
            "2021-02-01 09:16:09,232 - INFO - joeynmt.training - Epoch 417: total training loss 16.65\n",
            "2021-02-01 09:16:09,233 - INFO - joeynmt.training - EPOCH 418\n",
            "2021-02-01 09:16:21,586 - INFO - joeynmt.training - Epoch 418: total training loss 16.54\n",
            "2021-02-01 09:16:21,586 - INFO - joeynmt.training - EPOCH 419\n",
            "2021-02-01 09:16:33,814 - INFO - joeynmt.training - Epoch 419: total training loss 16.68\n",
            "2021-02-01 09:16:33,814 - INFO - joeynmt.training - EPOCH 420\n",
            "2021-02-01 09:16:46,264 - INFO - joeynmt.training - Epoch 420: total training loss 16.52\n",
            "2021-02-01 09:16:46,264 - INFO - joeynmt.training - EPOCH 421\n",
            "2021-02-01 09:16:58,646 - INFO - joeynmt.training - Epoch 421: total training loss 16.56\n",
            "2021-02-01 09:16:58,647 - INFO - joeynmt.training - EPOCH 422\n",
            "2021-02-01 09:17:05,187 - INFO - joeynmt.training - Epoch 422, Step:     6300, Batch Loss:     1.067823, Tokens per Sec:    18297, Lr: 0.000210\n",
            "2021-02-01 09:17:11,027 - INFO - joeynmt.training - Epoch 422: total training loss 16.61\n",
            "2021-02-01 09:17:11,028 - INFO - joeynmt.training - EPOCH 423\n",
            "2021-02-01 09:17:23,477 - INFO - joeynmt.training - Epoch 423: total training loss 16.49\n",
            "2021-02-01 09:17:23,478 - INFO - joeynmt.training - EPOCH 424\n",
            "2021-02-01 09:17:35,729 - INFO - joeynmt.training - Epoch 424: total training loss 16.61\n",
            "2021-02-01 09:17:35,730 - INFO - joeynmt.training - EPOCH 425\n",
            "2021-02-01 09:17:48,265 - INFO - joeynmt.training - Epoch 425: total training loss 16.39\n",
            "2021-02-01 09:17:48,266 - INFO - joeynmt.training - EPOCH 426\n",
            "2021-02-01 09:18:00,746 - INFO - joeynmt.training - Epoch 426: total training loss 16.46\n",
            "2021-02-01 09:18:00,747 - INFO - joeynmt.training - EPOCH 427\n",
            "2021-02-01 09:18:13,133 - INFO - joeynmt.training - Epoch 427: total training loss 16.43\n",
            "2021-02-01 09:18:13,134 - INFO - joeynmt.training - EPOCH 428\n",
            "2021-02-01 09:18:25,425 - INFO - joeynmt.training - Epoch 428: total training loss 16.34\n",
            "2021-02-01 09:18:25,425 - INFO - joeynmt.training - EPOCH 429\n",
            "2021-02-01 09:18:27,857 - INFO - joeynmt.training - Epoch 429, Step:     6400, Batch Loss:     0.991696, Tokens per Sec:    17105, Lr: 0.000210\n",
            "2021-02-01 09:18:37,758 - INFO - joeynmt.training - Epoch 429: total training loss 16.40\n",
            "2021-02-01 09:18:37,759 - INFO - joeynmt.training - EPOCH 430\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 09:18:50,291 - INFO - joeynmt.training - Epoch 430: total training loss 16.28\n",
            "2021-02-01 09:18:50,291 - INFO - joeynmt.training - EPOCH 431\n",
            "2021-02-01 09:19:02,551 - INFO - joeynmt.training - Epoch 431: total training loss 16.32\n",
            "2021-02-01 09:19:02,552 - INFO - joeynmt.training - EPOCH 432\n",
            "2021-02-01 09:19:14,907 - INFO - joeynmt.training - Epoch 432: total training loss 16.29\n",
            "2021-02-01 09:19:14,908 - INFO - joeynmt.training - EPOCH 433\n",
            "2021-02-01 09:19:27,199 - INFO - joeynmt.training - Epoch 433: total training loss 16.33\n",
            "2021-02-01 09:19:27,199 - INFO - joeynmt.training - EPOCH 434\n",
            "2021-02-01 09:19:39,537 - INFO - joeynmt.training - Epoch 434: total training loss 16.29\n",
            "2021-02-01 09:19:39,537 - INFO - joeynmt.training - EPOCH 435\n",
            "2021-02-01 09:19:50,162 - INFO - joeynmt.training - Epoch 435, Step:     6500, Batch Loss:     1.100676, Tokens per Sec:    18593, Lr: 0.000210\n",
            "2021-02-01 09:20:16,714 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:20:16,715 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:20:16,715 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:20:16,715 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбүркеуі үшін мәжілісханаларында※ мәжілісханаларға барып, жұртқа тәлім беріледі.\n",
            "2021-02-01 09:20:16,715 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:20:16,715 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - \tHypothesis: Жасшылар Исаны ұстап беруге тура келді. Оның бұлай деп үміттенді.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл адамзаттың ескі күнәкар адамдарға беріліп, сол адам суға бас тартты. Олар да көріп,※ Құдайды мадақтап жүретін айуанның тілі мен Құдайды көрдім. Оның сөзі мен көрінетін қатты ашуландырды.\n",
            "2021-02-01 09:20:16,716 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:20:16,717 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:20:16,717 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:20:16,717 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетінде орналасқан оң жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:20:16,717 - INFO - joeynmt.training - Validation result (greedy) at epoch 435, step     6500: bleu:   3.65, loss: 84509.0938, ppl:  67.0527, duration: 26.5545s\n",
            "2021-02-01 09:20:18,350 - INFO - joeynmt.training - Epoch 435: total training loss 16.35\n",
            "2021-02-01 09:20:18,351 - INFO - joeynmt.training - EPOCH 436\n",
            "2021-02-01 09:20:30,548 - INFO - joeynmt.training - Epoch 436: total training loss 16.23\n",
            "2021-02-01 09:20:30,549 - INFO - joeynmt.training - EPOCH 437\n",
            "2021-02-01 09:20:42,850 - INFO - joeynmt.training - Epoch 437: total training loss 16.19\n",
            "2021-02-01 09:20:42,851 - INFO - joeynmt.training - EPOCH 438\n",
            "2021-02-01 09:20:55,129 - INFO - joeynmt.training - Epoch 438: total training loss 16.17\n",
            "2021-02-01 09:20:55,130 - INFO - joeynmt.training - EPOCH 439\n",
            "2021-02-01 09:21:07,468 - INFO - joeynmt.training - Epoch 439: total training loss 16.22\n",
            "2021-02-01 09:21:07,469 - INFO - joeynmt.training - EPOCH 440\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:21:20,012 - INFO - joeynmt.training - Epoch 440: total training loss 16.04\n",
            "2021-02-01 09:21:20,013 - INFO - joeynmt.training - EPOCH 441\n",
            "2021-02-01 09:21:32,278 - INFO - joeynmt.training - Epoch 441: total training loss 16.18\n",
            "2021-02-01 09:21:32,278 - INFO - joeynmt.training - EPOCH 442\n",
            "2021-02-01 09:21:38,896 - INFO - joeynmt.training - Epoch 442, Step:     6600, Batch Loss:     1.140371, Tokens per Sec:    18971, Lr: 0.000210\n",
            "2021-02-01 09:21:44,525 - INFO - joeynmt.training - Epoch 442: total training loss 15.10\n",
            "2021-02-01 09:21:44,525 - INFO - joeynmt.training - EPOCH 443\n",
            "2021-02-01 09:21:57,020 - INFO - joeynmt.training - Epoch 443: total training loss 16.02\n",
            "2021-02-01 09:21:57,021 - INFO - joeynmt.training - EPOCH 444\n",
            "2021-02-01 09:22:09,572 - INFO - joeynmt.training - Epoch 444: total training loss 16.07\n",
            "2021-02-01 09:22:09,573 - INFO - joeynmt.training - EPOCH 445\n",
            "2021-02-01 09:22:21,735 - INFO - joeynmt.training - Epoch 445: total training loss 15.04\n",
            "2021-02-01 09:22:21,735 - INFO - joeynmt.training - EPOCH 446\n",
            "2021-02-01 09:22:34,212 - INFO - joeynmt.training - Epoch 446: total training loss 16.00\n",
            "2021-02-01 09:22:34,212 - INFO - joeynmt.training - EPOCH 447\n",
            "2021-02-01 09:22:46,629 - INFO - joeynmt.training - Epoch 447: total training loss 15.99\n",
            "2021-02-01 09:22:46,629 - INFO - joeynmt.training - EPOCH 448\n",
            "2021-02-01 09:22:59,150 - INFO - joeynmt.training - Epoch 448: total training loss 15.98\n",
            "2021-02-01 09:22:59,151 - INFO - joeynmt.training - EPOCH 449\n",
            "2021-02-01 09:23:03,178 - INFO - joeynmt.training - Epoch 449, Step:     6700, Batch Loss:     1.059823, Tokens per Sec:    17824, Lr: 0.000210\n",
            "2021-02-01 09:23:11,849 - INFO - joeynmt.training - Epoch 449: total training loss 15.89\n",
            "2021-02-01 09:23:11,849 - INFO - joeynmt.training - EPOCH 450\n",
            "2021-02-01 09:23:24,261 - INFO - joeynmt.training - Epoch 450: total training loss 15.94\n",
            "2021-02-01 09:23:24,262 - INFO - joeynmt.training - EPOCH 451\n",
            "2021-02-01 09:23:36,618 - INFO - joeynmt.training - Epoch 451: total training loss 15.95\n",
            "2021-02-01 09:23:36,618 - INFO - joeynmt.training - EPOCH 452\n",
            "2021-02-01 09:23:49,046 - INFO - joeynmt.training - Epoch 452: total training loss 15.77\n",
            "2021-02-01 09:23:49,047 - INFO - joeynmt.training - EPOCH 453\n",
            "2021-02-01 09:24:01,460 - INFO - joeynmt.training - Epoch 453: total training loss 15.88\n",
            "2021-02-01 09:24:01,460 - INFO - joeynmt.training - EPOCH 454\n",
            "2021-02-01 09:24:13,845 - INFO - joeynmt.training - Epoch 454: total training loss 15.87\n",
            "2021-02-01 09:24:13,846 - INFO - joeynmt.training - EPOCH 455\n",
            "2021-02-01 09:24:26,066 - INFO - joeynmt.training - Epoch 455, Step:     6800, Batch Loss:     1.025620, Tokens per Sec:    18641, Lr: 0.000210\n",
            "2021-02-01 09:24:26,158 - INFO - joeynmt.training - Epoch 455: total training loss 15.88\n",
            "2021-02-01 09:24:26,159 - INFO - joeynmt.training - EPOCH 456\n",
            "2021-02-01 09:24:38,465 - INFO - joeynmt.training - Epoch 456: total training loss 15.78\n",
            "2021-02-01 09:24:38,466 - INFO - joeynmt.training - EPOCH 457\n",
            "2021-02-01 09:24:50,905 - INFO - joeynmt.training - Epoch 457: total training loss 15.77\n",
            "2021-02-01 09:24:50,906 - INFO - joeynmt.training - EPOCH 458\n",
            "2021-02-01 09:25:03,534 - INFO - joeynmt.training - Epoch 458: total training loss 15.77\n",
            "2021-02-01 09:25:03,535 - INFO - joeynmt.training - EPOCH 459\n",
            "2021-02-01 09:25:15,711 - INFO - joeynmt.training - Epoch 459: total training loss 15.76\n",
            "2021-02-01 09:25:15,711 - INFO - joeynmt.training - EPOCH 460\n",
            "2021-02-01 09:25:27,967 - INFO - joeynmt.training - Epoch 460: total training loss 15.76\n",
            "2021-02-01 09:25:27,967 - INFO - joeynmt.training - EPOCH 461\n",
            "2021-02-01 09:25:40,156 - INFO - joeynmt.training - Epoch 461: total training loss 15.71\n",
            "2021-02-01 09:25:40,157 - INFO - joeynmt.training - EPOCH 462\n",
            "2021-02-01 09:25:48,347 - INFO - joeynmt.training - Epoch 462, Step:     6900, Batch Loss:     1.058799, Tokens per Sec:    18606, Lr: 0.000210\n",
            "2021-02-01 09:25:52,512 - INFO - joeynmt.training - Epoch 462: total training loss 15.60\n",
            "2021-02-01 09:25:52,512 - INFO - joeynmt.training - EPOCH 463\n",
            "2021-02-01 09:26:04,853 - INFO - joeynmt.training - Epoch 463: total training loss 15.70\n",
            "2021-02-01 09:26:04,853 - INFO - joeynmt.training - EPOCH 464\n",
            "2021-02-01 09:26:17,149 - INFO - joeynmt.training - Epoch 464: total training loss 15.64\n",
            "2021-02-01 09:26:17,150 - INFO - joeynmt.training - EPOCH 465\n",
            "2021-02-01 09:26:29,741 - INFO - joeynmt.training - Epoch 465: total training loss 15.54\n",
            "2021-02-01 09:26:29,742 - INFO - joeynmt.training - EPOCH 466\n",
            "2021-02-01 09:26:42,156 - INFO - joeynmt.training - Epoch 466: total training loss 15.58\n",
            "2021-02-01 09:26:42,156 - INFO - joeynmt.training - EPOCH 467\n",
            "2021-02-01 09:26:54,620 - INFO - joeynmt.training - Epoch 467: total training loss 15.66\n",
            "2021-02-01 09:26:54,620 - INFO - joeynmt.training - EPOCH 468\n",
            "2021-02-01 09:27:07,306 - INFO - joeynmt.training - Epoch 468: total training loss 15.50\n",
            "2021-02-01 09:27:07,307 - INFO - joeynmt.training - EPOCH 469\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:27:11,443 - INFO - joeynmt.training - Epoch 469, Step:     7000, Batch Loss:     1.021169, Tokens per Sec:    18211, Lr: 0.000210\n",
            "2021-02-01 09:27:41,018 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін ертіп алып, мәжілісханаларға тәлім беру үшін мәжілісханаларында※ мәжілісханаларға жіберіп келе жатыр.※\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - \tHypothesis: Жігіт түскен кісі Оны байлатып, Исаны өлтіргендеп үміттті.\n",
            "2021-02-01 09:27:41,019 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, сол адам сылуға да берілді. Тоқтының үстінде отырған Құдайды мадақтайтын әйелдер Құдайды мадақтап жүрдім. Оның мәнінде маған, оның тілі мен қатты ашуланып,※ тілі мен айуанды көрдім.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге қайта тірілмеген кезде де көкке көтерілдіңдер. Енді Құдай оң жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:27:41,020 - INFO - joeynmt.training - Validation result (greedy) at epoch 469, step     7000: bleu:   4.02, loss: 85052.3594, ppl:  68.8902, duration: 29.5767s\n",
            "2021-02-01 09:27:49,485 - INFO - joeynmt.training - Epoch 469: total training loss 15.50\n",
            "2021-02-01 09:27:49,485 - INFO - joeynmt.training - EPOCH 470\n",
            "2021-02-01 09:28:01,981 - INFO - joeynmt.training - Epoch 470: total training loss 15.42\n",
            "2021-02-01 09:28:01,982 - INFO - joeynmt.training - EPOCH 471\n",
            "2021-02-01 09:28:14,439 - INFO - joeynmt.training - Epoch 471: total training loss 15.52\n",
            "2021-02-01 09:28:14,440 - INFO - joeynmt.training - EPOCH 472\n",
            "2021-02-01 09:28:26,947 - INFO - joeynmt.training - Epoch 472: total training loss 15.37\n",
            "2021-02-01 09:28:26,947 - INFO - joeynmt.training - EPOCH 473\n",
            "2021-02-01 09:28:39,447 - INFO - joeynmt.training - Epoch 473: total training loss 15.36\n",
            "2021-02-01 09:28:39,447 - INFO - joeynmt.training - EPOCH 474\n",
            "2021-02-01 09:28:51,873 - INFO - joeynmt.training - Epoch 474: total training loss 15.44\n",
            "2021-02-01 09:28:51,874 - INFO - joeynmt.training - EPOCH 475\n",
            "2021-02-01 09:29:04,072 - INFO - joeynmt.training - Epoch 475: total training loss 14.42\n",
            "2021-02-01 09:29:04,072 - INFO - joeynmt.training - EPOCH 476\n",
            "2021-02-01 09:29:04,931 - INFO - joeynmt.training - Epoch 476, Step:     7100, Batch Loss:     1.102948, Tokens per Sec:    19392, Lr: 0.000210\n",
            "2021-02-01 09:29:16,643 - INFO - joeynmt.training - Epoch 476: total training loss 15.33\n",
            "2021-02-01 09:29:16,643 - INFO - joeynmt.training - EPOCH 477\n",
            "2021-02-01 09:29:28,945 - INFO - joeynmt.training - Epoch 477: total training loss 15.40\n",
            "2021-02-01 09:29:28,946 - INFO - joeynmt.training - EPOCH 478\n",
            "2021-02-01 09:29:41,321 - INFO - joeynmt.training - Epoch 478: total training loss 15.37\n",
            "2021-02-01 09:29:41,322 - INFO - joeynmt.training - EPOCH 479\n",
            "2021-02-01 09:29:53,611 - INFO - joeynmt.training - Epoch 479: total training loss 15.35\n",
            "2021-02-01 09:29:53,611 - INFO - joeynmt.training - EPOCH 480\n",
            "2021-02-01 09:30:05,759 - INFO - joeynmt.training - Epoch 480: total training loss 14.29\n",
            "2021-02-01 09:30:05,759 - INFO - joeynmt.training - EPOCH 481\n",
            "2021-02-01 09:30:18,291 - INFO - joeynmt.training - Epoch 481: total training loss 15.25\n",
            "2021-02-01 09:30:18,292 - INFO - joeynmt.training - EPOCH 482\n",
            "2021-02-01 09:30:28,100 - INFO - joeynmt.training - Epoch 482, Step:     7200, Batch Loss:     1.045407, Tokens per Sec:    18187, Lr: 0.000210\n",
            "2021-02-01 09:30:30,752 - INFO - joeynmt.training - Epoch 482: total training loss 15.24\n",
            "2021-02-01 09:30:30,752 - INFO - joeynmt.training - EPOCH 483\n",
            "2021-02-01 09:30:43,156 - INFO - joeynmt.training - Epoch 483: total training loss 15.26\n",
            "2021-02-01 09:30:43,156 - INFO - joeynmt.training - EPOCH 484\n",
            "2021-02-01 09:30:55,662 - INFO - joeynmt.training - Epoch 484: total training loss 15.16\n",
            "2021-02-01 09:30:55,663 - INFO - joeynmt.training - EPOCH 485\n",
            "2021-02-01 09:31:08,020 - INFO - joeynmt.training - Epoch 485: total training loss 15.19\n",
            "2021-02-01 09:31:08,020 - INFO - joeynmt.training - EPOCH 486\n",
            "2021-02-01 09:31:20,300 - INFO - joeynmt.training - Epoch 486: total training loss 15.19\n",
            "2021-02-01 09:31:20,301 - INFO - joeynmt.training - EPOCH 487\n",
            "2021-02-01 09:31:32,627 - INFO - joeynmt.training - Epoch 487: total training loss 15.23\n",
            "2021-02-01 09:31:32,627 - INFO - joeynmt.training - EPOCH 488\n",
            "2021-02-01 09:31:45,086 - INFO - joeynmt.training - Epoch 488: total training loss 15.16\n",
            "2021-02-01 09:31:45,087 - INFO - joeynmt.training - EPOCH 489\n",
            "2021-02-01 09:31:50,794 - INFO - joeynmt.training - Epoch 489, Step:     7300, Batch Loss:     0.873252, Tokens per Sec:    18329, Lr: 0.000210\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:31:57,506 - INFO - joeynmt.training - Epoch 489: total training loss 15.03\n",
            "2021-02-01 09:31:57,506 - INFO - joeynmt.training - EPOCH 490\n",
            "2021-02-01 09:32:09,972 - INFO - joeynmt.training - Epoch 490: total training loss 15.10\n",
            "2021-02-01 09:32:09,972 - INFO - joeynmt.training - EPOCH 491\n",
            "2021-02-01 09:32:22,344 - INFO - joeynmt.training - Epoch 491: total training loss 15.08\n",
            "2021-02-01 09:32:22,344 - INFO - joeynmt.training - EPOCH 492\n",
            "2021-02-01 09:32:34,740 - INFO - joeynmt.training - Epoch 492: total training loss 15.11\n",
            "2021-02-01 09:32:34,741 - INFO - joeynmt.training - EPOCH 493\n",
            "2021-02-01 09:32:47,223 - INFO - joeynmt.training - Epoch 493: total training loss 15.01\n",
            "2021-02-01 09:32:47,223 - INFO - joeynmt.training - EPOCH 494\n",
            "2021-02-01 09:32:59,725 - INFO - joeynmt.training - Epoch 494: total training loss 15.01\n",
            "2021-02-01 09:32:59,726 - INFO - joeynmt.training - EPOCH 495\n",
            "2021-02-01 09:33:12,318 - INFO - joeynmt.training - Epoch 495: total training loss 15.05\n",
            "2021-02-01 09:33:12,319 - INFO - joeynmt.training - EPOCH 496\n",
            "2021-02-01 09:33:13,992 - INFO - joeynmt.training - Epoch 496, Step:     7400, Batch Loss:     0.927023, Tokens per Sec:    17459, Lr: 0.000210\n",
            "2021-02-01 09:33:24,603 - INFO - joeynmt.training - Epoch 496: total training loss 15.07\n",
            "2021-02-01 09:33:24,603 - INFO - joeynmt.training - EPOCH 497\n",
            "2021-02-01 09:33:37,157 - INFO - joeynmt.training - Epoch 497: total training loss 14.87\n",
            "2021-02-01 09:33:37,157 - INFO - joeynmt.training - EPOCH 498\n",
            "2021-02-01 09:33:49,563 - INFO - joeynmt.training - Epoch 498: total training loss 14.99\n",
            "2021-02-01 09:33:49,563 - INFO - joeynmt.training - EPOCH 499\n",
            "2021-02-01 09:34:01,846 - INFO - joeynmt.training - Epoch 499: total training loss 15.01\n",
            "2021-02-01 09:34:01,847 - INFO - joeynmt.training - EPOCH 500\n",
            "2021-02-01 09:34:14,216 - INFO - joeynmt.training - Epoch 500: total training loss 14.89\n",
            "2021-02-01 09:34:14,217 - INFO - joeynmt.training - EPOCH 501\n",
            "2021-02-01 09:34:26,682 - INFO - joeynmt.training - Epoch 501: total training loss 14.87\n",
            "2021-02-01 09:34:26,683 - INFO - joeynmt.training - EPOCH 502\n",
            "2021-02-01 09:34:36,392 - INFO - joeynmt.training - Epoch 502, Step:     7500, Batch Loss:     0.976141, Tokens per Sec:    17993, Lr: 0.000210\n",
            "2021-02-01 09:35:04,642 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбасында Діни демалыс күні мәжілісханаларында※ барып, мәжілісханаларға айналдыра бастады.\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:35:04,643 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - \tHypothesis: Жестдері түскен ұңғиық жеп тойды. Бұлар Исаны өлтіру үшін деп ойлап алды.\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл әлемді ескі күнәкар адамдарға ғана беріліп, ондағы мақсаттың үстінде бас тарту бар екенін көрдім. Ол қатты ашулана тұрған айуанның үйінде мас деп тапты; нештеңе көрдім.\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:35:04,644 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:35:04,645 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:35:04,645 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетінде орналасқан жердің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 09:35:04,645 - INFO - joeynmt.training - Validation result (greedy) at epoch 502, step     7500: bleu:   3.94, loss: 85362.1484, ppl:  69.9604, duration: 28.2527s\n",
            "2021-02-01 09:35:07,462 - INFO - joeynmt.training - Epoch 502: total training loss 14.87\n",
            "2021-02-01 09:35:07,462 - INFO - joeynmt.training - EPOCH 503\n",
            "2021-02-01 09:35:20,183 - INFO - joeynmt.training - Epoch 503: total training loss 14.82\n",
            "2021-02-01 09:35:20,183 - INFO - joeynmt.training - EPOCH 504\n",
            "2021-02-01 09:35:32,639 - INFO - joeynmt.training - Epoch 504: total training loss 14.84\n",
            "2021-02-01 09:35:32,640 - INFO - joeynmt.training - EPOCH 505\n",
            "2021-02-01 09:35:45,270 - INFO - joeynmt.training - Epoch 505: total training loss 14.85\n",
            "2021-02-01 09:35:45,270 - INFO - joeynmt.training - EPOCH 506\n",
            "2021-02-01 09:35:57,782 - INFO - joeynmt.training - Epoch 506: total training loss 14.82\n",
            "2021-02-01 09:35:57,782 - INFO - joeynmt.training - EPOCH 507\n",
            "2021-02-01 09:36:09,903 - INFO - joeynmt.training - Epoch 507: total training loss 13.82\n",
            "2021-02-01 09:36:09,903 - INFO - joeynmt.training - EPOCH 508\n",
            "2021-02-01 09:36:22,137 - INFO - joeynmt.training - Epoch 508: total training loss 14.76\n",
            "2021-02-01 09:36:22,138 - INFO - joeynmt.training - EPOCH 509\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:36:28,569 - INFO - joeynmt.training - Epoch 509, Step:     7600, Batch Loss:     0.857502, Tokens per Sec:    18823, Lr: 0.000210\n",
            "2021-02-01 09:36:34,467 - INFO - joeynmt.training - Epoch 509: total training loss 14.69\n",
            "2021-02-01 09:36:34,467 - INFO - joeynmt.training - EPOCH 510\n",
            "2021-02-01 09:36:46,940 - INFO - joeynmt.training - Epoch 510: total training loss 14.78\n",
            "2021-02-01 09:36:46,940 - INFO - joeynmt.training - EPOCH 511\n",
            "2021-02-01 09:36:59,615 - INFO - joeynmt.training - Epoch 511: total training loss 14.77\n",
            "2021-02-01 09:36:59,616 - INFO - joeynmt.training - EPOCH 512\n",
            "2021-02-01 09:37:12,163 - INFO - joeynmt.training - Epoch 512: total training loss 14.77\n",
            "2021-02-01 09:37:12,163 - INFO - joeynmt.training - EPOCH 513\n",
            "2021-02-01 09:37:24,737 - INFO - joeynmt.training - Epoch 513: total training loss 14.64\n",
            "2021-02-01 09:37:24,737 - INFO - joeynmt.training - EPOCH 514\n",
            "2021-02-01 09:37:37,121 - INFO - joeynmt.training - Epoch 514: total training loss 14.70\n",
            "2021-02-01 09:37:37,122 - INFO - joeynmt.training - EPOCH 515\n",
            "2021-02-01 09:37:49,452 - INFO - joeynmt.training - Epoch 515: total training loss 14.71\n",
            "2021-02-01 09:37:49,452 - INFO - joeynmt.training - EPOCH 516\n",
            "2021-02-01 09:37:51,901 - INFO - joeynmt.training - Epoch 516, Step:     7700, Batch Loss:     0.976060, Tokens per Sec:    18324, Lr: 0.000210\n",
            "2021-02-01 09:38:01,793 - INFO - joeynmt.training - Epoch 516: total training loss 14.71\n",
            "2021-02-01 09:38:01,793 - INFO - joeynmt.training - EPOCH 517\n",
            "2021-02-01 09:38:14,247 - INFO - joeynmt.training - Epoch 517: total training loss 14.69\n",
            "2021-02-01 09:38:14,248 - INFO - joeynmt.training - EPOCH 518\n",
            "2021-02-01 09:38:26,769 - INFO - joeynmt.training - Epoch 518: total training loss 14.60\n",
            "2021-02-01 09:38:26,769 - INFO - joeynmt.training - EPOCH 519\n",
            "2021-02-01 09:38:39,140 - INFO - joeynmt.training - Epoch 519: total training loss 14.52\n",
            "2021-02-01 09:38:39,141 - INFO - joeynmt.training - EPOCH 520\n",
            "2021-02-01 09:38:51,507 - INFO - joeynmt.training - Epoch 520: total training loss 14.61\n",
            "2021-02-01 09:38:51,507 - INFO - joeynmt.training - EPOCH 521\n",
            "2021-02-01 09:39:03,724 - INFO - joeynmt.training - Epoch 521: total training loss 13.67\n",
            "2021-02-01 09:39:03,724 - INFO - joeynmt.training - EPOCH 522\n",
            "2021-02-01 09:39:15,179 - INFO - joeynmt.training - Epoch 522, Step:     7800, Batch Loss:     1.009152, Tokens per Sec:    18347, Lr: 0.000210\n",
            "2021-02-01 09:39:16,181 - INFO - joeynmt.training - Epoch 522: total training loss 14.63\n",
            "2021-02-01 09:39:16,181 - INFO - joeynmt.training - EPOCH 523\n",
            "2021-02-01 09:39:28,541 - INFO - joeynmt.training - Epoch 523: total training loss 14.59\n",
            "2021-02-01 09:39:28,541 - INFO - joeynmt.training - EPOCH 524\n",
            "2021-02-01 09:39:41,181 - INFO - joeynmt.training - Epoch 524: total training loss 14.48\n",
            "2021-02-01 09:39:41,182 - INFO - joeynmt.training - EPOCH 525\n",
            "2021-02-01 09:39:53,761 - INFO - joeynmt.training - Epoch 525: total training loss 14.48\n",
            "2021-02-01 09:39:53,762 - INFO - joeynmt.training - EPOCH 526\n",
            "2021-02-01 09:40:06,065 - INFO - joeynmt.training - Epoch 526: total training loss 14.52\n",
            "2021-02-01 09:40:06,066 - INFO - joeynmt.training - EPOCH 527\n",
            "2021-02-01 09:40:18,517 - INFO - joeynmt.training - Epoch 527: total training loss 14.43\n",
            "2021-02-01 09:40:18,518 - INFO - joeynmt.training - EPOCH 528\n",
            "2021-02-01 09:40:30,911 - INFO - joeynmt.training - Epoch 528: total training loss 14.51\n",
            "2021-02-01 09:40:30,911 - INFO - joeynmt.training - EPOCH 529\n",
            "2021-02-01 09:40:38,327 - INFO - joeynmt.training - Epoch 529, Step:     7900, Batch Loss:     0.916926, Tokens per Sec:    18662, Lr: 0.000210\n",
            "2021-02-01 09:40:43,175 - INFO - joeynmt.training - Epoch 529: total training loss 13.58\n",
            "2021-02-01 09:40:43,176 - INFO - joeynmt.training - EPOCH 530\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:40:55,562 - INFO - joeynmt.training - Epoch 530: total training loss 14.35\n",
            "2021-02-01 09:40:55,563 - INFO - joeynmt.training - EPOCH 531\n",
            "2021-02-01 09:41:07,981 - INFO - joeynmt.training - Epoch 531: total training loss 14.47\n",
            "2021-02-01 09:41:07,981 - INFO - joeynmt.training - EPOCH 532\n",
            "2021-02-01 09:41:20,455 - INFO - joeynmt.training - Epoch 532: total training loss 14.40\n",
            "2021-02-01 09:41:20,456 - INFO - joeynmt.training - EPOCH 533\n",
            "2021-02-01 09:41:32,903 - INFO - joeynmt.training - Epoch 533: total training loss 14.37\n",
            "2021-02-01 09:41:32,904 - INFO - joeynmt.training - EPOCH 534\n",
            "2021-02-01 09:41:45,577 - INFO - joeynmt.training - Epoch 534: total training loss 14.20\n",
            "2021-02-01 09:41:45,577 - INFO - joeynmt.training - EPOCH 535\n",
            "2021-02-01 09:41:58,045 - INFO - joeynmt.training - Epoch 535: total training loss 14.29\n",
            "2021-02-01 09:41:58,046 - INFO - joeynmt.training - EPOCH 536\n",
            "2021-02-01 09:42:02,167 - INFO - joeynmt.training - Epoch 536, Step:     8000, Batch Loss:     0.958816, Tokens per Sec:    19261, Lr: 0.000210\n",
            "2021-02-01 09:42:31,554 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:42:31,554 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:42:31,554 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың жазбүрей демалыс күні мәжілісханаларында※ жіберіп отырғанда, мәжілісханаларға айналдымда болды.\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - \tHypothesis: Сол кісі дері түскен қылмыскерді бұғаға қарап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 09:42:31,555 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tHypothesis: Ал әлемнің ескі күнәкар адамдарға Құдай Рухын қабылдаған Ізгі хабардың үстінде бас тарту үшін бөлігі бар екенін көрдім. Ол оны Құдайды мадақтап жүрген мына тәрізді сөздер айтты: «Тенды мас деп саналдымның мәнінде тұратын. Олар айуанды көрмей көрме!\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:42:31,556 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетінде орналасқан оң жағындағы барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:42:31,557 - INFO - joeynmt.training - Validation result (greedy) at epoch 536, step     8000: bleu:   4.32, loss: 85573.7266, ppl:  70.7009, duration: 29.3890s\n",
            "2021-02-01 09:42:39,842 - INFO - joeynmt.training - Epoch 536: total training loss 14.34\n",
            "2021-02-01 09:42:39,842 - INFO - joeynmt.training - EPOCH 537\n",
            "2021-02-01 09:42:52,293 - INFO - joeynmt.training - Epoch 537: total training loss 14.33\n",
            "2021-02-01 09:42:52,294 - INFO - joeynmt.training - EPOCH 538\n",
            "2021-02-01 09:43:04,818 - INFO - joeynmt.training - Epoch 538: total training loss 14.27\n",
            "2021-02-01 09:43:04,819 - INFO - joeynmt.training - EPOCH 539\n",
            "2021-02-01 09:43:17,326 - INFO - joeynmt.training - Epoch 539: total training loss 14.31\n",
            "2021-02-01 09:43:17,326 - INFO - joeynmt.training - EPOCH 540\n",
            "2021-02-01 09:43:29,796 - INFO - joeynmt.training - Epoch 540: total training loss 14.26\n",
            "2021-02-01 09:43:29,796 - INFO - joeynmt.training - EPOCH 541\n",
            "2021-02-01 09:43:42,377 - INFO - joeynmt.training - Epoch 541: total training loss 14.25\n",
            "2021-02-01 09:43:42,377 - INFO - joeynmt.training - EPOCH 542\n",
            "2021-02-01 09:43:54,707 - INFO - joeynmt.training - Epoch 542, Step:     8100, Batch Loss:     0.949596, Tokens per Sec:    18456, Lr: 0.000210\n",
            "2021-02-01 09:43:54,801 - INFO - joeynmt.training - Epoch 542: total training loss 14.26\n",
            "2021-02-01 09:43:54,801 - INFO - joeynmt.training - EPOCH 543\n",
            "2021-02-01 09:44:07,369 - INFO - joeynmt.training - Epoch 543: total training loss 14.21\n",
            "2021-02-01 09:44:07,369 - INFO - joeynmt.training - EPOCH 544\n",
            "2021-02-01 09:44:19,705 - INFO - joeynmt.training - Epoch 544: total training loss 14.20\n",
            "2021-02-01 09:44:19,706 - INFO - joeynmt.training - EPOCH 545\n",
            "2021-02-01 09:44:32,144 - INFO - joeynmt.training - Epoch 545: total training loss 14.15\n",
            "2021-02-01 09:44:32,145 - INFO - joeynmt.training - EPOCH 546\n",
            "2021-02-01 09:44:44,841 - INFO - joeynmt.training - Epoch 546: total training loss 14.13\n",
            "2021-02-01 09:44:44,841 - INFO - joeynmt.training - EPOCH 547\n",
            "2021-02-01 09:44:57,296 - INFO - joeynmt.training - Epoch 547: total training loss 14.10\n",
            "2021-02-01 09:44:57,296 - INFO - joeynmt.training - EPOCH 548\n",
            "2021-02-01 09:45:09,671 - INFO - joeynmt.training - Epoch 548: total training loss 14.16\n",
            "2021-02-01 09:45:09,671 - INFO - joeynmt.training - EPOCH 549\n",
            "2021-02-01 09:45:17,834 - INFO - joeynmt.training - Epoch 549, Step:     8200, Batch Loss:     0.976637, Tokens per Sec:    18198, Lr: 0.000210\n",
            "2021-02-01 09:45:22,014 - INFO - joeynmt.training - Epoch 549: total training loss 14.07\n",
            "2021-02-01 09:45:22,014 - INFO - joeynmt.training - EPOCH 550\n",
            "2021-02-01 09:45:34,377 - INFO - joeynmt.training - Epoch 550: total training loss 14.13\n",
            "2021-02-01 09:45:34,377 - INFO - joeynmt.training - EPOCH 551\n",
            "2021-02-01 09:45:46,653 - INFO - joeynmt.training - Epoch 551: total training loss 14.13\n",
            "2021-02-01 09:45:46,654 - INFO - joeynmt.training - EPOCH 552\n",
            "2021-02-01 09:45:59,188 - INFO - joeynmt.training - Epoch 552: total training loss 14.05\n",
            "2021-02-01 09:45:59,188 - INFO - joeynmt.training - EPOCH 553\n",
            "2021-02-01 09:46:11,580 - INFO - joeynmt.training - Epoch 553: total training loss 13.97\n",
            "2021-02-01 09:46:11,580 - INFO - joeynmt.training - EPOCH 554\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:46:24,082 - INFO - joeynmt.training - Epoch 554: total training loss 14.02\n",
            "2021-02-01 09:46:24,082 - INFO - joeynmt.training - EPOCH 555\n",
            "2021-02-01 09:46:36,441 - INFO - joeynmt.training - Epoch 555: total training loss 14.01\n",
            "2021-02-01 09:46:36,441 - INFO - joeynmt.training - EPOCH 556\n",
            "2021-02-01 09:46:40,545 - INFO - joeynmt.training - Epoch 556, Step:     8300, Batch Loss:     1.012716, Tokens per Sec:    18971, Lr: 0.000210\n",
            "2021-02-01 09:46:48,879 - INFO - joeynmt.training - Epoch 556: total training loss 14.02\n",
            "2021-02-01 09:46:48,879 - INFO - joeynmt.training - EPOCH 557\n",
            "2021-02-01 09:47:01,274 - INFO - joeynmt.training - Epoch 557: total training loss 14.03\n",
            "2021-02-01 09:47:01,275 - INFO - joeynmt.training - EPOCH 558\n",
            "2021-02-01 09:47:13,671 - INFO - joeynmt.training - Epoch 558: total training loss 13.92\n",
            "2021-02-01 09:47:13,671 - INFO - joeynmt.training - EPOCH 559\n",
            "2021-02-01 09:47:25,831 - INFO - joeynmt.training - Epoch 559: total training loss 13.01\n",
            "2021-02-01 09:47:25,832 - INFO - joeynmt.training - EPOCH 560\n",
            "2021-02-01 09:47:38,222 - INFO - joeynmt.training - Epoch 560: total training loss 13.95\n",
            "2021-02-01 09:47:38,223 - INFO - joeynmt.training - EPOCH 561\n",
            "2021-02-01 09:47:50,507 - INFO - joeynmt.training - Epoch 561: total training loss 14.04\n",
            "2021-02-01 09:47:50,508 - INFO - joeynmt.training - EPOCH 562\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 09:48:02,786 - INFO - joeynmt.training - Epoch 562: total training loss 13.91\n",
            "2021-02-01 09:48:02,787 - INFO - joeynmt.training - EPOCH 563\n",
            "2021-02-01 09:48:03,621 - INFO - joeynmt.training - Epoch 563, Step:     8400, Batch Loss:     0.912002, Tokens per Sec:    16845, Lr: 0.000210\n",
            "2021-02-01 09:48:15,300 - INFO - joeynmt.training - Epoch 563: total training loss 13.86\n",
            "2021-02-01 09:48:15,301 - INFO - joeynmt.training - EPOCH 564\n",
            "2021-02-01 09:48:27,877 - INFO - joeynmt.training - Epoch 564: total training loss 13.87\n",
            "2021-02-01 09:48:27,878 - INFO - joeynmt.training - EPOCH 565\n",
            "2021-02-01 09:48:40,383 - INFO - joeynmt.training - Epoch 565: total training loss 13.81\n",
            "2021-02-01 09:48:40,383 - INFO - joeynmt.training - EPOCH 566\n",
            "2021-02-01 09:48:53,019 - INFO - joeynmt.training - Epoch 566: total training loss 13.78\n",
            "2021-02-01 09:48:53,020 - INFO - joeynmt.training - EPOCH 567\n",
            "2021-02-01 09:49:05,492 - INFO - joeynmt.training - Epoch 567: total training loss 13.89\n",
            "2021-02-01 09:49:05,493 - INFO - joeynmt.training - EPOCH 568\n",
            "2021-02-01 09:49:17,884 - INFO - joeynmt.training - Epoch 568: total training loss 13.80\n",
            "2021-02-01 09:49:17,885 - INFO - joeynmt.training - EPOCH 569\n",
            "2021-02-01 09:49:26,790 - INFO - joeynmt.training - Epoch 569, Step:     8500, Batch Loss:     0.950070, Tokens per Sec:    18100, Lr: 0.000210\n",
            "2021-02-01 09:49:55,941 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың жазбүркелер туралы мәжілісханаларында※ барып, мәжілісханаларға айналдыру үшін мәжілісханаға жіберді.※\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:49:55,942 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - \tHypothesis: Жындардың түскен дәуір ағашқа шегелеп іліп өлтірді.\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, сол адам сұлт деп аталатын Құдайға бас тартты. Тоқтының он патшаны көрмеген әйелдің қатты ашты※ және Құдайды мадақтап күкірт көрдім.\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:49:55,943 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:49:55,944 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:49:55,944 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетінде орналасқан жердің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 09:49:55,944 - INFO - joeynmt.training - Validation result (greedy) at epoch 569, step     8500: bleu:   4.05, loss: 86085.0781, ppl:  72.5231, duration: 29.1535s\n",
            "2021-02-01 09:49:59,536 - INFO - joeynmt.training - Epoch 569: total training loss 13.84\n",
            "2021-02-01 09:49:59,536 - INFO - joeynmt.training - EPOCH 570\n",
            "2021-02-01 09:50:11,681 - INFO - joeynmt.training - Epoch 570: total training loss 12.84\n",
            "2021-02-01 09:50:11,681 - INFO - joeynmt.training - EPOCH 571\n",
            "2021-02-01 09:50:24,186 - INFO - joeynmt.training - Epoch 571: total training loss 13.70\n",
            "2021-02-01 09:50:24,187 - INFO - joeynmt.training - EPOCH 572\n",
            "2021-02-01 09:50:36,497 - INFO - joeynmt.training - Epoch 572: total training loss 13.85\n",
            "2021-02-01 09:50:36,497 - INFO - joeynmt.training - EPOCH 573\n",
            "2021-02-01 09:50:49,071 - INFO - joeynmt.training - Epoch 573: total training loss 13.78\n",
            "2021-02-01 09:50:49,071 - INFO - joeynmt.training - EPOCH 574\n",
            "2021-02-01 09:51:01,332 - INFO - joeynmt.training - Epoch 574: total training loss 13.79\n",
            "2021-02-01 09:51:01,332 - INFO - joeynmt.training - EPOCH 575\n",
            "2021-02-01 09:51:13,823 - INFO - joeynmt.training - Epoch 575: total training loss 13.77\n",
            "2021-02-01 09:51:13,823 - INFO - joeynmt.training - EPOCH 576\n",
            "2021-02-01 09:51:19,526 - INFO - joeynmt.training - Epoch 576, Step:     8600, Batch Loss:     0.882809, Tokens per Sec:    18325, Lr: 0.000210\n",
            "2021-02-01 09:51:26,242 - INFO - joeynmt.training - Epoch 576: total training loss 13.73\n",
            "2021-02-01 09:51:26,243 - INFO - joeynmt.training - EPOCH 577\n",
            "2021-02-01 09:51:38,474 - INFO - joeynmt.training - Epoch 577: total training loss 12.75\n",
            "2021-02-01 09:51:38,474 - INFO - joeynmt.training - EPOCH 578\n",
            "2021-02-01 09:51:50,928 - INFO - joeynmt.training - Epoch 578: total training loss 13.74\n",
            "2021-02-01 09:51:50,929 - INFO - joeynmt.training - EPOCH 579\n",
            "2021-02-01 09:52:03,093 - INFO - joeynmt.training - Epoch 579: total training loss 12.80\n",
            "2021-02-01 09:52:03,093 - INFO - joeynmt.training - EPOCH 580\n",
            "2021-02-01 09:52:15,579 - INFO - joeynmt.training - Epoch 580: total training loss 13.69\n",
            "2021-02-01 09:52:15,580 - INFO - joeynmt.training - EPOCH 581\n",
            "2021-02-01 09:52:28,079 - INFO - joeynmt.training - Epoch 581: total training loss 13.69\n",
            "2021-02-01 09:52:28,079 - INFO - joeynmt.training - EPOCH 582\n",
            "2021-02-01 09:52:40,589 - INFO - joeynmt.training - Epoch 582: total training loss 13.65\n",
            "2021-02-01 09:52:40,589 - INFO - joeynmt.training - EPOCH 583\n",
            "2021-02-01 09:52:43,888 - INFO - joeynmt.training - Epoch 583, Step:     8700, Batch Loss:     0.808598, Tokens per Sec:    16961, Lr: 0.000210\n",
            "2021-02-01 09:52:53,324 - INFO - joeynmt.training - Epoch 583: total training loss 13.62\n",
            "2021-02-01 09:52:53,324 - INFO - joeynmt.training - EPOCH 584\n",
            "2021-02-01 09:53:05,614 - INFO - joeynmt.training - Epoch 584: total training loss 13.53\n",
            "2021-02-01 09:53:05,615 - INFO - joeynmt.training - EPOCH 585\n",
            "2021-02-01 09:53:17,897 - INFO - joeynmt.training - Epoch 585: total training loss 12.76\n",
            "2021-02-01 09:53:17,897 - INFO - joeynmt.training - EPOCH 586\n",
            "2021-02-01 09:53:30,185 - INFO - joeynmt.training - Epoch 586: total training loss 13.58\n",
            "2021-02-01 09:53:30,185 - INFO - joeynmt.training - EPOCH 587\n",
            "2021-02-01 09:53:42,532 - INFO - joeynmt.training - Epoch 587: total training loss 13.55\n",
            "2021-02-01 09:53:42,532 - INFO - joeynmt.training - EPOCH 588\n",
            "2021-02-01 09:53:55,080 - INFO - joeynmt.training - Epoch 588: total training loss 13.59\n",
            "2021-02-01 09:53:55,081 - INFO - joeynmt.training - EPOCH 589\n",
            "2021-02-01 09:54:07,481 - INFO - joeynmt.training - Epoch 589, Step:     8800, Batch Loss:     0.817664, Tokens per Sec:    18438, Lr: 0.000210\n",
            "2021-02-01 09:54:07,482 - INFO - joeynmt.training - Epoch 589: total training loss 13.57\n",
            "2021-02-01 09:54:07,482 - INFO - joeynmt.training - EPOCH 590\n",
            "2021-02-01 09:54:19,815 - INFO - joeynmt.training - Epoch 590: total training loss 13.53\n",
            "2021-02-01 09:54:19,816 - INFO - joeynmt.training - EPOCH 591\n",
            "2021-02-01 09:54:32,373 - INFO - joeynmt.training - Epoch 591: total training loss 13.52\n",
            "2021-02-01 09:54:32,374 - INFO - joeynmt.training - EPOCH 592\n",
            "2021-02-01 09:54:44,817 - INFO - joeynmt.training - Epoch 592: total training loss 13.51\n",
            "2021-02-01 09:54:44,818 - INFO - joeynmt.training - EPOCH 593\n",
            "2021-02-01 09:54:57,286 - INFO - joeynmt.training - Epoch 593: total training loss 13.44\n",
            "2021-02-01 09:54:57,287 - INFO - joeynmt.training - EPOCH 594\n",
            "2021-02-01 09:55:09,623 - INFO - joeynmt.training - Epoch 594: total training loss 13.49\n",
            "2021-02-01 09:55:09,624 - INFO - joeynmt.training - EPOCH 595\n",
            "2021-02-01 09:55:21,949 - INFO - joeynmt.training - Epoch 595: total training loss 13.49\n",
            "2021-02-01 09:55:21,949 - INFO - joeynmt.training - EPOCH 596\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 09:55:30,131 - INFO - joeynmt.training - Epoch 596, Step:     8900, Batch Loss:     0.938539, Tokens per Sec:    18698, Lr: 0.000210\n",
            "2021-02-01 09:55:34,238 - INFO - joeynmt.training - Epoch 596: total training loss 13.45\n",
            "2021-02-01 09:55:34,239 - INFO - joeynmt.training - EPOCH 597\n",
            "2021-02-01 09:55:46,817 - INFO - joeynmt.training - Epoch 597: total training loss 13.33\n",
            "2021-02-01 09:55:46,818 - INFO - joeynmt.training - EPOCH 598\n",
            "2021-02-01 09:55:59,148 - INFO - joeynmt.training - Epoch 598: total training loss 13.47\n",
            "2021-02-01 09:55:59,149 - INFO - joeynmt.training - EPOCH 599\n",
            "2021-02-01 09:56:11,501 - INFO - joeynmt.training - Epoch 599: total training loss 13.47\n",
            "2021-02-01 09:56:11,502 - INFO - joeynmt.training - EPOCH 600\n",
            "2021-02-01 09:56:24,226 - INFO - joeynmt.training - Epoch 600: total training loss 13.30\n",
            "2021-02-01 09:56:24,227 - INFO - joeynmt.training - EPOCH 601\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 09:56:36,594 - INFO - joeynmt.training - Epoch 601: total training loss 13.41\n",
            "2021-02-01 09:56:36,595 - INFO - joeynmt.training - EPOCH 602\n",
            "2021-02-01 09:56:48,838 - INFO - joeynmt.training - Epoch 602: total training loss 12.48\n",
            "2021-02-01 09:56:48,838 - INFO - joeynmt.training - EPOCH 603\n",
            "2021-02-01 09:56:53,776 - INFO - joeynmt.training - Epoch 603, Step:     9000, Batch Loss:     0.880434, Tokens per Sec:    18641, Lr: 0.000210\n",
            "2021-02-01 09:57:22,890 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 09:57:22,891 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 09:57:22,891 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 09:57:22,891 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың жазбасында мінажат ететін. Сол кезде мәжілісханаларға барып, жұртқа тәлім берудің мәжілісханаға жіберді.※\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - \tHypothesis: Сол кісі түскен қылмыскерді бұғаға қарап тұрып, Исаны өлтіруге мәжбүр етті.\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 09:57:22,892 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартулар сияқты Құдай оны қабылдаған Ізгі хабарға әкеліп тапқан кісінің көрдім. Олар қатты ашылып, Құдайды мадақтап білмей көрмедім. Олар қатты аштыр екен. Үшінде ма, алтын жалатқан әйелдер мен нештеңе көрмеді.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетінде орналасқан оң жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 09:57:22,893 - INFO - joeynmt.training - Validation result (greedy) at epoch 603, step     9000: bleu:   4.24, loss: 86217.4531, ppl:  73.0024, duration: 29.1165s\n",
            "2021-02-01 09:57:30,271 - INFO - joeynmt.training - Epoch 603: total training loss 13.37\n",
            "2021-02-01 09:57:30,271 - INFO - joeynmt.training - EPOCH 604\n",
            "2021-02-01 09:57:42,787 - INFO - joeynmt.training - Epoch 604: total training loss 13.38\n",
            "2021-02-01 09:57:42,788 - INFO - joeynmt.training - EPOCH 605\n",
            "2021-02-01 09:57:55,294 - INFO - joeynmt.training - Epoch 605: total training loss 13.30\n",
            "2021-02-01 09:57:55,294 - INFO - joeynmt.training - EPOCH 606\n",
            "2021-02-01 09:58:07,571 - INFO - joeynmt.training - Epoch 606: total training loss 13.33\n",
            "2021-02-01 09:58:07,572 - INFO - joeynmt.training - EPOCH 607\n",
            "2021-02-01 09:58:19,840 - INFO - joeynmt.training - Epoch 607: total training loss 13.36\n",
            "2021-02-01 09:58:19,841 - INFO - joeynmt.training - EPOCH 608\n",
            "2021-02-01 09:58:32,214 - INFO - joeynmt.training - Epoch 608: total training loss 13.29\n",
            "2021-02-01 09:58:32,215 - INFO - joeynmt.training - EPOCH 609\n",
            "2021-02-01 09:58:44,735 - INFO - joeynmt.training - Epoch 609: total training loss 13.24\n",
            "2021-02-01 09:58:44,736 - INFO - joeynmt.training - EPOCH 610\n",
            "2021-02-01 09:58:45,584 - INFO - joeynmt.training - Epoch 610, Step:     9100, Batch Loss:     0.909802, Tokens per Sec:    17876, Lr: 0.000210\n",
            "2021-02-01 09:58:57,143 - INFO - joeynmt.training - Epoch 610: total training loss 13.31\n",
            "2021-02-01 09:58:57,143 - INFO - joeynmt.training - EPOCH 611\n",
            "2021-02-01 09:59:09,592 - INFO - joeynmt.training - Epoch 611: total training loss 13.24\n",
            "2021-02-01 09:59:09,592 - INFO - joeynmt.training - EPOCH 612\n",
            "2021-02-01 09:59:21,888 - INFO - joeynmt.training - Epoch 612: total training loss 13.28\n",
            "2021-02-01 09:59:21,888 - INFO - joeynmt.training - EPOCH 613\n",
            "2021-02-01 09:59:34,435 - INFO - joeynmt.training - Epoch 613: total training loss 13.18\n",
            "2021-02-01 09:59:34,435 - INFO - joeynmt.training - EPOCH 614\n",
            "2021-02-01 09:59:46,821 - INFO - joeynmt.training - Epoch 614: total training loss 13.22\n",
            "2021-02-01 09:59:46,822 - INFO - joeynmt.training - EPOCH 615\n",
            "2021-02-01 09:59:59,327 - INFO - joeynmt.training - Epoch 615: total training loss 13.16\n",
            "2021-02-01 09:59:59,328 - INFO - joeynmt.training - EPOCH 616\n",
            "2021-02-01 10:00:08,280 - INFO - joeynmt.training - Epoch 616, Step:     9200, Batch Loss:     0.837752, Tokens per Sec:    18008, Lr: 0.000210\n",
            "2021-02-01 10:00:11,821 - INFO - joeynmt.training - Epoch 616: total training loss 13.19\n",
            "2021-02-01 10:00:11,822 - INFO - joeynmt.training - EPOCH 617\n",
            "2021-02-01 10:00:24,257 - INFO - joeynmt.training - Epoch 617: total training loss 13.22\n",
            "2021-02-01 10:00:24,258 - INFO - joeynmt.training - EPOCH 618\n",
            "2021-02-01 10:00:36,523 - INFO - joeynmt.training - Epoch 618: total training loss 13.19\n",
            "2021-02-01 10:00:36,523 - INFO - joeynmt.training - EPOCH 619\n",
            "2021-02-01 10:00:49,083 - INFO - joeynmt.training - Epoch 619: total training loss 13.22\n",
            "2021-02-01 10:00:49,083 - INFO - joeynmt.training - EPOCH 620\n",
            "2021-02-01 10:01:01,812 - INFO - joeynmt.training - Epoch 620: total training loss 13.12\n",
            "2021-02-01 10:01:01,813 - INFO - joeynmt.training - EPOCH 621\n",
            "2021-02-01 10:01:14,400 - INFO - joeynmt.training - Epoch 621: total training loss 13.12\n",
            "2021-02-01 10:01:14,400 - INFO - joeynmt.training - EPOCH 622\n",
            "2021-02-01 10:01:27,141 - INFO - joeynmt.training - Epoch 622: total training loss 13.14\n",
            "2021-02-01 10:01:27,141 - INFO - joeynmt.training - EPOCH 623\n",
            "2021-02-01 10:01:32,304 - INFO - joeynmt.training - Epoch 623, Step:     9300, Batch Loss:     0.954322, Tokens per Sec:    18211, Lr: 0.000210\n",
            "2021-02-01 10:01:39,769 - INFO - joeynmt.training - Epoch 623: total training loss 12.29\n",
            "2021-02-01 10:01:39,770 - INFO - joeynmt.training - EPOCH 624\n",
            "2021-02-01 10:01:52,496 - INFO - joeynmt.training - Epoch 624: total training loss 13.08\n",
            "2021-02-01 10:01:52,497 - INFO - joeynmt.training - EPOCH 625\n",
            "2021-02-01 10:02:04,934 - INFO - joeynmt.training - Epoch 625: total training loss 13.07\n",
            "2021-02-01 10:02:04,934 - INFO - joeynmt.training - EPOCH 626\n",
            "2021-02-01 10:02:17,301 - INFO - joeynmt.training - Epoch 626: total training loss 12.98\n",
            "2021-02-01 10:02:17,302 - INFO - joeynmt.training - EPOCH 627\n",
            "2021-02-01 10:02:29,672 - INFO - joeynmt.training - Epoch 627: total training loss 12.98\n",
            "2021-02-01 10:02:29,672 - INFO - joeynmt.training - EPOCH 628\n",
            "2021-02-01 10:02:42,054 - INFO - joeynmt.training - Epoch 628: total training loss 13.06\n",
            "2021-02-01 10:02:42,054 - INFO - joeynmt.training - EPOCH 629\n",
            "2021-02-01 10:02:54,545 - INFO - joeynmt.training - Epoch 629: total training loss 13.03\n",
            "2021-02-01 10:02:54,546 - INFO - joeynmt.training - EPOCH 630\n",
            "2021-02-01 10:02:56,235 - INFO - joeynmt.training - Epoch 630, Step:     9400, Batch Loss:     0.852080, Tokens per Sec:    18965, Lr: 0.000210\n",
            "2021-02-01 10:03:07,019 - INFO - joeynmt.training - Epoch 630: total training loss 12.98\n",
            "2021-02-01 10:03:07,020 - INFO - joeynmt.training - EPOCH 631\n",
            "2021-02-01 10:03:19,555 - INFO - joeynmt.training - Epoch 631: total training loss 13.02\n",
            "2021-02-01 10:03:19,555 - INFO - joeynmt.training - EPOCH 632\n",
            "2021-02-01 10:03:31,786 - INFO - joeynmt.training - Epoch 632: total training loss 13.07\n",
            "2021-02-01 10:03:31,787 - INFO - joeynmt.training - EPOCH 633\n",
            "2021-02-01 10:03:44,017 - INFO - joeynmt.training - Epoch 633: total training loss 13.04\n",
            "2021-02-01 10:03:44,018 - INFO - joeynmt.training - EPOCH 634\n",
            "2021-02-01 10:03:56,532 - INFO - joeynmt.training - Epoch 634: total training loss 12.95\n",
            "2021-02-01 10:03:56,532 - INFO - joeynmt.training - EPOCH 635\n",
            "2021-02-01 10:04:09,108 - INFO - joeynmt.training - Epoch 635: total training loss 12.98\n",
            "2021-02-01 10:04:09,108 - INFO - joeynmt.training - EPOCH 636\n",
            "2021-02-01 10:04:18,815 - INFO - joeynmt.training - Epoch 636, Step:     9500, Batch Loss:     0.838963, Tokens per Sec:    18595, Lr: 0.000210\n",
            "2021-02-01 10:04:46,583 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:04:46,584 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:04:46,585 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:04:46,585 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген Таурат заңы бірнеше ғау※ мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:04:46,585 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - \tHypothesis: Сол кісі өлтіргенде генде Исаны байлап түсіп, Оны оятты.\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:04:46,586 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың ескі күнәкар адамдарға арнап Ізгі хабарды қабылдаған жоқ. Тоқтының үстінде інде отырған Құдай мау үшін Құдайды мадақтай тіліп, Құдайды мадақтап жүрдім. Олар қатты ашуландырды. Олар айуанның мәнін көрмей берілмеген әйелдер мен ашуландырдым.\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер, жер бетіндегі барлық жағындағы құрметті орнына жайғасқан кезде Мәсіх Оның оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 10:04:46,587 - INFO - joeynmt.training - Validation result (greedy) at epoch 636, step     9500: bleu:   4.45, loss: 86327.3594, ppl:  73.4028, duration: 27.7715s\n",
            "2021-02-01 10:04:49,237 - INFO - joeynmt.training - Epoch 636: total training loss 12.97\n",
            "2021-02-01 10:04:49,238 - INFO - joeynmt.training - EPOCH 637\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:05:01,720 - INFO - joeynmt.training - Epoch 637: total training loss 12.94\n",
            "2021-02-01 10:05:01,720 - INFO - joeynmt.training - EPOCH 638\n",
            "2021-02-01 10:05:14,243 - INFO - joeynmt.training - Epoch 638: total training loss 12.96\n",
            "2021-02-01 10:05:14,244 - INFO - joeynmt.training - EPOCH 639\n",
            "2021-02-01 10:05:26,598 - INFO - joeynmt.training - Epoch 639: total training loss 12.95\n",
            "2021-02-01 10:05:26,598 - INFO - joeynmt.training - EPOCH 640\n",
            "2021-02-01 10:05:39,246 - INFO - joeynmt.training - Epoch 640: total training loss 12.94\n",
            "2021-02-01 10:05:39,246 - INFO - joeynmt.training - EPOCH 641\n",
            "2021-02-01 10:05:51,706 - INFO - joeynmt.training - Epoch 641: total training loss 12.86\n",
            "2021-02-01 10:05:51,706 - INFO - joeynmt.training - EPOCH 642\n",
            "2021-02-01 10:06:03,983 - INFO - joeynmt.training - Epoch 642: total training loss 12.94\n",
            "2021-02-01 10:06:03,984 - INFO - joeynmt.training - EPOCH 643\n",
            "2021-02-01 10:06:09,717 - INFO - joeynmt.training - Epoch 643, Step:     9600, Batch Loss:     0.811259, Tokens per Sec:    17802, Lr: 0.000210\n",
            "2021-02-01 10:06:16,575 - INFO - joeynmt.training - Epoch 643: total training loss 12.89\n",
            "2021-02-01 10:06:16,575 - INFO - joeynmt.training - EPOCH 644\n",
            "2021-02-01 10:06:28,964 - INFO - joeynmt.training - Epoch 644: total training loss 12.80\n",
            "2021-02-01 10:06:28,971 - INFO - joeynmt.training - EPOCH 645\n",
            "2021-02-01 10:06:41,503 - INFO - joeynmt.training - Epoch 645: total training loss 12.89\n",
            "2021-02-01 10:06:41,504 - INFO - joeynmt.training - EPOCH 646\n",
            "2021-02-01 10:06:54,048 - INFO - joeynmt.training - Epoch 646: total training loss 12.84\n",
            "2021-02-01 10:06:54,048 - INFO - joeynmt.training - EPOCH 647\n",
            "2021-02-01 10:07:06,416 - INFO - joeynmt.training - Epoch 647: total training loss 12.84\n",
            "2021-02-01 10:07:06,417 - INFO - joeynmt.training - EPOCH 648\n",
            "2021-02-01 10:07:18,921 - INFO - joeynmt.training - Epoch 648: total training loss 12.79\n",
            "2021-02-01 10:07:18,921 - INFO - joeynmt.training - EPOCH 649\n",
            "2021-02-01 10:07:31,283 - INFO - joeynmt.training - Epoch 649: total training loss 12.81\n",
            "2021-02-01 10:07:31,284 - INFO - joeynmt.training - EPOCH 650\n",
            "2021-02-01 10:07:32,946 - INFO - joeynmt.training - Epoch 650, Step:     9700, Batch Loss:     0.934571, Tokens per Sec:    18946, Lr: 0.000210\n",
            "2021-02-01 10:07:43,786 - INFO - joeynmt.training - Epoch 650: total training loss 12.78\n",
            "2021-02-01 10:07:43,786 - INFO - joeynmt.training - EPOCH 651\n",
            "2021-02-01 10:07:56,341 - INFO - joeynmt.training - Epoch 651: total training loss 12.75\n",
            "2021-02-01 10:07:56,342 - INFO - joeynmt.training - EPOCH 652\n",
            "2021-02-01 10:08:09,072 - INFO - joeynmt.training - Epoch 652: total training loss 12.75\n",
            "2021-02-01 10:08:09,073 - INFO - joeynmt.training - EPOCH 653\n",
            "2021-02-01 10:08:21,608 - INFO - joeynmt.training - Epoch 653: total training loss 12.75\n",
            "2021-02-01 10:08:21,608 - INFO - joeynmt.training - EPOCH 654\n",
            "2021-02-01 10:08:34,147 - INFO - joeynmt.training - Epoch 654: total training loss 12.73\n",
            "2021-02-01 10:08:34,148 - INFO - joeynmt.training - EPOCH 655\n",
            "2021-02-01 10:08:46,503 - INFO - joeynmt.training - Epoch 655: total training loss 12.74\n",
            "2021-02-01 10:08:46,504 - INFO - joeynmt.training - EPOCH 656\n",
            "2021-02-01 10:08:56,201 - INFO - joeynmt.training - Epoch 656, Step:     9800, Batch Loss:     0.886297, Tokens per Sec:    18245, Lr: 0.000210\n",
            "2021-02-01 10:08:58,971 - INFO - joeynmt.training - Epoch 656: total training loss 12.69\n",
            "2021-02-01 10:08:58,971 - INFO - joeynmt.training - EPOCH 657\n",
            "2021-02-01 10:09:11,304 - INFO - joeynmt.training - Epoch 657: total training loss 12.79\n",
            "2021-02-01 10:09:11,305 - INFO - joeynmt.training - EPOCH 658\n",
            "2021-02-01 10:09:23,695 - INFO - joeynmt.training - Epoch 658: total training loss 12.66\n",
            "2021-02-01 10:09:23,695 - INFO - joeynmt.training - EPOCH 659\n",
            "2021-02-01 10:09:36,210 - INFO - joeynmt.training - Epoch 659: total training loss 12.68\n",
            "2021-02-01 10:09:36,211 - INFO - joeynmt.training - EPOCH 660\n",
            "2021-02-01 10:09:48,505 - INFO - joeynmt.training - Epoch 660: total training loss 12.70\n",
            "2021-02-01 10:09:48,506 - INFO - joeynmt.training - EPOCH 661\n",
            "2021-02-01 10:10:00,940 - INFO - joeynmt.training - Epoch 661: total training loss 12.74\n",
            "2021-02-01 10:10:00,941 - INFO - joeynmt.training - EPOCH 662\n",
            "2021-02-01 10:10:13,246 - INFO - joeynmt.training - Epoch 662: total training loss 12.65\n",
            "2021-02-01 10:10:13,246 - INFO - joeynmt.training - EPOCH 663\n",
            "2021-02-01 10:10:18,942 - INFO - joeynmt.training - Epoch 663, Step:     9900, Batch Loss:     0.876472, Tokens per Sec:    18567, Lr: 0.000210\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:10:25,596 - INFO - joeynmt.training - Epoch 663: total training loss 12.66\n",
            "2021-02-01 10:10:25,596 - INFO - joeynmt.training - EPOCH 664\n",
            "2021-02-01 10:10:38,101 - INFO - joeynmt.training - Epoch 664: total training loss 12.58\n",
            "2021-02-01 10:10:38,102 - INFO - joeynmt.training - EPOCH 665\n",
            "2021-02-01 10:10:50,333 - INFO - joeynmt.training - Epoch 665: total training loss 12.63\n",
            "2021-02-01 10:10:50,334 - INFO - joeynmt.training - EPOCH 666\n",
            "2021-02-01 10:11:02,712 - INFO - joeynmt.training - Epoch 666: total training loss 12.54\n",
            "2021-02-01 10:11:02,712 - INFO - joeynmt.training - EPOCH 667\n",
            "2021-02-01 10:11:15,130 - INFO - joeynmt.training - Epoch 667: total training loss 12.59\n",
            "2021-02-01 10:11:15,130 - INFO - joeynmt.training - EPOCH 668\n",
            "2021-02-01 10:11:27,328 - INFO - joeynmt.training - Epoch 668: total training loss 12.60\n",
            "2021-02-01 10:11:27,328 - INFO - joeynmt.training - EPOCH 669\n",
            "2021-02-01 10:11:39,592 - INFO - joeynmt.training - Epoch 669: total training loss 12.62\n",
            "2021-02-01 10:11:39,592 - INFO - joeynmt.training - EPOCH 670\n",
            "2021-02-01 10:11:41,271 - INFO - joeynmt.training - Epoch 670, Step:    10000, Batch Loss:     0.886409, Tokens per Sec:    19055, Lr: 0.000210\n",
            "2021-02-01 10:12:09,008 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:12:09,009 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:12:09,009 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:12:09,009 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың жазбүркеуіп бара жатып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:12:09,010 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:12:09,010 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:12:09,011 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:12:09,011 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі «Оны байлап тың» деп үміттенді.\n",
            "2021-02-01 10:12:09,011 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:12:09,011 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:12:09,011 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:12:09,012 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл әлемді ескі күнәкар адамдарға арнап құрбандық ретінде ұсынған Ізгі хабардың үстінде барған Құдай оны көрдім. Ол Тоқтының жер бетінде мағанын※ көрдім. Олар қатты ашуландырды. Олар айуанның мәнін көрмей берілмеген әйелдер мен әйелдер айуанды қатты ашуланды.\n",
            "2021-02-01 10:12:09,012 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:12:09,012 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:12:09,012 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:12:09,013 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге қайта тірілмеген кезде де көкке көтерілмеңдер. Енді Оның оң жағындағы құрметті орнына жайғасты. Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 10:12:09,013 - INFO - joeynmt.training - Validation result (greedy) at epoch 670, step    10000: bleu:   4.72, loss: 86616.2031, ppl:  74.4655, duration: 27.7413s\n",
            "2021-02-01 10:12:19,749 - INFO - joeynmt.training - Epoch 670: total training loss 12.51\n",
            "2021-02-01 10:12:19,749 - INFO - joeynmt.training - EPOCH 671\n",
            "2021-02-01 10:12:32,023 - INFO - joeynmt.training - Epoch 671: total training loss 12.44\n",
            "2021-02-01 10:12:32,024 - INFO - joeynmt.training - EPOCH 672\n",
            "2021-02-01 10:12:44,421 - INFO - joeynmt.training - Epoch 672: total training loss 12.45\n",
            "2021-02-01 10:12:44,421 - INFO - joeynmt.training - EPOCH 673\n",
            "2021-02-01 10:12:56,836 - INFO - joeynmt.training - Epoch 673: total training loss 12.45\n",
            "2021-02-01 10:12:56,837 - INFO - joeynmt.training - EPOCH 674\n",
            "2021-02-01 10:13:09,167 - INFO - joeynmt.training - Epoch 674: total training loss 12.42\n",
            "2021-02-01 10:13:09,168 - INFO - joeynmt.training - EPOCH 675\n",
            "2021-02-01 10:13:21,648 - INFO - joeynmt.training - Epoch 675: total training loss 12.34\n",
            "2021-02-01 10:13:21,648 - INFO - joeynmt.training - EPOCH 676\n",
            "2021-02-01 10:13:31,571 - INFO - joeynmt.training - Epoch 676, Step:    10100, Batch Loss:     0.866617, Tokens per Sec:    18098, Lr: 0.000147\n",
            "2021-02-01 10:13:34,294 - INFO - joeynmt.training - Epoch 676: total training loss 12.44\n",
            "2021-02-01 10:13:34,294 - INFO - joeynmt.training - EPOCH 677\n",
            "2021-02-01 10:13:46,598 - INFO - joeynmt.training - Epoch 677: total training loss 12.35\n",
            "2021-02-01 10:13:46,598 - INFO - joeynmt.training - EPOCH 678\n",
            "2021-02-01 10:13:58,971 - INFO - joeynmt.training - Epoch 678: total training loss 12.36\n",
            "2021-02-01 10:13:58,971 - INFO - joeynmt.training - EPOCH 679\n",
            "2021-02-01 10:14:11,415 - INFO - joeynmt.training - Epoch 679: total training loss 12.45\n",
            "2021-02-01 10:14:11,415 - INFO - joeynmt.training - EPOCH 680\n",
            "2021-02-01 10:14:23,939 - INFO - joeynmt.training - Epoch 680: total training loss 12.28\n",
            "2021-02-01 10:14:23,940 - INFO - joeynmt.training - EPOCH 681\n",
            "2021-02-01 10:14:36,271 - INFO - joeynmt.training - Epoch 681: total training loss 12.29\n",
            "2021-02-01 10:14:36,272 - INFO - joeynmt.training - EPOCH 682\n",
            "2021-02-01 10:14:48,778 - INFO - joeynmt.training - Epoch 682: total training loss 12.29\n",
            "2021-02-01 10:14:48,779 - INFO - joeynmt.training - EPOCH 683\n",
            "2021-02-01 10:14:54,510 - INFO - joeynmt.training - Epoch 683, Step:    10200, Batch Loss:     0.816158, Tokens per Sec:    18385, Lr: 0.000147\n",
            "2021-02-01 10:15:01,286 - INFO - joeynmt.training - Epoch 683: total training loss 12.30\n",
            "2021-02-01 10:15:01,286 - INFO - joeynmt.training - EPOCH 684\n",
            "2021-02-01 10:15:13,524 - INFO - joeynmt.training - Epoch 684: total training loss 12.34\n",
            "2021-02-01 10:15:13,525 - INFO - joeynmt.training - EPOCH 685\n",
            "2021-02-01 10:15:25,927 - INFO - joeynmt.training - Epoch 685: total training loss 12.29\n",
            "2021-02-01 10:15:25,928 - INFO - joeynmt.training - EPOCH 686\n",
            "2021-02-01 10:15:38,202 - INFO - joeynmt.training - Epoch 686: total training loss 12.21\n",
            "2021-02-01 10:15:38,203 - INFO - joeynmt.training - EPOCH 687\n",
            "2021-02-01 10:15:50,051 - INFO - joeynmt.training - Epoch 687: total training loss 11.48\n",
            "2021-02-01 10:15:50,051 - INFO - joeynmt.training - EPOCH 688\n",
            "2021-02-01 10:16:02,472 - INFO - joeynmt.training - Epoch 688: total training loss 12.23\n",
            "2021-02-01 10:16:02,472 - INFO - joeynmt.training - EPOCH 689\n",
            "2021-02-01 10:16:14,866 - INFO - joeynmt.training - Epoch 689: total training loss 12.23\n",
            "2021-02-01 10:16:14,867 - INFO - joeynmt.training - EPOCH 690\n",
            "2021-02-01 10:16:17,304 - INFO - joeynmt.training - Epoch 690, Step:    10300, Batch Loss:     0.832802, Tokens per Sec:    18509, Lr: 0.000147\n",
            "2021-02-01 10:16:27,114 - INFO - joeynmt.training - Epoch 690: total training loss 12.29\n",
            "2021-02-01 10:16:27,115 - INFO - joeynmt.training - EPOCH 691\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:16:39,309 - INFO - joeynmt.training - Epoch 691: total training loss 12.16\n",
            "2021-02-01 10:16:39,309 - INFO - joeynmt.training - EPOCH 692\n",
            "2021-02-01 10:16:51,564 - INFO - joeynmt.training - Epoch 692: total training loss 12.25\n",
            "2021-02-01 10:16:51,564 - INFO - joeynmt.training - EPOCH 693\n",
            "2021-02-01 10:17:04,012 - INFO - joeynmt.training - Epoch 693: total training loss 12.23\n",
            "2021-02-01 10:17:04,013 - INFO - joeynmt.training - EPOCH 694\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 10:17:16,231 - INFO - joeynmt.training - Epoch 694: total training loss 12.22\n",
            "2021-02-01 10:17:16,232 - INFO - joeynmt.training - EPOCH 695\n",
            "2021-02-01 10:17:28,628 - INFO - joeynmt.training - Epoch 695: total training loss 12.14\n",
            "2021-02-01 10:17:28,628 - INFO - joeynmt.training - EPOCH 696\n",
            "2021-02-01 10:17:39,103 - INFO - joeynmt.training - Epoch 696, Step:    10400, Batch Loss:     0.748990, Tokens per Sec:    18631, Lr: 0.000147\n",
            "2021-02-01 10:17:41,016 - INFO - joeynmt.training - Epoch 696: total training loss 12.17\n",
            "2021-02-01 10:17:41,016 - INFO - joeynmt.training - EPOCH 697\n",
            "2021-02-01 10:17:53,444 - INFO - joeynmt.training - Epoch 697: total training loss 12.15\n",
            "2021-02-01 10:17:53,444 - INFO - joeynmt.training - EPOCH 698\n",
            "2021-02-01 10:18:05,754 - INFO - joeynmt.training - Epoch 698: total training loss 12.23\n",
            "2021-02-01 10:18:05,754 - INFO - joeynmt.training - EPOCH 699\n",
            "2021-02-01 10:18:18,123 - INFO - joeynmt.training - Epoch 699: total training loss 12.19\n",
            "2021-02-01 10:18:18,124 - INFO - joeynmt.training - EPOCH 700\n",
            "2021-02-01 10:18:30,324 - INFO - joeynmt.training - Epoch 700: total training loss 12.18\n",
            "2021-02-01 10:18:30,324 - INFO - joeynmt.training - EPOCH 701\n",
            "2021-02-01 10:18:42,788 - INFO - joeynmt.training - Epoch 701: total training loss 12.18\n",
            "2021-02-01 10:18:42,788 - INFO - joeynmt.training - EPOCH 702\n",
            "2021-02-01 10:18:55,220 - INFO - joeynmt.training - Epoch 702: total training loss 12.09\n",
            "2021-02-01 10:18:55,220 - INFO - joeynmt.training - EPOCH 703\n",
            "2021-02-01 10:19:01,722 - INFO - joeynmt.training - Epoch 703, Step:    10500, Batch Loss:     0.779061, Tokens per Sec:    18013, Lr: 0.000147\n",
            "2021-02-01 10:19:28,704 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:19:28,705 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:19:28,705 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:19:28,705 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін суға жіберіп қаладан қуып шығарды. Ол мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:19:28,705 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - \tHypothesis: Жындардың түскен дәуір ағашқа шегеленіп, Оны байлап ап келген еді.\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:19:28,706 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - \tHypothesis: Ал бүкіл әлемді ескі күнәкар адамдарға арнап құрбандық ретінде ұсынған Ізгі хабардың үстінде барған әрбір рет бөлмей көрдім. Олар қатты ашуланып, Құдайды мадақтап жүрмедім. Оның мәнінде әйелдерінде тұрған, алтын шам болмағанды көрдім.\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене орнына орналасқан кісінің оң жағындағы құрметті орнына жайғасты. Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 10:19:28,707 - INFO - joeynmt.training - Validation result (greedy) at epoch 703, step    10500: bleu:   4.57, loss: 86732.5391, ppl:  74.8979, duration: 26.9845s\n",
            "2021-02-01 10:19:34,478 - INFO - joeynmt.training - Epoch 703: total training loss 12.16\n",
            "2021-02-01 10:19:34,478 - INFO - joeynmt.training - EPOCH 704\n",
            "2021-02-01 10:19:46,744 - INFO - joeynmt.training - Epoch 704: total training loss 12.14\n",
            "2021-02-01 10:19:46,744 - INFO - joeynmt.training - EPOCH 705\n",
            "2021-02-01 10:19:59,256 - INFO - joeynmt.training - Epoch 705: total training loss 12.11\n",
            "2021-02-01 10:19:59,257 - INFO - joeynmt.training - EPOCH 706\n",
            "2021-02-01 10:20:11,678 - INFO - joeynmt.training - Epoch 706: total training loss 12.10\n",
            "2021-02-01 10:20:11,678 - INFO - joeynmt.training - EPOCH 707\n",
            "2021-02-01 10:20:23,993 - INFO - joeynmt.training - Epoch 707: total training loss 12.02\n",
            "2021-02-01 10:20:23,993 - INFO - joeynmt.training - EPOCH 708\n",
            "2021-02-01 10:20:36,291 - INFO - joeynmt.training - Epoch 708: total training loss 12.03\n",
            "2021-02-01 10:20:36,292 - INFO - joeynmt.training - EPOCH 709\n",
            "2021-02-01 10:20:48,625 - INFO - joeynmt.training - Epoch 709: total training loss 12.07\n",
            "2021-02-01 10:20:48,625 - INFO - joeynmt.training - EPOCH 710\n",
            "2021-02-01 10:20:51,103 - INFO - joeynmt.training - Epoch 710, Step:    10600, Batch Loss:     0.864892, Tokens per Sec:    18110, Lr: 0.000147\n",
            "2021-02-01 10:21:00,958 - INFO - joeynmt.training - Epoch 710: total training loss 12.14\n",
            "2021-02-01 10:21:00,958 - INFO - joeynmt.training - EPOCH 711\n",
            "2021-02-01 10:21:13,282 - INFO - joeynmt.training - Epoch 711: total training loss 12.03\n",
            "2021-02-01 10:21:13,282 - INFO - joeynmt.training - EPOCH 712\n",
            "2021-02-01 10:21:25,629 - INFO - joeynmt.training - Epoch 712: total training loss 12.05\n",
            "2021-02-01 10:21:25,629 - INFO - joeynmt.training - EPOCH 713\n",
            "2021-02-01 10:21:37,959 - INFO - joeynmt.training - Epoch 713: total training loss 12.05\n",
            "2021-02-01 10:21:37,960 - INFO - joeynmt.training - EPOCH 714\n",
            "2021-02-01 10:21:50,265 - INFO - joeynmt.training - Epoch 714: total training loss 11.98\n",
            "2021-02-01 10:21:50,265 - INFO - joeynmt.training - EPOCH 715\n",
            "2021-02-01 10:22:02,625 - INFO - joeynmt.training - Epoch 715: total training loss 12.01\n",
            "2021-02-01 10:22:02,626 - INFO - joeynmt.training - EPOCH 716\n",
            "2021-02-01 10:22:13,196 - INFO - joeynmt.training - Epoch 716, Step:    10700, Batch Loss:     0.831341, Tokens per Sec:    18683, Lr: 0.000147\n",
            "2021-02-01 10:22:14,891 - INFO - joeynmt.training - Epoch 716: total training loss 12.08\n",
            "2021-02-01 10:22:14,891 - INFO - joeynmt.training - EPOCH 717\n",
            "2021-02-01 10:22:27,102 - INFO - joeynmt.training - Epoch 717: total training loss 12.06\n",
            "2021-02-01 10:22:27,102 - INFO - joeynmt.training - EPOCH 718\n",
            "2021-02-01 10:22:39,458 - INFO - joeynmt.training - Epoch 718: total training loss 11.95\n",
            "2021-02-01 10:22:39,459 - INFO - joeynmt.training - EPOCH 719\n",
            "2021-02-01 10:22:51,683 - INFO - joeynmt.training - Epoch 719: total training loss 12.06\n",
            "2021-02-01 10:22:51,683 - INFO - joeynmt.training - EPOCH 720\n",
            "2021-02-01 10:23:03,969 - INFO - joeynmt.training - Epoch 720: total training loss 11.94\n",
            "2021-02-01 10:23:03,969 - INFO - joeynmt.training - EPOCH 721\n",
            "2021-02-01 10:23:16,298 - INFO - joeynmt.training - Epoch 721: total training loss 12.01\n",
            "2021-02-01 10:23:16,299 - INFO - joeynmt.training - EPOCH 722\n",
            "2021-02-01 10:23:28,702 - INFO - joeynmt.training - Epoch 722: total training loss 11.96\n",
            "2021-02-01 10:23:28,702 - INFO - joeynmt.training - EPOCH 723\n",
            "2021-02-01 10:23:35,150 - INFO - joeynmt.training - Epoch 723, Step:    10800, Batch Loss:     0.763258, Tokens per Sec:    18730, Lr: 0.000147\n",
            "2021-02-01 10:23:40,936 - INFO - joeynmt.training - Epoch 723: total training loss 11.95\n",
            "2021-02-01 10:23:40,937 - INFO - joeynmt.training - EPOCH 724\n",
            "2021-02-01 10:23:53,373 - INFO - joeynmt.training - Epoch 724: total training loss 12.01\n",
            "2021-02-01 10:23:53,374 - INFO - joeynmt.training - EPOCH 725\n",
            "2021-02-01 10:24:05,821 - INFO - joeynmt.training - Epoch 725: total training loss 11.98\n",
            "2021-02-01 10:24:05,821 - INFO - joeynmt.training - EPOCH 726\n",
            "2021-02-01 10:24:18,278 - INFO - joeynmt.training - Epoch 726: total training loss 11.86\n",
            "2021-02-01 10:24:18,278 - INFO - joeynmt.training - EPOCH 727\n",
            "2021-02-01 10:24:30,615 - INFO - joeynmt.training - Epoch 727: total training loss 11.87\n",
            "2021-02-01 10:24:30,615 - INFO - joeynmt.training - EPOCH 728\n",
            "2021-02-01 10:24:42,887 - INFO - joeynmt.training - Epoch 728: total training loss 11.97\n",
            "2021-02-01 10:24:42,888 - INFO - joeynmt.training - EPOCH 729\n",
            "2021-02-01 10:24:55,164 - INFO - joeynmt.training - Epoch 729: total training loss 11.96\n",
            "2021-02-01 10:24:55,164 - INFO - joeynmt.training - EPOCH 730\n",
            "2021-02-01 10:24:57,616 - INFO - joeynmt.training - Epoch 730, Step:    10900, Batch Loss:     0.829282, Tokens per Sec:    18672, Lr: 0.000147\n",
            "2021-02-01 10:25:07,562 - INFO - joeynmt.training - Epoch 730: total training loss 11.90\n",
            "2021-02-01 10:25:07,562 - INFO - joeynmt.training - EPOCH 731\n",
            "2021-02-01 10:25:19,974 - INFO - joeynmt.training - Epoch 731: total training loss 11.89\n",
            "2021-02-01 10:25:19,974 - INFO - joeynmt.training - EPOCH 732\n",
            "2021-02-01 10:25:32,226 - INFO - joeynmt.training - Epoch 732: total training loss 11.87\n",
            "2021-02-01 10:25:32,226 - INFO - joeynmt.training - EPOCH 733\n",
            "2021-02-01 10:25:44,624 - INFO - joeynmt.training - Epoch 733: total training loss 11.86\n",
            "2021-02-01 10:25:44,624 - INFO - joeynmt.training - EPOCH 734\n",
            "2021-02-01 10:25:56,939 - INFO - joeynmt.training - Epoch 734: total training loss 11.92\n",
            "2021-02-01 10:25:56,940 - INFO - joeynmt.training - EPOCH 735\n",
            "2021-02-01 10:26:09,250 - INFO - joeynmt.training - Epoch 735: total training loss 11.91\n",
            "2021-02-01 10:26:09,251 - INFO - joeynmt.training - EPOCH 736\n",
            "2021-02-01 10:26:19,745 - INFO - joeynmt.training - Epoch 736, Step:    11000, Batch Loss:     0.829537, Tokens per Sec:    18669, Lr: 0.000147\n",
            "2021-02-01 10:26:46,397 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:26:46,398 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып мінажат ету※ мәжілісханаға барды.\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:26:46,399 - INFO - joeynmt.training - \tHypothesis: Жындардың түскен дәуір ағашқа шегеленген кісі оны Исаның мәйітін деп аңғап алды.\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл әлемді ескі күнәкар болмыстарына беріліп, бас тартқан Құдай оны қайтадан тапта көрдім. Ол Тоқтының үстінде отырған Құдайды мадақтай тілі айды деп тапты; оның нештеңе тілінде қатты ашулана берілді.\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:26:46,400 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:26:46,401 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:26:46,401 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене орнына кендерің көкке көтермеңдер. Сол жағындағы жер бетіндегі барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 10:26:46,401 - INFO - joeynmt.training - Validation result (greedy) at epoch 736, step    11000: bleu:   4.57, loss: 86626.7188, ppl:  74.5045, duration: 26.6552s\n",
            "2021-02-01 10:26:48,220 - INFO - joeynmt.training - Epoch 736: total training loss 11.85\n",
            "2021-02-01 10:26:48,221 - INFO - joeynmt.training - EPOCH 737\n",
            "2021-02-01 10:27:00,630 - INFO - joeynmt.training - Epoch 737: total training loss 11.76\n",
            "2021-02-01 10:27:00,630 - INFO - joeynmt.training - EPOCH 738\n",
            "2021-02-01 10:27:12,874 - INFO - joeynmt.training - Epoch 738: total training loss 11.90\n",
            "2021-02-01 10:27:12,875 - INFO - joeynmt.training - EPOCH 739\n",
            "2021-02-01 10:27:24,983 - INFO - joeynmt.training - Epoch 739: total training loss 11.87\n",
            "2021-02-01 10:27:24,984 - INFO - joeynmt.training - EPOCH 740\n",
            "2021-02-01 10:27:37,409 - INFO - joeynmt.training - Epoch 740: total training loss 11.90\n",
            "2021-02-01 10:27:37,410 - INFO - joeynmt.training - EPOCH 741\n",
            "2021-02-01 10:27:49,797 - INFO - joeynmt.training - Epoch 741: total training loss 11.83\n",
            "2021-02-01 10:27:49,797 - INFO - joeynmt.training - EPOCH 742\n",
            "2021-02-01 10:28:02,250 - INFO - joeynmt.training - Epoch 742: total training loss 11.83\n",
            "2021-02-01 10:28:02,250 - INFO - joeynmt.training - EPOCH 743\n",
            "2021-02-01 10:28:08,732 - INFO - joeynmt.training - Epoch 743, Step:    11100, Batch Loss:     0.804759, Tokens per Sec:    17761, Lr: 0.000147\n",
            "2021-02-01 10:28:14,752 - INFO - joeynmt.training - Epoch 743: total training loss 11.85\n",
            "2021-02-01 10:28:14,753 - INFO - joeynmt.training - EPOCH 744\n",
            "2021-02-01 10:28:27,050 - INFO - joeynmt.training - Epoch 744: total training loss 11.77\n",
            "2021-02-01 10:28:27,050 - INFO - joeynmt.training - EPOCH 745\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 10:28:39,383 - INFO - joeynmt.training - Epoch 745: total training loss 11.75\n",
            "2021-02-01 10:28:39,384 - INFO - joeynmt.training - EPOCH 746\n",
            "2021-02-01 10:28:51,541 - INFO - joeynmt.training - Epoch 746: total training loss 11.81\n",
            "2021-02-01 10:28:51,541 - INFO - joeynmt.training - EPOCH 747\n",
            "2021-02-01 10:29:03,925 - INFO - joeynmt.training - Epoch 747: total training loss 11.74\n",
            "2021-02-01 10:29:03,925 - INFO - joeynmt.training - EPOCH 748\n",
            "2021-02-01 10:29:16,198 - INFO - joeynmt.training - Epoch 748: total training loss 11.79\n",
            "2021-02-01 10:29:16,198 - INFO - joeynmt.training - EPOCH 749\n",
            "2021-02-01 10:29:28,501 - INFO - joeynmt.training - Epoch 749: total training loss 11.80\n",
            "2021-02-01 10:29:28,502 - INFO - joeynmt.training - EPOCH 750\n",
            "2021-02-01 10:29:30,898 - INFO - joeynmt.training - Epoch 750, Step:    11200, Batch Loss:     0.734887, Tokens per Sec:    17497, Lr: 0.000147\n",
            "2021-02-01 10:29:40,675 - INFO - joeynmt.training - Epoch 750: total training loss 11.78\n",
            "2021-02-01 10:29:40,675 - INFO - joeynmt.training - EPOCH 751\n",
            "2021-02-01 10:29:53,052 - INFO - joeynmt.training - Epoch 751: total training loss 11.72\n",
            "2021-02-01 10:29:53,052 - INFO - joeynmt.training - EPOCH 752\n",
            "2021-02-01 10:30:05,290 - INFO - joeynmt.training - Epoch 752: total training loss 11.81\n",
            "2021-02-01 10:30:05,290 - INFO - joeynmt.training - EPOCH 753\n",
            "2021-02-01 10:30:17,486 - INFO - joeynmt.training - Epoch 753: total training loss 11.80\n",
            "2021-02-01 10:30:17,487 - INFO - joeynmt.training - EPOCH 754\n",
            "2021-02-01 10:30:29,638 - INFO - joeynmt.training - Epoch 754: total training loss 11.74\n",
            "2021-02-01 10:30:29,639 - INFO - joeynmt.training - EPOCH 755\n",
            "2021-02-01 10:30:42,013 - INFO - joeynmt.training - Epoch 755: total training loss 11.79\n",
            "2021-02-01 10:30:42,013 - INFO - joeynmt.training - EPOCH 756\n",
            "2021-02-01 10:30:52,581 - INFO - joeynmt.training - Epoch 756, Step:    11300, Batch Loss:     0.768134, Tokens per Sec:    18341, Lr: 0.000147\n",
            "2021-02-01 10:30:54,510 - INFO - joeynmt.training - Epoch 756: total training loss 11.74\n",
            "2021-02-01 10:30:54,510 - INFO - joeynmt.training - EPOCH 757\n",
            "2021-02-01 10:31:07,003 - INFO - joeynmt.training - Epoch 757: total training loss 11.72\n",
            "2021-02-01 10:31:07,004 - INFO - joeynmt.training - EPOCH 758\n",
            "2021-02-01 10:31:19,289 - INFO - joeynmt.training - Epoch 758: total training loss 11.75\n",
            "2021-02-01 10:31:19,289 - INFO - joeynmt.training - EPOCH 759\n",
            "2021-02-01 10:31:31,665 - INFO - joeynmt.training - Epoch 759: total training loss 11.74\n",
            "2021-02-01 10:31:31,666 - INFO - joeynmt.training - EPOCH 760\n",
            "2021-02-01 10:31:44,218 - INFO - joeynmt.training - Epoch 760: total training loss 11.63\n",
            "2021-02-01 10:31:44,218 - INFO - joeynmt.training - EPOCH 761\n",
            "2021-02-01 10:31:56,445 - INFO - joeynmt.training - Epoch 761: total training loss 11.73\n",
            "2021-02-01 10:31:56,445 - INFO - joeynmt.training - EPOCH 762\n",
            "2021-02-01 10:32:08,759 - INFO - joeynmt.training - Epoch 762: total training loss 11.65\n",
            "2021-02-01 10:32:08,759 - INFO - joeynmt.training - EPOCH 763\n",
            "2021-02-01 10:32:15,313 - INFO - joeynmt.training - Epoch 763, Step:    11400, Batch Loss:     0.801476, Tokens per Sec:    19086, Lr: 0.000147\n",
            "2021-02-01 10:32:21,107 - INFO - joeynmt.training - Epoch 763: total training loss 11.70\n",
            "2021-02-01 10:32:21,107 - INFO - joeynmt.training - EPOCH 764\n",
            "2021-02-01 10:32:33,391 - INFO - joeynmt.training - Epoch 764: total training loss 11.74\n",
            "2021-02-01 10:32:33,391 - INFO - joeynmt.training - EPOCH 765\n",
            "2021-02-01 10:32:45,605 - INFO - joeynmt.training - Epoch 765: total training loss 11.68\n",
            "2021-02-01 10:32:45,605 - INFO - joeynmt.training - EPOCH 766\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 10:32:57,868 - INFO - joeynmt.training - Epoch 766: total training loss 11.58\n",
            "2021-02-01 10:32:57,869 - INFO - joeynmt.training - EPOCH 767\n",
            "2021-02-01 10:33:09,999 - INFO - joeynmt.training - Epoch 767: total training loss 10.89\n",
            "2021-02-01 10:33:10,000 - INFO - joeynmt.training - EPOCH 768\n",
            "2021-02-01 10:33:22,260 - INFO - joeynmt.training - Epoch 768: total training loss 11.66\n",
            "2021-02-01 10:33:22,260 - INFO - joeynmt.training - EPOCH 769\n",
            "2021-02-01 10:33:34,551 - INFO - joeynmt.training - Epoch 769: total training loss 11.65\n",
            "2021-02-01 10:33:34,552 - INFO - joeynmt.training - EPOCH 770\n",
            "2021-02-01 10:33:37,821 - INFO - joeynmt.training - Epoch 770, Step:    11500, Batch Loss:     0.785006, Tokens per Sec:    17401, Lr: 0.000147\n",
            "2021-02-01 10:34:04,770 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:34:04,771 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:34:04,771 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:34:04,771 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып мінажат ету※ мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:34:04,771 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:34:04,772 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:34:04,772 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:34:04,772 - INFO - joeynmt.training - \tHypothesis: Жындардың түскен дәуір ағашқа шегелеп іліп Киелі жазбалардың мәйітін деп аңғы түскен соң,\n",
            "2021-02-01 10:34:04,772 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:34:04,773 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:34:04,773 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:34:04,773 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқан Ізгі хабарды түгел айдаған оған ілестеп жіберіп, Оның есімі бар қылған басқа халықтарға қатты аштыр екен. Олар Құдайды мадақтап күтіп тұрды.※\n",
            "2021-02-01 10:34:04,773 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:34:04,774 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:34:04,774 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:34:04,774 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене орнына кендерің көкке көтермеді! Сол жағындағы жер бетіндегі барлық жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 10:34:04,774 - INFO - joeynmt.training - Validation result (greedy) at epoch 770, step    11500: bleu:   4.75, loss: 86929.8594, ppl:  75.6369, duration: 26.9531s\n",
            "2021-02-01 10:34:13,933 - INFO - joeynmt.training - Epoch 770: total training loss 11.61\n",
            "2021-02-01 10:34:13,933 - INFO - joeynmt.training - EPOCH 771\n",
            "2021-02-01 10:34:26,320 - INFO - joeynmt.training - Epoch 771: total training loss 11.60\n",
            "2021-02-01 10:34:26,320 - INFO - joeynmt.training - EPOCH 772\n",
            "2021-02-01 10:34:38,721 - INFO - joeynmt.training - Epoch 772: total training loss 11.60\n",
            "2021-02-01 10:34:38,722 - INFO - joeynmt.training - EPOCH 773\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:34:51,047 - INFO - joeynmt.training - Epoch 773: total training loss 11.58\n",
            "2021-02-01 10:34:51,047 - INFO - joeynmt.training - EPOCH 774\n",
            "2021-02-01 10:35:03,465 - INFO - joeynmt.training - Epoch 774: total training loss 11.62\n",
            "2021-02-01 10:35:03,466 - INFO - joeynmt.training - EPOCH 775\n",
            "2021-02-01 10:35:15,738 - INFO - joeynmt.training - Epoch 775: total training loss 11.56\n",
            "2021-02-01 10:35:15,739 - INFO - joeynmt.training - EPOCH 776\n",
            "2021-02-01 10:35:27,198 - INFO - joeynmt.training - Epoch 776, Step:    11600, Batch Loss:     0.798287, Tokens per Sec:    18578, Lr: 0.000147\n",
            "2021-02-01 10:35:28,104 - INFO - joeynmt.training - Epoch 776: total training loss 11.66\n",
            "2021-02-01 10:35:28,104 - INFO - joeynmt.training - EPOCH 777\n",
            "2021-02-01 10:35:40,584 - INFO - joeynmt.training - Epoch 777: total training loss 11.56\n",
            "2021-02-01 10:35:40,584 - INFO - joeynmt.training - EPOCH 778\n",
            "2021-02-01 10:35:53,089 - INFO - joeynmt.training - Epoch 778: total training loss 11.61\n",
            "2021-02-01 10:35:53,089 - INFO - joeynmt.training - EPOCH 779\n",
            "2021-02-01 10:36:05,149 - INFO - joeynmt.training - Epoch 779: total training loss 10.87\n",
            "2021-02-01 10:36:05,149 - INFO - joeynmt.training - EPOCH 780\n",
            "2021-02-01 10:36:17,442 - INFO - joeynmt.training - Epoch 780: total training loss 11.61\n",
            "2021-02-01 10:36:17,442 - INFO - joeynmt.training - EPOCH 781\n",
            "2021-02-01 10:36:29,696 - INFO - joeynmt.training - Epoch 781: total training loss 11.58\n",
            "2021-02-01 10:36:29,697 - INFO - joeynmt.training - EPOCH 782\n",
            "2021-02-01 10:36:42,090 - INFO - joeynmt.training - Epoch 782: total training loss 11.55\n",
            "2021-02-01 10:36:42,090 - INFO - joeynmt.training - EPOCH 783\n",
            "2021-02-01 10:36:50,148 - INFO - joeynmt.training - Epoch 783, Step:    11700, Batch Loss:     0.709488, Tokens per Sec:    18369, Lr: 0.000147\n",
            "2021-02-01 10:36:54,500 - INFO - joeynmt.training - Epoch 783: total training loss 11.50\n",
            "2021-02-01 10:36:54,500 - INFO - joeynmt.training - EPOCH 784\n",
            "2021-02-01 10:37:06,748 - INFO - joeynmt.training - Epoch 784: total training loss 11.57\n",
            "2021-02-01 10:37:06,748 - INFO - joeynmt.training - EPOCH 785\n",
            "2021-02-01 10:37:19,060 - INFO - joeynmt.training - Epoch 785: total training loss 11.48\n",
            "2021-02-01 10:37:19,060 - INFO - joeynmt.training - EPOCH 786\n",
            "2021-02-01 10:37:31,509 - INFO - joeynmt.training - Epoch 786: total training loss 11.53\n",
            "2021-02-01 10:37:31,509 - INFO - joeynmt.training - EPOCH 787\n",
            "2021-02-01 10:37:43,905 - INFO - joeynmt.training - Epoch 787: total training loss 11.59\n",
            "2021-02-01 10:37:43,905 - INFO - joeynmt.training - EPOCH 788\n",
            "2021-02-01 10:37:56,092 - INFO - joeynmt.training - Epoch 788: total training loss 11.53\n",
            "2021-02-01 10:37:56,092 - INFO - joeynmt.training - EPOCH 789\n",
            "2021-02-01 10:38:08,460 - INFO - joeynmt.training - Epoch 789: total training loss 11.51\n",
            "2021-02-01 10:38:08,460 - INFO - joeynmt.training - EPOCH 790\n",
            "2021-02-01 10:38:12,594 - INFO - joeynmt.training - Epoch 790, Step:    11800, Batch Loss:     0.761712, Tokens per Sec:    18910, Lr: 0.000147\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:38:20,648 - INFO - joeynmt.training - Epoch 790: total training loss 11.47\n",
            "2021-02-01 10:38:20,648 - INFO - joeynmt.training - EPOCH 791\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 10:38:32,978 - INFO - joeynmt.training - Epoch 791: total training loss 11.42\n",
            "2021-02-01 10:38:32,978 - INFO - joeynmt.training - EPOCH 792\n",
            "2021-02-01 10:38:45,197 - INFO - joeynmt.training - Epoch 792: total training loss 11.48\n",
            "2021-02-01 10:38:45,198 - INFO - joeynmt.training - EPOCH 793\n",
            "2021-02-01 10:38:57,473 - INFO - joeynmt.training - Epoch 793: total training loss 11.45\n",
            "2021-02-01 10:38:57,473 - INFO - joeynmt.training - EPOCH 794\n",
            "2021-02-01 10:39:09,831 - INFO - joeynmt.training - Epoch 794: total training loss 11.48\n",
            "2021-02-01 10:39:09,831 - INFO - joeynmt.training - EPOCH 795\n",
            "2021-02-01 10:39:22,178 - INFO - joeynmt.training - Epoch 795: total training loss 11.45\n",
            "2021-02-01 10:39:22,178 - INFO - joeynmt.training - EPOCH 796\n",
            "2021-02-01 10:39:34,317 - INFO - joeynmt.training - Epoch 796: total training loss 10.73\n",
            "2021-02-01 10:39:34,317 - INFO - joeynmt.training - EPOCH 797\n",
            "2021-02-01 10:39:35,148 - INFO - joeynmt.training - Epoch 797, Step:    11900, Batch Loss:     0.768602, Tokens per Sec:    16881, Lr: 0.000147\n",
            "2021-02-01 10:39:46,619 - INFO - joeynmt.training - Epoch 797: total training loss 11.49\n",
            "2021-02-01 10:39:46,620 - INFO - joeynmt.training - EPOCH 798\n",
            "2021-02-01 10:39:58,965 - INFO - joeynmt.training - Epoch 798: total training loss 11.45\n",
            "2021-02-01 10:39:58,965 - INFO - joeynmt.training - EPOCH 799\n",
            "2021-02-01 10:40:11,231 - INFO - joeynmt.training - Epoch 799: total training loss 11.43\n",
            "2021-02-01 10:40:11,232 - INFO - joeynmt.training - EPOCH 800\n",
            "2021-02-01 10:40:23,505 - INFO - joeynmt.training - Epoch 800: total training loss 11.45\n",
            "2021-02-01 10:40:23,505 - INFO - joeynmt.training - EPOCH 801\n",
            "2021-02-01 10:40:35,729 - INFO - joeynmt.training - Epoch 801: total training loss 11.46\n",
            "2021-02-01 10:40:35,730 - INFO - joeynmt.training - EPOCH 802\n",
            "2021-02-01 10:40:47,984 - INFO - joeynmt.training - Epoch 802: total training loss 11.37\n",
            "2021-02-01 10:40:47,984 - INFO - joeynmt.training - EPOCH 803\n",
            "2021-02-01 10:40:56,960 - INFO - joeynmt.training - Epoch 803, Step:    12000, Batch Loss:     0.798698, Tokens per Sec:    18506, Lr: 0.000147\n",
            "2021-02-01 10:41:26,682 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:41:26,683 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:41:26,683 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:41:26,683 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып жіберу үшін мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:41:26,683 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:41:26,684 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:41:26,684 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:41:26,684 - INFO - joeynmt.training - \tHypothesis: Жасақшылар Исаны түнектен ұстап алғанда, ол есімді кісі өлтіруге мәжбүр деп шешті.\n",
            "2021-02-01 10:41:26,684 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:41:26,685 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:41:26,685 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:41:26,685 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, Құдайдың хабарын жеткізіп, бас тарту үшін бөлігі бар малайды көрдім. Ол қатты ашуланған Құдайды қастерлейтін дүниеде көрінді. Олар қатты аштыр екен. Үшеуі бар Құдайды балағаттады көрмеді.\n",
            "2021-02-01 10:41:26,685 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:41:26,686 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:41:26,686 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:41:26,686 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермеңдер, енді Оның оң жағындағы құрметті орнына жайғасқан кезде Мәсіх деген құрметті орнында отыр.※\n",
            "2021-02-01 10:41:26,686 - INFO - joeynmt.training - Validation result (greedy) at epoch 803, step    12000: bleu:   4.74, loss: 86966.7656, ppl:  75.7760, duration: 29.7262s\n",
            "2021-02-01 10:41:30,104 - INFO - joeynmt.training - Epoch 803: total training loss 11.45\n",
            "2021-02-01 10:41:30,104 - INFO - joeynmt.training - EPOCH 804\n",
            "2021-02-01 10:41:42,214 - INFO - joeynmt.training - Epoch 804: total training loss 10.64\n",
            "2021-02-01 10:41:42,214 - INFO - joeynmt.training - EPOCH 805\n",
            "2021-02-01 10:41:54,615 - INFO - joeynmt.training - Epoch 805: total training loss 11.33\n",
            "2021-02-01 10:41:54,616 - INFO - joeynmt.training - EPOCH 806\n",
            "2021-02-01 10:42:06,835 - INFO - joeynmt.training - Epoch 806: total training loss 11.44\n",
            "2021-02-01 10:42:06,836 - INFO - joeynmt.training - EPOCH 807\n",
            "2021-02-01 10:42:19,185 - INFO - joeynmt.training - Epoch 807: total training loss 11.31\n",
            "2021-02-01 10:42:19,185 - INFO - joeynmt.training - EPOCH 808\n",
            "2021-02-01 10:42:31,648 - INFO - joeynmt.training - Epoch 808: total training loss 11.40\n",
            "2021-02-01 10:42:31,648 - INFO - joeynmt.training - EPOCH 809\n",
            "2021-02-01 10:42:43,890 - INFO - joeynmt.training - Epoch 809: total training loss 11.41\n",
            "2021-02-01 10:42:43,890 - INFO - joeynmt.training - EPOCH 810\n",
            "2021-02-01 10:42:49,609 - INFO - joeynmt.training - Epoch 810, Step:    12100, Batch Loss:     0.770828, Tokens per Sec:    18588, Lr: 0.000147\n",
            "2021-02-01 10:42:56,301 - INFO - joeynmt.training - Epoch 810: total training loss 11.41\n",
            "2021-02-01 10:42:56,301 - INFO - joeynmt.training - EPOCH 811\n",
            "2021-02-01 10:43:08,783 - INFO - joeynmt.training - Epoch 811: total training loss 11.36\n",
            "2021-02-01 10:43:08,784 - INFO - joeynmt.training - EPOCH 812\n",
            "2021-02-01 10:43:21,171 - INFO - joeynmt.training - Epoch 812: total training loss 11.34\n",
            "2021-02-01 10:43:21,172 - INFO - joeynmt.training - EPOCH 813\n",
            "2021-02-01 10:43:33,420 - INFO - joeynmt.training - Epoch 813: total training loss 11.35\n",
            "2021-02-01 10:43:33,420 - INFO - joeynmt.training - EPOCH 814\n",
            "2021-02-01 10:43:45,709 - INFO - joeynmt.training - Epoch 814: total training loss 11.34\n",
            "2021-02-01 10:43:45,710 - INFO - joeynmt.training - EPOCH 815\n",
            "2021-02-01 10:43:57,876 - INFO - joeynmt.training - Epoch 815: total training loss 11.34\n",
            "2021-02-01 10:43:57,876 - INFO - joeynmt.training - EPOCH 816\n",
            "2021-02-01 10:44:10,173 - INFO - joeynmt.training - Epoch 816: total training loss 11.29\n",
            "2021-02-01 10:44:10,173 - INFO - joeynmt.training - EPOCH 817\n",
            "2021-02-01 10:44:11,839 - INFO - joeynmt.training - Epoch 817, Step:    12200, Batch Loss:     0.814372, Tokens per Sec:    18821, Lr: 0.000147\n",
            "2021-02-01 10:44:22,579 - INFO - joeynmt.training - Epoch 817: total training loss 11.32\n",
            "2021-02-01 10:44:22,579 - INFO - joeynmt.training - EPOCH 818\n",
            "2021-02-01 10:44:35,037 - INFO - joeynmt.training - Epoch 818: total training loss 11.30\n",
            "2021-02-01 10:44:35,038 - INFO - joeynmt.training - EPOCH 819\n",
            "2021-02-01 10:44:47,316 - INFO - joeynmt.training - Epoch 819: total training loss 11.36\n",
            "2021-02-01 10:44:47,316 - INFO - joeynmt.training - EPOCH 820\n",
            "2021-02-01 10:44:59,586 - INFO - joeynmt.training - Epoch 820: total training loss 11.35\n",
            "2021-02-01 10:44:59,587 - INFO - joeynmt.training - EPOCH 821\n",
            "2021-02-01 10:45:12,086 - INFO - joeynmt.training - Epoch 821: total training loss 11.28\n",
            "2021-02-01 10:45:12,086 - INFO - joeynmt.training - EPOCH 822\n",
            "2021-02-01 10:45:24,162 - INFO - joeynmt.training - Epoch 822: total training loss 10.62\n",
            "2021-02-01 10:45:24,163 - INFO - joeynmt.training - EPOCH 823\n",
            "2021-02-01 10:45:34,760 - INFO - joeynmt.training - Epoch 823, Step:    12300, Batch Loss:     0.775477, Tokens per Sec:    18819, Lr: 0.000147\n",
            "2021-02-01 10:45:36,261 - INFO - joeynmt.training - Epoch 823: total training loss 10.56\n",
            "2021-02-01 10:45:36,261 - INFO - joeynmt.training - EPOCH 824\n",
            "2021-02-01 10:45:48,565 - INFO - joeynmt.training - Epoch 824: total training loss 11.25\n",
            "2021-02-01 10:45:48,566 - INFO - joeynmt.training - EPOCH 825\n",
            "2021-02-01 10:46:00,520 - INFO - joeynmt.training - Epoch 825: total training loss 10.58\n",
            "2021-02-01 10:46:00,521 - INFO - joeynmt.training - EPOCH 826\n",
            "2021-02-01 10:46:12,880 - INFO - joeynmt.training - Epoch 826: total training loss 11.26\n",
            "2021-02-01 10:46:12,881 - INFO - joeynmt.training - EPOCH 827\n",
            "2021-02-01 10:46:25,185 - INFO - joeynmt.training - Epoch 827: total training loss 11.31\n",
            "2021-02-01 10:46:25,186 - INFO - joeynmt.training - EPOCH 828\n",
            "2021-02-01 10:46:37,380 - INFO - joeynmt.training - Epoch 828: total training loss 11.30\n",
            "2021-02-01 10:46:37,380 - INFO - joeynmt.training - EPOCH 829\n",
            "2021-02-01 10:46:50,005 - INFO - joeynmt.training - Epoch 829: total training loss 11.26\n",
            "2021-02-01 10:46:50,005 - INFO - joeynmt.training - EPOCH 830\n",
            "2021-02-01 10:46:58,146 - INFO - joeynmt.training - Epoch 830, Step:    12400, Batch Loss:     0.733561, Tokens per Sec:    18342, Lr: 0.000147\n",
            "2021-02-01 10:47:02,416 - INFO - joeynmt.training - Epoch 830: total training loss 11.21\n",
            "2021-02-01 10:47:02,417 - INFO - joeynmt.training - EPOCH 831\n",
            "2021-02-01 10:47:14,909 - INFO - joeynmt.training - Epoch 831: total training loss 11.26\n",
            "2021-02-01 10:47:14,910 - INFO - joeynmt.training - EPOCH 832\n",
            "2021-02-01 10:47:27,050 - INFO - joeynmt.training - Epoch 832: total training loss 10.53\n",
            "2021-02-01 10:47:27,051 - INFO - joeynmt.training - EPOCH 833\n",
            "2021-02-01 10:47:39,427 - INFO - joeynmt.training - Epoch 833: total training loss 11.24\n",
            "2021-02-01 10:47:39,427 - INFO - joeynmt.training - EPOCH 834\n",
            "2021-02-01 10:47:51,726 - INFO - joeynmt.training - Epoch 834: total training loss 11.24\n",
            "2021-02-01 10:47:51,726 - INFO - joeynmt.training - EPOCH 835\n",
            "2021-02-01 10:48:03,978 - INFO - joeynmt.training - Epoch 835: total training loss 11.19\n",
            "2021-02-01 10:48:03,979 - INFO - joeynmt.training - EPOCH 836\n",
            "2021-02-01 10:48:16,212 - INFO - joeynmt.training - Epoch 836: total training loss 11.25\n",
            "2021-02-01 10:48:16,213 - INFO - joeynmt.training - EPOCH 837\n",
            "2021-02-01 10:48:21,176 - INFO - joeynmt.training - Epoch 837, Step:    12500, Batch Loss:     0.702930, Tokens per Sec:    17833, Lr: 0.000147\n",
            "2021-02-01 10:48:47,653 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:48:47,654 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:48:47,654 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:48:47,654 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға жіберіп отырғанда, сол жерде мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 10:48:47,654 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:48:47,655 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:48:47,655 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:48:47,655 - INFO - joeynmt.training - \tHypothesis: Жасақшылар Исаны көргенде Оны оятты. Сол кісі өлтіруді талап етуге мәжбүр деп шешті.\n",
            "2021-02-01 10:48:47,655 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:48:47,656 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:48:47,656 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:48:47,656 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есіне (яғни адам санай) бас тартқан Ізгі хабарды бұрмалған басқа ешбір сыр етіп жүрдім. Оның сөзін айуанның мәсер көрмей көрмей көрдім.\n",
            "2021-02-01 10:48:47,656 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:48:47,657 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:48:47,657 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:48:47,657 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене орнына кендерің көкке көтермеңдер. Сол жағындағы жер бетіндегі барлық жердің оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 10:48:47,658 - INFO - joeynmt.training - Validation result (greedy) at epoch 837, step    12500: bleu:   4.66, loss: 87160.0859, ppl:  76.5085, duration: 26.4808s\n",
            "2021-02-01 10:48:55,266 - INFO - joeynmt.training - Epoch 837: total training loss 11.30\n",
            "2021-02-01 10:48:55,266 - INFO - joeynmt.training - EPOCH 838\n",
            "2021-02-01 10:49:07,755 - INFO - joeynmt.training - Epoch 838: total training loss 11.20\n",
            "2021-02-01 10:49:07,755 - INFO - joeynmt.training - EPOCH 839\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 10:49:20,221 - INFO - joeynmt.training - Epoch 839: total training loss 11.18\n",
            "2021-02-01 10:49:20,222 - INFO - joeynmt.training - EPOCH 840\n",
            "2021-02-01 10:49:32,620 - INFO - joeynmt.training - Epoch 840: total training loss 11.18\n",
            "2021-02-01 10:49:32,620 - INFO - joeynmt.training - EPOCH 841\n",
            "2021-02-01 10:49:45,094 - INFO - joeynmt.training - Epoch 841: total training loss 11.20\n",
            "2021-02-01 10:49:45,095 - INFO - joeynmt.training - EPOCH 842\n",
            "2021-02-01 10:49:57,529 - INFO - joeynmt.training - Epoch 842: total training loss 11.25\n",
            "2021-02-01 10:49:57,529 - INFO - joeynmt.training - EPOCH 843\n",
            "2021-02-01 10:50:09,745 - INFO - joeynmt.training - Epoch 843: total training loss 10.48\n",
            "2021-02-01 10:50:09,745 - INFO - joeynmt.training - EPOCH 844\n",
            "2021-02-01 10:50:11,454 - INFO - joeynmt.training - Epoch 844, Step:    12600, Batch Loss:     0.770360, Tokens per Sec:    18596, Lr: 0.000147\n",
            "2021-02-01 10:50:22,060 - INFO - joeynmt.training - Epoch 844: total training loss 11.20\n",
            "2021-02-01 10:50:22,060 - INFO - joeynmt.training - EPOCH 845\n",
            "2021-02-01 10:50:34,533 - INFO - joeynmt.training - Epoch 845: total training loss 11.21\n",
            "2021-02-01 10:50:34,534 - INFO - joeynmt.training - EPOCH 846\n",
            "2021-02-01 10:50:46,889 - INFO - joeynmt.training - Epoch 846: total training loss 11.17\n",
            "2021-02-01 10:50:46,889 - INFO - joeynmt.training - EPOCH 847\n",
            "2021-02-01 10:50:59,481 - INFO - joeynmt.training - Epoch 847: total training loss 11.10\n",
            "2021-02-01 10:50:59,481 - INFO - joeynmt.training - EPOCH 848\n",
            "2021-02-01 10:51:11,850 - INFO - joeynmt.training - Epoch 848: total training loss 11.14\n",
            "2021-02-01 10:51:11,851 - INFO - joeynmt.training - EPOCH 849\n",
            "2021-02-01 10:51:24,137 - INFO - joeynmt.training - Epoch 849: total training loss 11.18\n",
            "2021-02-01 10:51:24,137 - INFO - joeynmt.training - EPOCH 850\n",
            "2021-02-01 10:51:33,903 - INFO - joeynmt.training - Epoch 850, Step:    12700, Batch Loss:     0.722397, Tokens per Sec:    18423, Lr: 0.000147\n",
            "2021-02-01 10:51:36,593 - INFO - joeynmt.training - Epoch 850: total training loss 11.15\n",
            "2021-02-01 10:51:36,594 - INFO - joeynmt.training - EPOCH 851\n",
            "2021-02-01 10:51:49,068 - INFO - joeynmt.training - Epoch 851: total training loss 11.12\n",
            "2021-02-01 10:51:49,068 - INFO - joeynmt.training - EPOCH 852\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 10:52:01,478 - INFO - joeynmt.training - Epoch 852: total training loss 11.13\n",
            "2021-02-01 10:52:01,479 - INFO - joeynmt.training - EPOCH 853\n",
            "2021-02-01 10:52:13,819 - INFO - joeynmt.training - Epoch 853: total training loss 11.10\n",
            "2021-02-01 10:52:13,819 - INFO - joeynmt.training - EPOCH 854\n",
            "2021-02-01 10:52:26,310 - INFO - joeynmt.training - Epoch 854: total training loss 11.12\n",
            "2021-02-01 10:52:26,310 - INFO - joeynmt.training - EPOCH 855\n",
            "2021-02-01 10:52:38,551 - INFO - joeynmt.training - Epoch 855: total training loss 11.08\n",
            "2021-02-01 10:52:38,552 - INFO - joeynmt.training - EPOCH 856\n",
            "2021-02-01 10:52:50,919 - INFO - joeynmt.training - Epoch 856: total training loss 11.09\n",
            "2021-02-01 10:52:50,920 - INFO - joeynmt.training - EPOCH 857\n",
            "2021-02-01 10:52:56,700 - INFO - joeynmt.training - Epoch 857, Step:    12800, Batch Loss:     0.764834, Tokens per Sec:    19005, Lr: 0.000147\n",
            "2021-02-01 10:53:03,184 - INFO - joeynmt.training - Epoch 857: total training loss 11.19\n",
            "2021-02-01 10:53:03,185 - INFO - joeynmt.training - EPOCH 858\n",
            "2021-02-01 10:53:15,628 - INFO - joeynmt.training - Epoch 858: total training loss 11.09\n",
            "2021-02-01 10:53:15,628 - INFO - joeynmt.training - EPOCH 859\n",
            "2021-02-01 10:53:27,925 - INFO - joeynmt.training - Epoch 859: total training loss 11.09\n",
            "2021-02-01 10:53:27,926 - INFO - joeynmt.training - EPOCH 860\n",
            "2021-02-01 10:53:40,437 - INFO - joeynmt.training - Epoch 860: total training loss 11.07\n",
            "2021-02-01 10:53:40,437 - INFO - joeynmt.training - EPOCH 861\n",
            "2021-02-01 10:53:52,695 - INFO - joeynmt.training - Epoch 861: total training loss 11.05\n",
            "2021-02-01 10:53:52,696 - INFO - joeynmt.training - EPOCH 862\n",
            "2021-02-01 10:54:05,068 - INFO - joeynmt.training - Epoch 862: total training loss 11.08\n",
            "2021-02-01 10:54:05,068 - INFO - joeynmt.training - EPOCH 863\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
            "2021-02-01 10:54:17,351 - INFO - joeynmt.training - Epoch 863: total training loss 11.03\n",
            "2021-02-01 10:54:17,352 - INFO - joeynmt.training - EPOCH 864\n",
            "2021-02-01 10:54:18,985 - INFO - joeynmt.training - Epoch 864, Step:    12900, Batch Loss:     0.719467, Tokens per Sec:    17412, Lr: 0.000147\n",
            "2021-02-01 10:54:29,730 - INFO - joeynmt.training - Epoch 864: total training loss 11.04\n",
            "2021-02-01 10:54:29,730 - INFO - joeynmt.training - EPOCH 865\n",
            "2021-02-01 10:54:42,000 - INFO - joeynmt.training - Epoch 865: total training loss 11.05\n",
            "2021-02-01 10:54:42,001 - INFO - joeynmt.training - EPOCH 866\n",
            "2021-02-01 10:54:54,459 - INFO - joeynmt.training - Epoch 866: total training loss 11.04\n",
            "2021-02-01 10:54:54,460 - INFO - joeynmt.training - EPOCH 867\n",
            "2021-02-01 10:55:06,757 - INFO - joeynmt.training - Epoch 867: total training loss 11.04\n",
            "2021-02-01 10:55:06,757 - INFO - joeynmt.training - EPOCH 868\n",
            "2021-02-01 10:55:19,037 - INFO - joeynmt.training - Epoch 868: total training loss 11.07\n",
            "2021-02-01 10:55:19,037 - INFO - joeynmt.training - EPOCH 869\n",
            "2021-02-01 10:55:31,399 - INFO - joeynmt.training - Epoch 869: total training loss 11.03\n",
            "2021-02-01 10:55:31,400 - INFO - joeynmt.training - EPOCH 870\n",
            "2021-02-01 10:55:41,114 - INFO - joeynmt.training - Epoch 870, Step:    13000, Batch Loss:     0.742784, Tokens per Sec:    18449, Lr: 0.000147\n",
            "2021-02-01 10:56:05,561 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 10:56:05,561 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың жетекшісі※ мәжілісханаларына барып, Өзі мәжілісханаларға жіберді.※\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Исаны ұстап беру үшін «Оны байлап тың» деп үміттенді.\n",
            "2021-02-01 10:56:05,562 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен Құдай Рухын жазып жіберіп, бас тарту етпанда Тоқтының үстінде інде інде тұрған, Құдайды көрдім. Олар қатты аштыр айуанның мәсер мен оның тілінде тұрғанын көрдім.\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте көкке көтермеңдер! Сол жағында Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 10:56:05,563 - INFO - joeynmt.training - Validation result (greedy) at epoch 870, step    13000: bleu:   4.82, loss: 87245.6250, ppl:  76.8348, duration: 24.4490s\n",
            "2021-02-01 10:56:08,277 - INFO - joeynmt.training - Epoch 870: total training loss 10.98\n",
            "2021-02-01 10:56:08,277 - INFO - joeynmt.training - EPOCH 871\n",
            "2021-02-01 10:56:20,648 - INFO - joeynmt.training - Epoch 871: total training loss 11.05\n",
            "2021-02-01 10:56:20,648 - INFO - joeynmt.training - EPOCH 872\n",
            "2021-02-01 10:56:32,900 - INFO - joeynmt.training - Epoch 872: total training loss 10.98\n",
            "2021-02-01 10:56:32,901 - INFO - joeynmt.training - EPOCH 873\n",
            "2021-02-01 10:56:45,353 - INFO - joeynmt.training - Epoch 873: total training loss 11.03\n",
            "2021-02-01 10:56:45,353 - INFO - joeynmt.training - EPOCH 874\n",
            "2021-02-01 10:56:57,684 - INFO - joeynmt.training - Epoch 874: total training loss 10.99\n",
            "2021-02-01 10:56:57,684 - INFO - joeynmt.training - EPOCH 875\n",
            "2021-02-01 10:57:09,981 - INFO - joeynmt.training - Epoch 875: total training loss 11.02\n",
            "2021-02-01 10:57:09,981 - INFO - joeynmt.training - EPOCH 876\n",
            "2021-02-01 10:57:22,393 - INFO - joeynmt.training - Epoch 876: total training loss 10.99\n",
            "2021-02-01 10:57:22,394 - INFO - joeynmt.training - EPOCH 877\n",
            "2021-02-01 10:57:28,047 - INFO - joeynmt.training - Epoch 877, Step:    13100, Batch Loss:     0.765379, Tokens per Sec:    18871, Lr: 0.000147\n",
            "2021-02-01 10:57:34,772 - INFO - joeynmt.training - Epoch 877: total training loss 11.03\n",
            "2021-02-01 10:57:34,772 - INFO - joeynmt.training - EPOCH 878\n",
            "2021-02-01 10:57:47,030 - INFO - joeynmt.training - Epoch 878: total training loss 10.96\n",
            "2021-02-01 10:57:47,031 - INFO - joeynmt.training - EPOCH 879\n",
            "2021-02-01 10:57:59,240 - INFO - joeynmt.training - Epoch 879: total training loss 10.96\n",
            "2021-02-01 10:57:59,241 - INFO - joeynmt.training - EPOCH 880\n",
            "2021-02-01 10:58:11,525 - INFO - joeynmt.training - Epoch 880: total training loss 10.93\n",
            "2021-02-01 10:58:11,526 - INFO - joeynmt.training - EPOCH 881\n",
            "2021-02-01 10:58:23,739 - INFO - joeynmt.training - Epoch 881: total training loss 11.03\n",
            "2021-02-01 10:58:23,740 - INFO - joeynmt.training - EPOCH 882\n",
            "2021-02-01 10:58:36,230 - INFO - joeynmt.training - Epoch 882: total training loss 10.97\n",
            "2021-02-01 10:58:36,230 - INFO - joeynmt.training - EPOCH 883\n",
            "2021-02-01 10:58:48,575 - INFO - joeynmt.training - Epoch 883: total training loss 10.96\n",
            "2021-02-01 10:58:48,576 - INFO - joeynmt.training - EPOCH 884\n",
            "2021-02-01 10:58:50,190 - INFO - joeynmt.training - Epoch 884, Step:    13200, Batch Loss:     0.725267, Tokens per Sec:    16665, Lr: 0.000147\n",
            "2021-02-01 10:59:01,105 - INFO - joeynmt.training - Epoch 884: total training loss 11.02\n",
            "2021-02-01 10:59:01,105 - INFO - joeynmt.training - EPOCH 885\n",
            "2021-02-01 10:59:13,460 - INFO - joeynmt.training - Epoch 885: total training loss 10.96\n",
            "2021-02-01 10:59:13,461 - INFO - joeynmt.training - EPOCH 886\n",
            "2021-02-01 10:59:25,748 - INFO - joeynmt.training - Epoch 886: total training loss 10.96\n",
            "2021-02-01 10:59:25,748 - INFO - joeynmt.training - EPOCH 887\n",
            "2021-02-01 10:59:38,099 - INFO - joeynmt.training - Epoch 887: total training loss 10.94\n",
            "2021-02-01 10:59:38,100 - INFO - joeynmt.training - EPOCH 888\n",
            "2021-02-01 10:59:50,321 - INFO - joeynmt.training - Epoch 888: total training loss 10.98\n",
            "2021-02-01 10:59:50,321 - INFO - joeynmt.training - EPOCH 889\n",
            "2021-02-01 11:00:02,596 - INFO - joeynmt.training - Epoch 889: total training loss 10.96\n",
            "2021-02-01 11:00:02,596 - INFO - joeynmt.training - EPOCH 890\n",
            "2021-02-01 11:00:12,345 - INFO - joeynmt.training - Epoch 890, Step:    13300, Batch Loss:     0.723244, Tokens per Sec:    18444, Lr: 0.000147\n",
            "2021-02-01 11:00:15,063 - INFO - joeynmt.training - Epoch 890: total training loss 10.87\n",
            "2021-02-01 11:00:15,063 - INFO - joeynmt.training - EPOCH 891\n",
            "2021-02-01 11:00:27,402 - INFO - joeynmt.training - Epoch 891: total training loss 10.91\n",
            "2021-02-01 11:00:27,403 - INFO - joeynmt.training - EPOCH 892\n",
            "2021-02-01 11:00:39,588 - INFO - joeynmt.training - Epoch 892: total training loss 10.94\n",
            "2021-02-01 11:00:39,589 - INFO - joeynmt.training - EPOCH 893\n",
            "2021-02-01 11:00:51,763 - INFO - joeynmt.training - Epoch 893: total training loss 10.93\n",
            "2021-02-01 11:00:51,763 - INFO - joeynmt.training - EPOCH 894\n",
            "2021-02-01 11:01:03,962 - INFO - joeynmt.training - Epoch 894: total training loss 10.94\n",
            "2021-02-01 11:01:03,962 - INFO - joeynmt.training - EPOCH 895\n",
            "2021-02-01 11:01:16,226 - INFO - joeynmt.training - Epoch 895: total training loss 10.89\n",
            "2021-02-01 11:01:16,227 - INFO - joeynmt.training - EPOCH 896\n",
            "2021-02-01 11:01:28,571 - INFO - joeynmt.training - Epoch 896: total training loss 10.83\n",
            "2021-02-01 11:01:28,571 - INFO - joeynmt.training - EPOCH 897\n",
            "2021-02-01 11:01:34,238 - INFO - joeynmt.training - Epoch 897, Step:    13400, Batch Loss:     0.755599, Tokens per Sec:    18090, Lr: 0.000147\n",
            "2021-02-01 11:01:40,990 - INFO - joeynmt.training - Epoch 897: total training loss 10.88\n",
            "2021-02-01 11:01:40,991 - INFO - joeynmt.training - EPOCH 898\n",
            "2021-02-01 11:01:53,349 - INFO - joeynmt.training - Epoch 898: total training loss 10.90\n",
            "2021-02-01 11:01:53,349 - INFO - joeynmt.training - EPOCH 899\n",
            "2021-02-01 11:02:05,506 - INFO - joeynmt.training - Epoch 899: total training loss 10.91\n",
            "2021-02-01 11:02:05,506 - INFO - joeynmt.training - EPOCH 900\n",
            "2021-02-01 11:02:17,782 - INFO - joeynmt.training - Epoch 900: total training loss 10.86\n",
            "2021-02-01 11:02:17,782 - INFO - joeynmt.training - EPOCH 901\n",
            "2021-02-01 11:02:30,118 - INFO - joeynmt.training - Epoch 901: total training loss 10.86\n",
            "2021-02-01 11:02:30,119 - INFO - joeynmt.training - EPOCH 902\n",
            "2021-02-01 11:02:42,320 - INFO - joeynmt.training - Epoch 902: total training loss 10.89\n",
            "2021-02-01 11:02:42,321 - INFO - joeynmt.training - EPOCH 903\n",
            "2021-02-01 11:02:54,305 - INFO - joeynmt.training - Epoch 903: total training loss 10.15\n",
            "2021-02-01 11:02:54,306 - INFO - joeynmt.training - EPOCH 904\n",
            "2021-02-01 11:02:56,766 - INFO - joeynmt.training - Epoch 904, Step:    13500, Batch Loss:     0.722314, Tokens per Sec:    18448, Lr: 0.000147\n",
            "2021-02-01 11:03:24,904 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:03:24,905 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:03:24,905 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:03:24,905 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың жетекшісі※ мәжілісханаға барды.\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - \tHypothesis: Жындардың түскен дәуірлер Оны оятқа салып, оны ұстап беруге мәжбүр етті.\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:03:24,906 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, Құдайдың хабарын жеткізіп, оған ілескен кен ештеңені көрдім. Ол Құдайды мадақтап шыққан осы күнәкар дүниеде көрмегендерді қатты аштым.\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болды, Оның оң жағындағы жер бетіндегі орналасқан жердің оң жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 11:03:24,907 - INFO - joeynmt.training - Validation result (greedy) at epoch 904, step    13500: bleu:   4.57, loss: 87254.0625, ppl:  76.8671, duration: 28.1408s\n",
            "2021-02-01 11:03:34,664 - INFO - joeynmt.training - Epoch 904: total training loss 10.83\n",
            "2021-02-01 11:03:34,665 - INFO - joeynmt.training - EPOCH 905\n",
            "2021-02-01 11:03:46,974 - INFO - joeynmt.training - Epoch 905: total training loss 10.85\n",
            "2021-02-01 11:03:46,974 - INFO - joeynmt.training - EPOCH 906\n",
            "2021-02-01 11:03:59,163 - INFO - joeynmt.training - Epoch 906: total training loss 10.90\n",
            "2021-02-01 11:03:59,163 - INFO - joeynmt.training - EPOCH 907\n",
            "2021-02-01 11:04:11,506 - INFO - joeynmt.training - Epoch 907: total training loss 10.79\n",
            "2021-02-01 11:04:11,506 - INFO - joeynmt.training - EPOCH 908\n",
            "2021-02-01 11:04:23,640 - INFO - joeynmt.training - Epoch 908: total training loss 10.11\n",
            "2021-02-01 11:04:23,640 - INFO - joeynmt.training - EPOCH 909\n",
            "2021-02-01 11:04:36,018 - INFO - joeynmt.training - Epoch 909: total training loss 10.79\n",
            "2021-02-01 11:04:36,018 - INFO - joeynmt.training - EPOCH 910\n",
            "2021-02-01 11:04:47,303 - INFO - joeynmt.training - Epoch 910, Step:    13600, Batch Loss:     0.732449, Tokens per Sec:    18424, Lr: 0.000147\n",
            "2021-02-01 11:04:48,332 - INFO - joeynmt.training - Epoch 910: total training loss 10.81\n",
            "2021-02-01 11:04:48,333 - INFO - joeynmt.training - EPOCH 911\n",
            "2021-02-01 11:05:00,577 - INFO - joeynmt.training - Epoch 911: total training loss 10.82\n",
            "2021-02-01 11:05:00,577 - INFO - joeynmt.training - EPOCH 912\n",
            "2021-02-01 11:05:13,008 - INFO - joeynmt.training - Epoch 912: total training loss 10.80\n",
            "2021-02-01 11:05:13,008 - INFO - joeynmt.training - EPOCH 913\n",
            "2021-02-01 11:05:25,455 - INFO - joeynmt.training - Epoch 913: total training loss 10.80\n",
            "2021-02-01 11:05:25,455 - INFO - joeynmt.training - EPOCH 914\n",
            "2021-02-01 11:05:37,890 - INFO - joeynmt.training - Epoch 914: total training loss 10.73\n",
            "2021-02-01 11:05:37,891 - INFO - joeynmt.training - EPOCH 915\n",
            "2021-02-01 11:05:50,308 - INFO - joeynmt.training - Epoch 915: total training loss 10.81\n",
            "2021-02-01 11:05:50,308 - INFO - joeynmt.training - EPOCH 916\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:06:02,419 - INFO - joeynmt.training - Epoch 916: total training loss 10.74\n",
            "2021-02-01 11:06:02,419 - INFO - joeynmt.training - EPOCH 917\n",
            "2021-02-01 11:06:09,728 - INFO - joeynmt.training - Epoch 917, Step:    13700, Batch Loss:     0.722047, Tokens per Sec:    18958, Lr: 0.000147\n",
            "2021-02-01 11:06:14,477 - INFO - joeynmt.training - Epoch 917: total training loss 10.08\n",
            "2021-02-01 11:06:14,477 - INFO - joeynmt.training - EPOCH 918\n",
            "2021-02-01 11:06:26,829 - INFO - joeynmt.training - Epoch 918: total training loss 10.79\n",
            "2021-02-01 11:06:26,830 - INFO - joeynmt.training - EPOCH 919\n",
            "2021-02-01 11:06:39,155 - INFO - joeynmt.training - Epoch 919: total training loss 10.72\n",
            "2021-02-01 11:06:39,155 - INFO - joeynmt.training - EPOCH 920\n",
            "2021-02-01 11:06:51,559 - INFO - joeynmt.training - Epoch 920: total training loss 10.69\n",
            "2021-02-01 11:06:51,559 - INFO - joeynmt.training - EPOCH 921\n",
            "2021-02-01 11:07:03,721 - INFO - joeynmt.training - Epoch 921: total training loss 10.78\n",
            "2021-02-01 11:07:03,722 - INFO - joeynmt.training - EPOCH 922\n",
            "2021-02-01 11:07:15,937 - INFO - joeynmt.training - Epoch 922: total training loss 10.78\n",
            "2021-02-01 11:07:15,938 - INFO - joeynmt.training - EPOCH 923\n",
            "2021-02-01 11:07:28,133 - INFO - joeynmt.training - Epoch 923: total training loss 10.76\n",
            "2021-02-01 11:07:28,133 - INFO - joeynmt.training - EPOCH 924\n",
            "2021-02-01 11:07:32,226 - INFO - joeynmt.training - Epoch 924, Step:    13800, Batch Loss:     0.712675, Tokens per Sec:    18935, Lr: 0.000147\n",
            "2021-02-01 11:07:40,530 - INFO - joeynmt.training - Epoch 924: total training loss 10.66\n",
            "2021-02-01 11:07:40,531 - INFO - joeynmt.training - EPOCH 925\n",
            "2021-02-01 11:07:52,709 - INFO - joeynmt.training - Epoch 925: total training loss 10.70\n",
            "2021-02-01 11:07:52,710 - INFO - joeynmt.training - EPOCH 926\n",
            "2021-02-01 11:08:05,029 - INFO - joeynmt.training - Epoch 926: total training loss 10.76\n",
            "2021-02-01 11:08:05,030 - INFO - joeynmt.training - EPOCH 927\n",
            "2021-02-01 11:08:17,315 - INFO - joeynmt.training - Epoch 927: total training loss 10.70\n",
            "2021-02-01 11:08:17,316 - INFO - joeynmt.training - EPOCH 928\n",
            "2021-02-01 11:08:29,541 - INFO - joeynmt.training - Epoch 928: total training loss 10.77\n",
            "2021-02-01 11:08:29,541 - INFO - joeynmt.training - EPOCH 929\n",
            "2021-02-01 11:08:41,925 - INFO - joeynmt.training - Epoch 929: total training loss 10.68\n",
            "2021-02-01 11:08:41,926 - INFO - joeynmt.training - EPOCH 930\n",
            "2021-02-01 11:08:54,009 - INFO - joeynmt.training - Epoch 930, Step:    13900, Batch Loss:     0.729381, Tokens per Sec:    18393, Lr: 0.000147\n",
            "2021-02-01 11:08:54,313 - INFO - joeynmt.training - Epoch 930: total training loss 10.67\n",
            "2021-02-01 11:08:54,314 - INFO - joeynmt.training - EPOCH 931\n",
            "2021-02-01 11:09:06,548 - INFO - joeynmt.training - Epoch 931: total training loss 10.69\n",
            "2021-02-01 11:09:06,548 - INFO - joeynmt.training - EPOCH 932\n",
            "2021-02-01 11:09:18,760 - INFO - joeynmt.training - Epoch 932: total training loss 10.72\n",
            "2021-02-01 11:09:18,760 - INFO - joeynmt.training - EPOCH 933\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:09:31,065 - INFO - joeynmt.training - Epoch 933: total training loss 10.68\n",
            "2021-02-01 11:09:31,065 - INFO - joeynmt.training - EPOCH 934\n",
            "2021-02-01 11:09:43,426 - INFO - joeynmt.training - Epoch 934: total training loss 10.74\n",
            "2021-02-01 11:09:43,427 - INFO - joeynmt.training - EPOCH 935\n",
            "2021-02-01 11:09:55,908 - INFO - joeynmt.training - Epoch 935: total training loss 10.70\n",
            "2021-02-01 11:09:55,908 - INFO - joeynmt.training - EPOCH 936\n",
            "2021-02-01 11:10:08,314 - INFO - joeynmt.training - Epoch 936: total training loss 10.75\n",
            "2021-02-01 11:10:08,314 - INFO - joeynmt.training - EPOCH 937\n",
            "2021-02-01 11:10:16,414 - INFO - joeynmt.training - Epoch 937, Step:    14000, Batch Loss:     0.707252, Tokens per Sec:    18725, Lr: 0.000147\n",
            "2021-02-01 11:10:45,107 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:10:45,108 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:10:45,108 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:10:45,108 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға жіберіп қаладағы мәжілісхана бастады.\n",
            "2021-02-01 11:10:45,108 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:10:45,109 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:10:45,109 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:10:45,109 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу деп санай бастады. Олар Оны өлтіріп,\n",
            "2021-02-01 11:10:45,109 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:10:45,110 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:10:45,110 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:10:45,110 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, Құдайдың хабарын жеткізіп, бас тарту ретінде қырық екі рет енді. Тоқтының үстінде отырған Құдайды қастерлейтін әйелдер мен Құдайды мадақтап күткен: «Тек үш рет тәжінр көрме!\n",
            "2021-02-01 11:10:45,110 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:10:45,111 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:10:45,111 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:10:45,111 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болыңдар: Ол бүкіл жерді оң жағындағы барлық жағындағы құрметті орнына жайғасқан кісінің Иесіне сай орналасқан болады.\n",
            "2021-02-01 11:10:45,111 - INFO - joeynmt.training - Validation result (greedy) at epoch 937, step    14000: bleu:   5.07, loss: 87305.9922, ppl:  77.0660, duration: 28.6971s\n",
            "2021-02-01 11:10:49,265 - INFO - joeynmt.training - Epoch 937: total training loss 10.72\n",
            "2021-02-01 11:10:49,266 - INFO - joeynmt.training - EPOCH 938\n",
            "2021-02-01 11:11:01,610 - INFO - joeynmt.training - Epoch 938: total training loss 10.67\n",
            "2021-02-01 11:11:01,611 - INFO - joeynmt.training - EPOCH 939\n",
            "2021-02-01 11:11:13,873 - INFO - joeynmt.training - Epoch 939: total training loss 10.64\n",
            "2021-02-01 11:11:13,874 - INFO - joeynmt.training - EPOCH 940\n",
            "2021-02-01 11:11:26,185 - INFO - joeynmt.training - Epoch 940: total training loss 10.56\n",
            "2021-02-01 11:11:26,185 - INFO - joeynmt.training - EPOCH 941\n",
            "2021-02-01 11:11:38,493 - INFO - joeynmt.training - Epoch 941: total training loss 10.60\n",
            "2021-02-01 11:11:38,494 - INFO - joeynmt.training - EPOCH 942\n",
            "2021-02-01 11:11:50,867 - INFO - joeynmt.training - Epoch 942: total training loss 10.62\n",
            "2021-02-01 11:11:50,868 - INFO - joeynmt.training - EPOCH 943\n",
            "2021-02-01 11:12:03,097 - INFO - joeynmt.training - Epoch 943: total training loss 10.58\n",
            "2021-02-01 11:12:03,097 - INFO - joeynmt.training - EPOCH 944\n",
            "2021-02-01 11:12:07,153 - INFO - joeynmt.training - Epoch 944, Step:    14100, Batch Loss:     0.685914, Tokens per Sec:    18654, Lr: 0.000103\n",
            "2021-02-01 11:12:15,438 - INFO - joeynmt.training - Epoch 944: total training loss 10.55\n",
            "2021-02-01 11:12:15,439 - INFO - joeynmt.training - EPOCH 945\n",
            "2021-02-01 11:12:27,789 - INFO - joeynmt.training - Epoch 945: total training loss 10.52\n",
            "2021-02-01 11:12:27,789 - INFO - joeynmt.training - EPOCH 946\n",
            "2021-02-01 11:12:40,059 - INFO - joeynmt.training - Epoch 946: total training loss 10.56\n",
            "2021-02-01 11:12:40,060 - INFO - joeynmt.training - EPOCH 947\n",
            "2021-02-01 11:12:52,260 - INFO - joeynmt.training - Epoch 947: total training loss 10.56\n",
            "2021-02-01 11:12:52,260 - INFO - joeynmt.training - EPOCH 948\n",
            "2021-02-01 11:13:04,615 - INFO - joeynmt.training - Epoch 948: total training loss 10.52\n",
            "2021-02-01 11:13:04,615 - INFO - joeynmt.training - EPOCH 949\n",
            "2021-02-01 11:13:16,916 - INFO - joeynmt.training - Epoch 949: total training loss 10.59\n",
            "2021-02-01 11:13:16,916 - INFO - joeynmt.training - EPOCH 950\n",
            "2021-02-01 11:13:28,957 - INFO - joeynmt.training - Epoch 950, Step:    14200, Batch Loss:     0.681889, Tokens per Sec:    18379, Lr: 0.000103\n",
            "2021-02-01 11:13:29,337 - INFO - joeynmt.training - Epoch 950: total training loss 10.53\n",
            "2021-02-01 11:13:29,337 - INFO - joeynmt.training - EPOCH 951\n",
            "2021-02-01 11:13:41,685 - INFO - joeynmt.training - Epoch 951: total training loss 10.53\n",
            "2021-02-01 11:13:41,685 - INFO - joeynmt.training - EPOCH 952\n",
            "2021-02-01 11:13:54,021 - INFO - joeynmt.training - Epoch 952: total training loss 10.50\n",
            "2021-02-01 11:13:54,022 - INFO - joeynmt.training - EPOCH 953\n",
            "2021-02-01 11:14:06,370 - INFO - joeynmt.training - Epoch 953: total training loss 10.52\n",
            "2021-02-01 11:14:06,371 - INFO - joeynmt.training - EPOCH 954\n",
            "2021-02-01 11:14:18,593 - INFO - joeynmt.training - Epoch 954: total training loss 10.53\n",
            "2021-02-01 11:14:18,594 - INFO - joeynmt.training - EPOCH 955\n",
            "2021-02-01 11:14:30,642 - INFO - joeynmt.training - Epoch 955: total training loss 9.81\n",
            "2021-02-01 11:14:30,642 - INFO - joeynmt.training - EPOCH 956\n",
            "2021-02-01 11:14:42,844 - INFO - joeynmt.training - Epoch 956: total training loss 10.44\n",
            "2021-02-01 11:14:42,844 - INFO - joeynmt.training - EPOCH 957\n",
            "2021-02-01 11:14:51,736 - INFO - joeynmt.training - Epoch 957, Step:    14300, Batch Loss:     0.675630, Tokens per Sec:    18721, Lr: 0.000103\n",
            "2021-02-01 11:14:55,152 - INFO - joeynmt.training - Epoch 957: total training loss 10.49\n",
            "2021-02-01 11:14:55,153 - INFO - joeynmt.training - EPOCH 958\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:15:07,456 - INFO - joeynmt.training - Epoch 958: total training loss 10.46\n",
            "2021-02-01 11:15:07,457 - INFO - joeynmt.training - EPOCH 959\n",
            "2021-02-01 11:15:19,568 - INFO - joeynmt.training - Epoch 959: total training loss 9.80\n",
            "2021-02-01 11:15:19,568 - INFO - joeynmt.training - EPOCH 960\n",
            "2021-02-01 11:15:31,985 - INFO - joeynmt.training - Epoch 960: total training loss 10.50\n",
            "2021-02-01 11:15:31,985 - INFO - joeynmt.training - EPOCH 961\n",
            "2021-02-01 11:15:44,295 - INFO - joeynmt.training - Epoch 961: total training loss 10.45\n",
            "2021-02-01 11:15:44,296 - INFO - joeynmt.training - EPOCH 962\n",
            "2021-02-01 11:15:56,652 - INFO - joeynmt.training - Epoch 962: total training loss 10.50\n",
            "2021-02-01 11:15:56,652 - INFO - joeynmt.training - EPOCH 963\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 11:16:09,044 - INFO - joeynmt.training - Epoch 963: total training loss 10.45\n",
            "2021-02-01 11:16:09,045 - INFO - joeynmt.training - EPOCH 964\n",
            "2021-02-01 11:16:14,723 - INFO - joeynmt.training - Epoch 964, Step:    14400, Batch Loss:     0.744012, Tokens per Sec:    18157, Lr: 0.000103\n",
            "2021-02-01 11:16:21,382 - INFO - joeynmt.training - Epoch 964: total training loss 10.45\n",
            "2021-02-01 11:16:21,383 - INFO - joeynmt.training - EPOCH 965\n",
            "2021-02-01 11:16:33,687 - INFO - joeynmt.training - Epoch 965: total training loss 10.46\n",
            "2021-02-01 11:16:33,688 - INFO - joeynmt.training - EPOCH 966\n",
            "2021-02-01 11:16:46,023 - INFO - joeynmt.training - Epoch 966: total training loss 10.50\n",
            "2021-02-01 11:16:46,023 - INFO - joeynmt.training - EPOCH 967\n",
            "2021-02-01 11:16:58,277 - INFO - joeynmt.training - Epoch 967: total training loss 10.44\n",
            "2021-02-01 11:16:58,278 - INFO - joeynmt.training - EPOCH 968\n",
            "2021-02-01 11:17:10,592 - INFO - joeynmt.training - Epoch 968: total training loss 10.51\n",
            "2021-02-01 11:17:10,593 - INFO - joeynmt.training - EPOCH 969\n",
            "2021-02-01 11:17:22,864 - INFO - joeynmt.training - Epoch 969: total training loss 10.45\n",
            "2021-02-01 11:17:22,864 - INFO - joeynmt.training - EPOCH 970\n",
            "2021-02-01 11:17:35,115 - INFO - joeynmt.training - Epoch 970: total training loss 10.44\n",
            "2021-02-01 11:17:35,115 - INFO - joeynmt.training - EPOCH 971\n",
            "2021-02-01 11:17:36,771 - INFO - joeynmt.training - Epoch 971, Step:    14500, Batch Loss:     0.710127, Tokens per Sec:    18708, Lr: 0.000103\n",
            "2021-02-01 11:18:04,860 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:18:04,861 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:18:04,861 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:18:04,861 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына алып кетіп, сол жерде қалаға жіберді.※\n",
            "2021-02-01 11:18:04,861 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:18:04,861 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап берудің деп үміттенді.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, бас тарту етінен бас тартқан жоқ. Тоқтының үстінде Құдайды көрмеген патшалық көрмегендерді қатты ашты,※ Құдайды мадақтап жүрдім.\n",
            "2021-02-01 11:18:04,862 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:18:04,863 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:18:04,863 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:18:04,863 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болыңдар: Оның оң жағындағы жер бетіндегі барлық жерді жариялауға тағайындаған едім.\n",
            "2021-02-01 11:18:04,863 - INFO - joeynmt.training - Validation result (greedy) at epoch 971, step    14500: bleu:   4.81, loss: 87306.8047, ppl:  77.0691, duration: 28.0920s\n",
            "2021-02-01 11:18:15,475 - INFO - joeynmt.training - Epoch 971: total training loss 10.47\n",
            "2021-02-01 11:18:15,475 - INFO - joeynmt.training - EPOCH 972\n",
            "2021-02-01 11:18:27,686 - INFO - joeynmt.training - Epoch 972: total training loss 10.44\n",
            "2021-02-01 11:18:27,686 - INFO - joeynmt.training - EPOCH 973\n",
            "2021-02-01 11:18:40,294 - INFO - joeynmt.training - Epoch 973: total training loss 10.43\n",
            "2021-02-01 11:18:40,295 - INFO - joeynmt.training - EPOCH 974\n",
            "2021-02-01 11:18:52,636 - INFO - joeynmt.training - Epoch 974: total training loss 10.41\n",
            "2021-02-01 11:18:52,637 - INFO - joeynmt.training - EPOCH 975\n",
            "2021-02-01 11:19:05,038 - INFO - joeynmt.training - Epoch 975: total training loss 10.41\n",
            "2021-02-01 11:19:05,038 - INFO - joeynmt.training - EPOCH 976\n",
            "2021-02-01 11:19:17,378 - INFO - joeynmt.training - Epoch 976: total training loss 10.42\n",
            "2021-02-01 11:19:17,378 - INFO - joeynmt.training - EPOCH 977\n",
            "2021-02-01 11:19:27,127 - INFO - joeynmt.training - Epoch 977, Step:    14600, Batch Loss:     0.718681, Tokens per Sec:    18756, Lr: 0.000103\n",
            "2021-02-01 11:19:29,560 - INFO - joeynmt.training - Epoch 977: total training loss 10.44\n",
            "2021-02-01 11:19:29,561 - INFO - joeynmt.training - EPOCH 978\n",
            "2021-02-01 11:19:41,808 - INFO - joeynmt.training - Epoch 978: total training loss 10.46\n",
            "2021-02-01 11:19:41,808 - INFO - joeynmt.training - EPOCH 979\n",
            "2021-02-01 11:19:54,058 - INFO - joeynmt.training - Epoch 979: total training loss 10.42\n",
            "2021-02-01 11:19:54,058 - INFO - joeynmt.training - EPOCH 980\n",
            "2021-02-01 11:20:06,302 - INFO - joeynmt.training - Epoch 980: total training loss 10.40\n",
            "2021-02-01 11:20:06,302 - INFO - joeynmt.training - EPOCH 981\n",
            "2021-02-01 11:20:18,576 - INFO - joeynmt.training - Epoch 981: total training loss 10.43\n",
            "2021-02-01 11:20:18,577 - INFO - joeynmt.training - EPOCH 982\n",
            "2021-02-01 11:20:30,751 - INFO - joeynmt.training - Epoch 982: total training loss 10.38\n",
            "2021-02-01 11:20:30,752 - INFO - joeynmt.training - EPOCH 983\n",
            "2021-02-01 11:20:43,239 - INFO - joeynmt.training - Epoch 983: total training loss 10.40\n",
            "2021-02-01 11:20:43,239 - INFO - joeynmt.training - EPOCH 984\n",
            "2021-02-01 11:20:48,972 - INFO - joeynmt.training - Epoch 984, Step:    14700, Batch Loss:     0.706824, Tokens per Sec:    18383, Lr: 0.000103\n",
            "2021-02-01 11:20:55,419 - INFO - joeynmt.training - Epoch 984: total training loss 9.67\n",
            "2021-02-01 11:20:55,419 - INFO - joeynmt.training - EPOCH 985\n",
            "2021-02-01 11:21:07,555 - INFO - joeynmt.training - Epoch 985: total training loss 10.39\n",
            "2021-02-01 11:21:07,555 - INFO - joeynmt.training - EPOCH 986\n",
            "2021-02-01 11:21:19,627 - INFO - joeynmt.training - Epoch 986: total training loss 9.73\n",
            "2021-02-01 11:21:19,627 - INFO - joeynmt.training - EPOCH 987\n",
            "2021-02-01 11:21:31,907 - INFO - joeynmt.training - Epoch 987: total training loss 10.41\n",
            "2021-02-01 11:21:31,907 - INFO - joeynmt.training - EPOCH 988\n",
            "2021-02-01 11:21:44,315 - INFO - joeynmt.training - Epoch 988: total training loss 10.38\n",
            "2021-02-01 11:21:44,316 - INFO - joeynmt.training - EPOCH 989\n",
            "2021-02-01 11:21:56,689 - INFO - joeynmt.training - Epoch 989: total training loss 10.34\n",
            "2021-02-01 11:21:56,690 - INFO - joeynmt.training - EPOCH 990\n",
            "2021-02-01 11:22:09,189 - INFO - joeynmt.training - Epoch 990: total training loss 10.34\n",
            "2021-02-01 11:22:09,190 - INFO - joeynmt.training - EPOCH 991\n",
            "2021-02-01 11:22:12,399 - INFO - joeynmt.training - Epoch 991, Step:    14800, Batch Loss:     0.692819, Tokens per Sec:    17834, Lr: 0.000103\n",
            "2021-02-01 11:22:21,511 - INFO - joeynmt.training - Epoch 991: total training loss 10.38\n",
            "2021-02-01 11:22:21,511 - INFO - joeynmt.training - EPOCH 992\n",
            "2021-02-01 11:22:33,813 - INFO - joeynmt.training - Epoch 992: total training loss 10.36\n",
            "2021-02-01 11:22:33,813 - INFO - joeynmt.training - EPOCH 993\n",
            "2021-02-01 11:22:46,045 - INFO - joeynmt.training - Epoch 993: total training loss 10.37\n",
            "2021-02-01 11:22:46,046 - INFO - joeynmt.training - EPOCH 994\n",
            "2021-02-01 11:22:58,278 - INFO - joeynmt.training - Epoch 994: total training loss 10.37\n",
            "2021-02-01 11:22:58,279 - INFO - joeynmt.training - EPOCH 995\n",
            "2021-02-01 11:23:10,646 - INFO - joeynmt.training - Epoch 995: total training loss 10.35\n",
            "2021-02-01 11:23:10,646 - INFO - joeynmt.training - EPOCH 996\n",
            "2021-02-01 11:23:23,019 - INFO - joeynmt.training - Epoch 996: total training loss 10.38\n",
            "2021-02-01 11:23:23,019 - INFO - joeynmt.training - EPOCH 997\n",
            "2021-02-01 11:23:34,298 - INFO - joeynmt.training - Epoch 997, Step:    14900, Batch Loss:     0.697120, Tokens per Sec:    18412, Lr: 0.000103\n",
            "2021-02-01 11:23:35,412 - INFO - joeynmt.training - Epoch 997: total training loss 10.34\n",
            "2021-02-01 11:23:35,413 - INFO - joeynmt.training - EPOCH 998\n",
            "2021-02-01 11:23:47,621 - INFO - joeynmt.training - Epoch 998: total training loss 10.33\n",
            "2021-02-01 11:23:47,622 - INFO - joeynmt.training - EPOCH 999\n",
            "2021-02-01 11:23:59,856 - INFO - joeynmt.training - Epoch 999: total training loss 10.34\n",
            "2021-02-01 11:23:59,857 - INFO - joeynmt.training - EPOCH 1000\n",
            "2021-02-01 11:24:12,260 - INFO - joeynmt.training - Epoch 1000: total training loss 10.33\n",
            "2021-02-01 11:24:12,261 - INFO - joeynmt.training - EPOCH 1001\n",
            "2021-02-01 11:24:24,698 - INFO - joeynmt.training - Epoch 1001: total training loss 10.36\n",
            "2021-02-01 11:24:24,698 - INFO - joeynmt.training - EPOCH 1002\n",
            "2021-02-01 11:24:37,001 - INFO - joeynmt.training - Epoch 1002: total training loss 10.31\n",
            "2021-02-01 11:24:37,001 - INFO - joeynmt.training - EPOCH 1003\n",
            "2021-02-01 11:24:49,343 - INFO - joeynmt.training - Epoch 1003: total training loss 10.30\n",
            "2021-02-01 11:24:49,344 - INFO - joeynmt.training - EPOCH 1004\n",
            "2021-02-01 11:24:56,581 - INFO - joeynmt.training - Epoch 1004, Step:    15000, Batch Loss:     0.661414, Tokens per Sec:    18277, Lr: 0.000103\n",
            "2021-02-01 11:25:24,393 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:25:24,394 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:25:24,394 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:25:24,394 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғаумәжілісханаларға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 11:25:24,394 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:25:24,395 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:25:24,395 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:25:24,395 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Исаны ұстап беру үшін өлімге кесіп, «грек деп үміттенді.\n",
            "2021-02-01 11:25:24,395 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:25:24,396 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:25:24,396 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:25:24,396 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, бас тарту ретінде сылдым. Олар Тоқтының үстінде отырған Құдайды көрмеген патшалық көрмегендерді қатты ашты; оның есімі көрініп, Құдайды мадақтап күткен, айуанның мәнінде тұратындар мен әйелге жіндей көрдім.\n",
            "2021-02-01 11:25:24,396 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:25:24,396 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:25:24,397 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:25:24,397 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге қайта тірілмеген кезде де көкке көтерілмеңдер де, табаныңның астындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 11:25:24,397 - INFO - joeynmt.training - Validation result (greedy) at epoch 1004, step    15000: bleu:   5.08, loss: 87478.1719, ppl:  77.7292, duration: 27.8155s\n",
            "2021-02-01 11:25:29,466 - INFO - joeynmt.training - Epoch 1004: total training loss 10.32\n",
            "2021-02-01 11:25:29,466 - INFO - joeynmt.training - EPOCH 1005\n",
            "2021-02-01 11:25:41,895 - INFO - joeynmt.training - Epoch 1005: total training loss 10.33\n",
            "2021-02-01 11:25:41,896 - INFO - joeynmt.training - EPOCH 1006\n",
            "2021-02-01 11:25:54,439 - INFO - joeynmt.training - Epoch 1006: total training loss 10.35\n",
            "2021-02-01 11:25:54,439 - INFO - joeynmt.training - EPOCH 1007\n",
            "2021-02-01 11:26:06,790 - INFO - joeynmt.training - Epoch 1007: total training loss 10.30\n",
            "2021-02-01 11:26:06,791 - INFO - joeynmt.training - EPOCH 1008\n",
            "2021-02-01 11:26:19,099 - INFO - joeynmt.training - Epoch 1008: total training loss 10.29\n",
            "2021-02-01 11:26:19,100 - INFO - joeynmt.training - EPOCH 1009\n",
            "2021-02-01 11:26:31,319 - INFO - joeynmt.training - Epoch 1009: total training loss 10.27\n",
            "2021-02-01 11:26:31,319 - INFO - joeynmt.training - EPOCH 1010\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:26:43,658 - INFO - joeynmt.training - Epoch 1010: total training loss 10.27\n",
            "2021-02-01 11:26:43,658 - INFO - joeynmt.training - EPOCH 1011\n",
            "2021-02-01 11:26:46,827 - INFO - joeynmt.training - Epoch 1011, Step:    15100, Batch Loss:     0.680435, Tokens per Sec:    17928, Lr: 0.000103\n",
            "2021-02-01 11:26:56,049 - INFO - joeynmt.training - Epoch 1011: total training loss 10.24\n",
            "2021-02-01 11:26:56,050 - INFO - joeynmt.training - EPOCH 1012\n",
            "2021-02-01 11:27:08,379 - INFO - joeynmt.training - Epoch 1012: total training loss 10.30\n",
            "2021-02-01 11:27:08,380 - INFO - joeynmt.training - EPOCH 1013\n",
            "2021-02-01 11:27:20,739 - INFO - joeynmt.training - Epoch 1013: total training loss 10.33\n",
            "2021-02-01 11:27:20,739 - INFO - joeynmt.training - EPOCH 1014\n",
            "2021-02-01 11:27:32,961 - INFO - joeynmt.training - Epoch 1014: total training loss 10.33\n",
            "2021-02-01 11:27:32,961 - INFO - joeynmt.training - EPOCH 1015\n",
            "2021-02-01 11:27:45,373 - INFO - joeynmt.training - Epoch 1015: total training loss 10.26\n",
            "2021-02-01 11:27:45,373 - INFO - joeynmt.training - EPOCH 1016\n",
            "2021-02-01 11:27:57,778 - INFO - joeynmt.training - Epoch 1016: total training loss 10.24\n",
            "2021-02-01 11:27:57,779 - INFO - joeynmt.training - EPOCH 1017\n",
            "2021-02-01 11:28:09,136 - INFO - joeynmt.training - Epoch 1017, Step:    15200, Batch Loss:     0.734270, Tokens per Sec:    18699, Lr: 0.000103\n",
            "2021-02-01 11:28:09,969 - INFO - joeynmt.training - Epoch 1017: total training loss 10.31\n",
            "2021-02-01 11:28:09,970 - INFO - joeynmt.training - EPOCH 1018\n",
            "2021-02-01 11:28:22,461 - INFO - joeynmt.training - Epoch 1018: total training loss 10.26\n",
            "2021-02-01 11:28:22,461 - INFO - joeynmt.training - EPOCH 1019\n",
            "2021-02-01 11:28:34,894 - INFO - joeynmt.training - Epoch 1019: total training loss 10.20\n",
            "2021-02-01 11:28:34,894 - INFO - joeynmt.training - EPOCH 1020\n",
            "2021-02-01 11:28:47,010 - INFO - joeynmt.training - Epoch 1020: total training loss 9.63\n",
            "2021-02-01 11:28:47,010 - INFO - joeynmt.training - EPOCH 1021\n",
            "2021-02-01 11:28:59,244 - INFO - joeynmt.training - Epoch 1021: total training loss 10.28\n",
            "2021-02-01 11:28:59,245 - INFO - joeynmt.training - EPOCH 1022\n",
            "2021-02-01 11:29:11,430 - INFO - joeynmt.training - Epoch 1022: total training loss 10.28\n",
            "2021-02-01 11:29:11,430 - INFO - joeynmt.training - EPOCH 1023\n",
            "2021-02-01 11:29:23,731 - INFO - joeynmt.training - Epoch 1023: total training loss 10.29\n",
            "2021-02-01 11:29:23,732 - INFO - joeynmt.training - EPOCH 1024\n",
            "2021-02-01 11:29:31,918 - INFO - joeynmt.training - Epoch 1024, Step:    15300, Batch Loss:     0.657367, Tokens per Sec:    17980, Lr: 0.000103\n",
            "2021-02-01 11:29:36,082 - INFO - joeynmt.training - Epoch 1024: total training loss 10.24\n",
            "2021-02-01 11:29:36,083 - INFO - joeynmt.training - EPOCH 1025\n",
            "2021-02-01 11:29:48,383 - INFO - joeynmt.training - Epoch 1025: total training loss 10.26\n",
            "2021-02-01 11:29:48,383 - INFO - joeynmt.training - EPOCH 1026\n",
            "2021-02-01 11:30:00,681 - INFO - joeynmt.training - Epoch 1026: total training loss 10.30\n",
            "2021-02-01 11:30:00,681 - INFO - joeynmt.training - EPOCH 1027\n",
            "2021-02-01 11:30:13,006 - INFO - joeynmt.training - Epoch 1027: total training loss 10.22\n",
            "2021-02-01 11:30:13,006 - INFO - joeynmt.training - EPOCH 1028\n",
            "2021-02-01 11:30:25,003 - INFO - joeynmt.training - Epoch 1028: total training loss 9.56\n",
            "2021-02-01 11:30:25,004 - INFO - joeynmt.training - EPOCH 1029\n",
            "2021-02-01 11:30:37,242 - INFO - joeynmt.training - Epoch 1029: total training loss 10.25\n",
            "2021-02-01 11:30:37,242 - INFO - joeynmt.training - EPOCH 1030\n",
            "2021-02-01 11:30:49,628 - INFO - joeynmt.training - Epoch 1030: total training loss 10.22\n",
            "2021-02-01 11:30:49,629 - INFO - joeynmt.training - EPOCH 1031\n",
            "2021-02-01 11:30:54,514 - INFO - joeynmt.training - Epoch 1031, Step:    15400, Batch Loss:     0.714152, Tokens per Sec:    18814, Lr: 0.000103\n",
            "2021-02-01 11:31:01,937 - INFO - joeynmt.training - Epoch 1031: total training loss 10.21\n",
            "2021-02-01 11:31:01,938 - INFO - joeynmt.training - EPOCH 1032\n",
            "2021-02-01 11:31:14,285 - INFO - joeynmt.training - Epoch 1032: total training loss 10.23\n",
            "2021-02-01 11:31:14,285 - INFO - joeynmt.training - EPOCH 1033\n",
            "2021-02-01 11:31:26,545 - INFO - joeynmt.training - Epoch 1033: total training loss 10.21\n",
            "2021-02-01 11:31:26,545 - INFO - joeynmt.training - EPOCH 1034\n",
            "2021-02-01 11:31:38,930 - INFO - joeynmt.training - Epoch 1034: total training loss 10.16\n",
            "2021-02-01 11:31:38,930 - INFO - joeynmt.training - EPOCH 1035\n",
            "2021-02-01 11:31:51,251 - INFO - joeynmt.training - Epoch 1035: total training loss 10.19\n",
            "2021-02-01 11:31:51,252 - INFO - joeynmt.training - EPOCH 1036\n",
            "2021-02-01 11:32:03,490 - INFO - joeynmt.training - Epoch 1036: total training loss 10.26\n",
            "2021-02-01 11:32:03,490 - INFO - joeynmt.training - EPOCH 1037\n",
            "2021-02-01 11:32:15,698 - INFO - joeynmt.training - Epoch 1037: total training loss 10.21\n",
            "2021-02-01 11:32:15,698 - INFO - joeynmt.training - EPOCH 1038\n",
            "2021-02-01 11:32:16,521 - INFO - joeynmt.training - Epoch 1038, Step:    15500, Batch Loss:     0.638379, Tokens per Sec:    15537, Lr: 0.000103\n",
            "2021-02-01 11:32:43,642 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:32:43,642 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:32:43,643 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:32:43,643 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға ертіп қаладан шығып, мәжілісханаға барды.\n",
            "2021-02-01 11:32:43,643 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:32:43,643 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:32:43,644 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:32:43,644 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Исаны көргенде «Оны байлап тыңдап алды.\n",
            "2021-02-01 11:32:43,644 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:32:43,644 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:32:43,645 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:32:43,645 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, Құдайдың Ізгі хабарын жеткізіп, оны көрдім. Ол Тоқтының үстінде отырған Құдайды мадақтай тілі көрдім. Олар қатты ашулана дүние, Құдайды қастерлейтін айуанның мәсер мен әйелге қатты ашты!\n",
            "2021-02-01 11:32:43,645 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:32:43,645 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:32:43,646 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:32:43,646 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене де көкке көтермеңдер. Бауырыңның оң жағындағы барлық жерінде орналасқан кісінің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 11:32:43,646 - INFO - joeynmt.training - Validation result (greedy) at epoch 1038, step    15500: bleu:   5.00, loss: 87453.1719, ppl:  77.6325, duration: 27.1249s\n",
            "2021-02-01 11:32:55,376 - INFO - joeynmt.training - Epoch 1038: total training loss 10.22\n",
            "2021-02-01 11:32:55,376 - INFO - joeynmt.training - EPOCH 1039\n",
            "2021-02-01 11:33:07,822 - INFO - joeynmt.training - Epoch 1039: total training loss 10.19\n",
            "2021-02-01 11:33:07,822 - INFO - joeynmt.training - EPOCH 1040\n",
            "2021-02-01 11:33:20,505 - INFO - joeynmt.training - Epoch 1040: total training loss 10.19\n",
            "2021-02-01 11:33:20,506 - INFO - joeynmt.training - EPOCH 1041\n",
            "2021-02-01 11:33:33,333 - INFO - joeynmt.training - Epoch 1041: total training loss 10.18\n",
            "2021-02-01 11:33:33,333 - INFO - joeynmt.training - EPOCH 1042\n",
            "2021-02-01 11:33:45,995 - INFO - joeynmt.training - Epoch 1042: total training loss 10.17\n",
            "2021-02-01 11:33:45,995 - INFO - joeynmt.training - EPOCH 1043\n",
            "2021-02-01 11:33:58,393 - INFO - joeynmt.training - Epoch 1043: total training loss 10.22\n",
            "2021-02-01 11:33:58,394 - INFO - joeynmt.training - EPOCH 1044\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 11:34:07,259 - INFO - joeynmt.training - Epoch 1044, Step:    15600, Batch Loss:     0.660401, Tokens per Sec:    18496, Lr: 0.000103\n",
            "2021-02-01 11:34:10,680 - INFO - joeynmt.training - Epoch 1044: total training loss 10.09\n",
            "2021-02-01 11:34:10,680 - INFO - joeynmt.training - EPOCH 1045\n",
            "2021-02-01 11:34:22,916 - INFO - joeynmt.training - Epoch 1045: total training loss 10.17\n",
            "2021-02-01 11:34:22,917 - INFO - joeynmt.training - EPOCH 1046\n",
            "2021-02-01 11:34:35,361 - INFO - joeynmt.training - Epoch 1046: total training loss 10.20\n",
            "2021-02-01 11:34:35,362 - INFO - joeynmt.training - EPOCH 1047\n",
            "2021-02-01 11:34:47,801 - INFO - joeynmt.training - Epoch 1047: total training loss 10.17\n",
            "2021-02-01 11:34:47,802 - INFO - joeynmt.training - EPOCH 1048\n",
            "2021-02-01 11:35:00,272 - INFO - joeynmt.training - Epoch 1048: total training loss 10.16\n",
            "2021-02-01 11:35:00,273 - INFO - joeynmt.training - EPOCH 1049\n",
            "2021-02-01 11:35:12,751 - INFO - joeynmt.training - Epoch 1049: total training loss 10.12\n",
            "2021-02-01 11:35:12,751 - INFO - joeynmt.training - EPOCH 1050\n",
            "2021-02-01 11:35:25,232 - INFO - joeynmt.training - Epoch 1050: total training loss 10.17\n",
            "2021-02-01 11:35:25,232 - INFO - joeynmt.training - EPOCH 1051\n",
            "2021-02-01 11:35:30,169 - INFO - joeynmt.training - Epoch 1051, Step:    15700, Batch Loss:     0.679363, Tokens per Sec:    19190, Lr: 0.000103\n",
            "2021-02-01 11:35:37,550 - INFO - joeynmt.training - Epoch 1051: total training loss 10.15\n",
            "2021-02-01 11:35:37,550 - INFO - joeynmt.training - EPOCH 1052\n",
            "2021-02-01 11:35:49,996 - INFO - joeynmt.training - Epoch 1052: total training loss 10.18\n",
            "2021-02-01 11:35:49,996 - INFO - joeynmt.training - EPOCH 1053\n",
            "2021-02-01 11:36:02,351 - INFO - joeynmt.training - Epoch 1053: total training loss 10.15\n",
            "2021-02-01 11:36:02,352 - INFO - joeynmt.training - EPOCH 1054\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:36:14,816 - INFO - joeynmt.training - Epoch 1054: total training loss 10.16\n",
            "2021-02-01 11:36:14,817 - INFO - joeynmt.training - EPOCH 1055\n",
            "2021-02-01 11:36:27,200 - INFO - joeynmt.training - Epoch 1055: total training loss 10.15\n",
            "2021-02-01 11:36:27,200 - INFO - joeynmt.training - EPOCH 1056\n",
            "2021-02-01 11:36:39,548 - INFO - joeynmt.training - Epoch 1056: total training loss 10.10\n",
            "2021-02-01 11:36:39,548 - INFO - joeynmt.training - EPOCH 1057\n",
            "2021-02-01 11:36:52,106 - INFO - joeynmt.training - Epoch 1057: total training loss 10.17\n",
            "2021-02-01 11:36:52,106 - INFO - joeynmt.training - EPOCH 1058\n",
            "2021-02-01 11:36:52,969 - INFO - joeynmt.training - Epoch 1058, Step:    15800, Batch Loss:     0.686749, Tokens per Sec:    19158, Lr: 0.000103\n",
            "2021-02-01 11:37:04,488 - INFO - joeynmt.training - Epoch 1058: total training loss 10.11\n",
            "2021-02-01 11:37:04,488 - INFO - joeynmt.training - EPOCH 1059\n",
            "2021-02-01 11:37:16,914 - INFO - joeynmt.training - Epoch 1059: total training loss 10.17\n",
            "2021-02-01 11:37:16,914 - INFO - joeynmt.training - EPOCH 1060\n",
            "2021-02-01 11:37:29,211 - INFO - joeynmt.training - Epoch 1060: total training loss 10.19\n",
            "2021-02-01 11:37:29,212 - INFO - joeynmt.training - EPOCH 1061\n",
            "2021-02-01 11:37:41,598 - INFO - joeynmt.training - Epoch 1061: total training loss 10.17\n",
            "2021-02-01 11:37:41,598 - INFO - joeynmt.training - EPOCH 1062\n",
            "2021-02-01 11:37:54,049 - INFO - joeynmt.training - Epoch 1062: total training loss 10.14\n",
            "2021-02-01 11:37:54,049 - INFO - joeynmt.training - EPOCH 1063\n",
            "2021-02-01 11:38:06,332 - INFO - joeynmt.training - Epoch 1063: total training loss 10.10\n",
            "2021-02-01 11:38:06,333 - INFO - joeynmt.training - EPOCH 1064\n",
            "2021-02-01 11:38:15,314 - INFO - joeynmt.training - Epoch 1064, Step:    15900, Batch Loss:     0.671630, Tokens per Sec:    18315, Lr: 0.000103\n",
            "2021-02-01 11:38:18,713 - INFO - joeynmt.training - Epoch 1064: total training loss 10.09\n",
            "2021-02-01 11:38:18,713 - INFO - joeynmt.training - EPOCH 1065\n",
            "2021-02-01 11:38:31,175 - INFO - joeynmt.training - Epoch 1065: total training loss 10.11\n",
            "2021-02-01 11:38:31,176 - INFO - joeynmt.training - EPOCH 1066\n",
            "2021-02-01 11:38:43,687 - INFO - joeynmt.training - Epoch 1066: total training loss 10.09\n",
            "2021-02-01 11:38:43,688 - INFO - joeynmt.training - EPOCH 1067\n",
            "2021-02-01 11:38:56,051 - INFO - joeynmt.training - Epoch 1067: total training loss 10.11\n",
            "2021-02-01 11:38:56,051 - INFO - joeynmt.training - EPOCH 1068\n",
            "2021-02-01 11:39:08,514 - INFO - joeynmt.training - Epoch 1068: total training loss 10.10\n",
            "2021-02-01 11:39:08,515 - INFO - joeynmt.training - EPOCH 1069\n",
            "2021-02-01 11:39:20,978 - INFO - joeynmt.training - Epoch 1069: total training loss 10.06\n",
            "2021-02-01 11:39:20,979 - INFO - joeynmt.training - EPOCH 1070\n",
            "2021-02-01 11:39:33,330 - INFO - joeynmt.training - Epoch 1070: total training loss 10.08\n",
            "2021-02-01 11:39:33,331 - INFO - joeynmt.training - EPOCH 1071\n",
            "2021-02-01 11:39:38,155 - INFO - joeynmt.training - Epoch 1071, Step:    16000, Batch Loss:     0.648330, Tokens per Sec:    18233, Lr: 0.000103\n",
            "2021-02-01 11:40:05,788 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:40:05,789 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - \tHypothesis: Мұса арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 11:40:05,790 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:40:05,791 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:40:05,791 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:40:05,791 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, оған ілескен ештеңе мей етіп жүрдім. Оның есімі мен Құдайды мадақтап тілейтін дүниеде көрінді. Олар қатты ашты※ әйелдің үстінде тұратын, нештеңе болмадым.\n",
            "2021-02-01 11:40:05,791 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:40:05,791 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:40:05,792 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:40:05,792 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермеңдер, енді Оның оң жағындағы құрметті орнына жайғасқан кезде Мәсіх деген құрметті орнында отыр.※\n",
            "2021-02-01 11:40:05,792 - INFO - joeynmt.training - Validation result (greedy) at epoch 1071, step    16000: bleu:   5.39, loss: 87512.3672, ppl:  77.8615, duration: 27.6363s\n",
            "2021-02-01 11:40:13,385 - INFO - joeynmt.training - Epoch 1071: total training loss 10.10\n",
            "2021-02-01 11:40:13,385 - INFO - joeynmt.training - EPOCH 1072\n",
            "2021-02-01 11:40:25,715 - INFO - joeynmt.training - Epoch 1072: total training loss 10.11\n",
            "2021-02-01 11:40:25,715 - INFO - joeynmt.training - EPOCH 1073\n",
            "2021-02-01 11:40:38,011 - INFO - joeynmt.training - Epoch 1073: total training loss 10.15\n",
            "2021-02-01 11:40:38,012 - INFO - joeynmt.training - EPOCH 1074\n",
            "2021-02-01 11:40:50,375 - INFO - joeynmt.training - Epoch 1074: total training loss 10.07\n",
            "2021-02-01 11:40:50,375 - INFO - joeynmt.training - EPOCH 1075\n",
            "2021-02-01 11:41:02,616 - INFO - joeynmt.training - Epoch 1075: total training loss 10.13\n",
            "2021-02-01 11:41:02,616 - INFO - joeynmt.training - EPOCH 1076\n",
            "2021-02-01 11:41:15,091 - INFO - joeynmt.training - Epoch 1076: total training loss 10.08\n",
            "2021-02-01 11:41:15,092 - INFO - joeynmt.training - EPOCH 1077\n",
            "2021-02-01 11:41:27,578 - INFO - joeynmt.training - Epoch 1077: total training loss 10.11\n",
            "2021-02-01 11:41:27,578 - INFO - joeynmt.training - EPOCH 1078\n",
            "2021-02-01 11:41:28,443 - INFO - joeynmt.training - Epoch 1078, Step:    16100, Batch Loss:     0.720669, Tokens per Sec:    18959, Lr: 0.000103\n",
            "2021-02-01 11:41:40,030 - INFO - joeynmt.training - Epoch 1078: total training loss 10.15\n",
            "2021-02-01 11:41:40,031 - INFO - joeynmt.training - EPOCH 1079\n",
            "2021-02-01 11:41:52,345 - INFO - joeynmt.training - Epoch 1079: total training loss 10.11\n",
            "2021-02-01 11:41:52,346 - INFO - joeynmt.training - EPOCH 1080\n",
            "2021-02-01 11:42:04,754 - INFO - joeynmt.training - Epoch 1080: total training loss 10.13\n",
            "2021-02-01 11:42:04,755 - INFO - joeynmt.training - EPOCH 1081\n",
            "2021-02-01 11:42:17,179 - INFO - joeynmt.training - Epoch 1081: total training loss 10.05\n",
            "2021-02-01 11:42:17,179 - INFO - joeynmt.training - EPOCH 1082\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 11:42:29,570 - INFO - joeynmt.training - Epoch 1082: total training loss 10.06\n",
            "2021-02-01 11:42:29,570 - INFO - joeynmt.training - EPOCH 1083\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 11:42:42,153 - INFO - joeynmt.training - Epoch 1083: total training loss 10.05\n",
            "2021-02-01 11:42:42,153 - INFO - joeynmt.training - EPOCH 1084\n",
            "2021-02-01 11:42:51,147 - INFO - joeynmt.training - Epoch 1084, Step:    16200, Batch Loss:     0.684583, Tokens per Sec:    18739, Lr: 0.000103\n",
            "2021-02-01 11:42:54,306 - INFO - joeynmt.training - Epoch 1084: total training loss 9.35\n",
            "2021-02-01 11:42:54,306 - INFO - joeynmt.training - EPOCH 1085\n",
            "2021-02-01 11:43:06,610 - INFO - joeynmt.training - Epoch 1085: total training loss 10.05\n",
            "2021-02-01 11:43:06,610 - INFO - joeynmt.training - EPOCH 1086\n",
            "2021-02-01 11:43:18,946 - INFO - joeynmt.training - Epoch 1086: total training loss 10.03\n",
            "2021-02-01 11:43:18,947 - INFO - joeynmt.training - EPOCH 1087\n",
            "2021-02-01 11:43:31,310 - INFO - joeynmt.training - Epoch 1087: total training loss 10.03\n",
            "2021-02-01 11:43:31,311 - INFO - joeynmt.training - EPOCH 1088\n",
            "2021-02-01 11:43:43,752 - INFO - joeynmt.training - Epoch 1088: total training loss 10.05\n",
            "2021-02-01 11:43:43,753 - INFO - joeynmt.training - EPOCH 1089\n",
            "2021-02-01 11:43:56,089 - INFO - joeynmt.training - Epoch 1089: total training loss 10.05\n",
            "2021-02-01 11:43:56,089 - INFO - joeynmt.training - EPOCH 1090\n",
            "2021-02-01 11:44:08,277 - INFO - joeynmt.training - Epoch 1090: total training loss 10.05\n",
            "2021-02-01 11:44:08,277 - INFO - joeynmt.training - EPOCH 1091\n",
            "2021-02-01 11:44:13,978 - INFO - joeynmt.training - Epoch 1091, Step:    16300, Batch Loss:     0.690571, Tokens per Sec:    19060, Lr: 0.000103\n",
            "2021-02-01 11:44:20,657 - INFO - joeynmt.training - Epoch 1091: total training loss 10.00\n",
            "2021-02-01 11:44:20,657 - INFO - joeynmt.training - EPOCH 1092\n",
            "2021-02-01 11:44:33,084 - INFO - joeynmt.training - Epoch 1092: total training loss 10.06\n",
            "2021-02-01 11:44:33,084 - INFO - joeynmt.training - EPOCH 1093\n",
            "2021-02-01 11:44:45,412 - INFO - joeynmt.training - Epoch 1093: total training loss 10.00\n",
            "2021-02-01 11:44:45,413 - INFO - joeynmt.training - EPOCH 1094\n",
            "2021-02-01 11:44:57,763 - INFO - joeynmt.training - Epoch 1094: total training loss 10.02\n",
            "2021-02-01 11:44:57,764 - INFO - joeynmt.training - EPOCH 1095\n",
            "2021-02-01 11:45:10,064 - INFO - joeynmt.training - Epoch 1095: total training loss 10.02\n",
            "2021-02-01 11:45:10,065 - INFO - joeynmt.training - EPOCH 1096\n",
            "2021-02-01 11:45:22,379 - INFO - joeynmt.training - Epoch 1096: total training loss 10.02\n",
            "2021-02-01 11:45:22,379 - INFO - joeynmt.training - EPOCH 1097\n",
            "2021-02-01 11:45:34,764 - INFO - joeynmt.training - Epoch 1097: total training loss 10.02\n",
            "2021-02-01 11:45:34,764 - INFO - joeynmt.training - EPOCH 1098\n",
            "2021-02-01 11:45:36,445 - INFO - joeynmt.training - Epoch 1098, Step:    16400, Batch Loss:     0.672020, Tokens per Sec:    19181, Lr: 0.000103\n",
            "2021-02-01 11:45:47,214 - INFO - joeynmt.training - Epoch 1098: total training loss 10.03\n",
            "2021-02-01 11:45:47,215 - INFO - joeynmt.training - EPOCH 1099\n",
            "2021-02-01 11:45:59,521 - INFO - joeynmt.training - Epoch 1099: total training loss 9.98\n",
            "2021-02-01 11:45:59,521 - INFO - joeynmt.training - EPOCH 1100\n",
            "2021-02-01 11:46:11,870 - INFO - joeynmt.training - Epoch 1100: total training loss 10.03\n",
            "2021-02-01 11:46:11,871 - INFO - joeynmt.training - EPOCH 1101\n",
            "2021-02-01 11:46:24,357 - INFO - joeynmt.training - Epoch 1101: total training loss 9.96\n",
            "2021-02-01 11:46:24,357 - INFO - joeynmt.training - EPOCH 1102\n",
            "2021-02-01 11:46:36,887 - INFO - joeynmt.training - Epoch 1102: total training loss 10.00\n",
            "2021-02-01 11:46:36,888 - INFO - joeynmt.training - EPOCH 1103\n",
            "2021-02-01 11:46:49,304 - INFO - joeynmt.training - Epoch 1103: total training loss 9.97\n",
            "2021-02-01 11:46:49,304 - INFO - joeynmt.training - EPOCH 1104\n",
            "2021-02-01 11:46:59,034 - INFO - joeynmt.training - Epoch 1104, Step:    16500, Batch Loss:     0.635364, Tokens per Sec:    18529, Lr: 0.000103\n",
            "2021-02-01 11:47:26,395 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:47:26,396 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:47:26,396 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:47:26,397 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына алып кетіп, сол жерде қалаға жіберіп отырды.\n",
            "2021-02-01 11:47:26,397 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:47:26,397 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:47:26,397 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:47:26,398 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап түсіп, Оны оятты.\n",
            "2021-02-01 11:47:26,398 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:47:26,398 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:47:26,398 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:47:26,399 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, бас тарту етінен шыққан осы Ізгі хабарын※ ұшырап, қатты аштыр екен. Үй көрмей қатты ашты,※ Құдайды мадақтап отырған жеті мың нештеңе болмадым.\n",
            "2021-02-01 11:47:26,399 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:47:26,399 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:47:26,399 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:47:26,399 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болыңдар: Ол жер бетінде орналасқан жердің оң жағындағы құрметті орнына жайғасты.\n",
            "2021-02-01 11:47:26,400 - INFO - joeynmt.training - Validation result (greedy) at epoch 1104, step    16500: bleu:   4.72, loss: 87427.2812, ppl:  77.5326, duration: 27.3655s\n",
            "2021-02-01 11:47:29,140 - INFO - joeynmt.training - Epoch 1104: total training loss 9.96\n",
            "2021-02-01 11:47:29,141 - INFO - joeynmt.training - EPOCH 1105\n",
            "2021-02-01 11:47:41,581 - INFO - joeynmt.training - Epoch 1105: total training loss 9.97\n",
            "2021-02-01 11:47:41,581 - INFO - joeynmt.training - EPOCH 1106\n",
            "2021-02-01 11:47:53,830 - INFO - joeynmt.training - Epoch 1106: total training loss 10.00\n",
            "2021-02-01 11:47:53,830 - INFO - joeynmt.training - EPOCH 1107\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 11:48:06,105 - INFO - joeynmt.training - Epoch 1107: total training loss 9.93\n",
            "2021-02-01 11:48:06,105 - INFO - joeynmt.training - EPOCH 1108\n",
            "2021-02-01 11:48:18,487 - INFO - joeynmt.training - Epoch 1108: total training loss 9.99\n",
            "2021-02-01 11:48:18,487 - INFO - joeynmt.training - EPOCH 1109\n",
            "2021-02-01 11:48:30,842 - INFO - joeynmt.training - Epoch 1109: total training loss 9.99\n",
            "2021-02-01 11:48:30,843 - INFO - joeynmt.training - EPOCH 1110\n",
            "2021-02-01 11:48:43,191 - INFO - joeynmt.training - Epoch 1110: total training loss 9.96\n",
            "2021-02-01 11:48:43,192 - INFO - joeynmt.training - EPOCH 1111\n",
            "2021-02-01 11:48:48,914 - INFO - joeynmt.training - Epoch 1111, Step:    16600, Batch Loss:     0.627943, Tokens per Sec:    17840, Lr: 0.000103\n",
            "2021-02-01 11:48:55,703 - INFO - joeynmt.training - Epoch 1111: total training loss 9.98\n",
            "2021-02-01 11:48:55,703 - INFO - joeynmt.training - EPOCH 1112\n",
            "2021-02-01 11:49:08,136 - INFO - joeynmt.training - Epoch 1112: total training loss 9.98\n",
            "2021-02-01 11:49:08,136 - INFO - joeynmt.training - EPOCH 1113\n",
            "2021-02-01 11:49:20,436 - INFO - joeynmt.training - Epoch 1113: total training loss 9.96\n",
            "2021-02-01 11:49:20,436 - INFO - joeynmt.training - EPOCH 1114\n",
            "2021-02-01 11:49:32,895 - INFO - joeynmt.training - Epoch 1114: total training loss 9.95\n",
            "2021-02-01 11:49:32,896 - INFO - joeynmt.training - EPOCH 1115\n",
            "2021-02-01 11:49:45,650 - INFO - joeynmt.training - Epoch 1115: total training loss 9.97\n",
            "2021-02-01 11:49:45,651 - INFO - joeynmt.training - EPOCH 1116\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
            "2021-02-01 11:49:58,205 - INFO - joeynmt.training - Epoch 1116: total training loss 9.90\n",
            "2021-02-01 11:49:58,205 - INFO - joeynmt.training - EPOCH 1117\n",
            "2021-02-01 11:50:10,627 - INFO - joeynmt.training - Epoch 1117: total training loss 10.00\n",
            "2021-02-01 11:50:10,628 - INFO - joeynmt.training - EPOCH 1118\n",
            "2021-02-01 11:50:12,285 - INFO - joeynmt.training - Epoch 1118, Step:    16700, Batch Loss:     0.637110, Tokens per Sec:    16869, Lr: 0.000103\n",
            "2021-02-01 11:50:23,124 - INFO - joeynmt.training - Epoch 1118: total training loss 9.91\n",
            "2021-02-01 11:50:23,125 - INFO - joeynmt.training - EPOCH 1119\n",
            "2021-02-01 11:50:35,511 - INFO - joeynmt.training - Epoch 1119: total training loss 9.92\n",
            "2021-02-01 11:50:35,511 - INFO - joeynmt.training - EPOCH 1120\n",
            "2021-02-01 11:50:47,881 - INFO - joeynmt.training - Epoch 1120: total training loss 9.91\n",
            "2021-02-01 11:50:47,881 - INFO - joeynmt.training - EPOCH 1121\n",
            "2021-02-01 11:51:00,274 - INFO - joeynmt.training - Epoch 1121: total training loss 9.97\n",
            "2021-02-01 11:51:00,275 - INFO - joeynmt.training - EPOCH 1122\n",
            "2021-02-01 11:51:12,369 - INFO - joeynmt.training - Epoch 1122: total training loss 9.31\n",
            "2021-02-01 11:51:12,369 - INFO - joeynmt.training - EPOCH 1123\n",
            "2021-02-01 11:51:24,885 - INFO - joeynmt.training - Epoch 1123: total training loss 9.92\n",
            "2021-02-01 11:51:24,886 - INFO - joeynmt.training - EPOCH 1124\n",
            "2021-02-01 11:51:35,505 - INFO - joeynmt.training - Epoch 1124, Step:    16800, Batch Loss:     0.678895, Tokens per Sec:    18501, Lr: 0.000103\n",
            "2021-02-01 11:51:37,243 - INFO - joeynmt.training - Epoch 1124: total training loss 9.96\n",
            "2021-02-01 11:51:37,244 - INFO - joeynmt.training - EPOCH 1125\n",
            "2021-02-01 11:51:49,611 - INFO - joeynmt.training - Epoch 1125: total training loss 9.93\n",
            "2021-02-01 11:51:49,611 - INFO - joeynmt.training - EPOCH 1126\n",
            "2021-02-01 11:52:01,860 - INFO - joeynmt.training - Epoch 1126: total training loss 9.26\n",
            "2021-02-01 11:52:01,860 - INFO - joeynmt.training - EPOCH 1127\n",
            "2021-02-01 11:52:14,133 - INFO - joeynmt.training - Epoch 1127: total training loss 9.94\n",
            "2021-02-01 11:52:14,133 - INFO - joeynmt.training - EPOCH 1128\n",
            "2021-02-01 11:52:26,415 - INFO - joeynmt.training - Epoch 1128: total training loss 9.89\n",
            "2021-02-01 11:52:26,416 - INFO - joeynmt.training - EPOCH 1129\n",
            "2021-02-01 11:52:38,681 - INFO - joeynmt.training - Epoch 1129: total training loss 9.95\n",
            "2021-02-01 11:52:38,681 - INFO - joeynmt.training - EPOCH 1130\n",
            "2021-02-01 11:52:51,454 - INFO - joeynmt.training - Epoch 1130: total training loss 9.94\n",
            "2021-02-01 11:52:51,454 - INFO - joeynmt.training - EPOCH 1131\n",
            "2021-02-01 11:52:58,836 - INFO - joeynmt.training - Epoch 1131, Step:    16900, Batch Loss:     0.634682, Tokens per Sec:    17876, Lr: 0.000103\n",
            "2021-02-01 11:53:04,007 - INFO - joeynmt.training - Epoch 1131: total training loss 9.89\n",
            "2021-02-01 11:53:04,008 - INFO - joeynmt.training - EPOCH 1132\n",
            "2021-02-01 11:53:16,336 - INFO - joeynmt.training - Epoch 1132: total training loss 9.92\n",
            "2021-02-01 11:53:16,336 - INFO - joeynmt.training - EPOCH 1133\n",
            "2021-02-01 11:53:29,013 - INFO - joeynmt.training - Epoch 1133: total training loss 9.89\n",
            "2021-02-01 11:53:29,013 - INFO - joeynmt.training - EPOCH 1134\n",
            "2021-02-01 11:53:41,513 - INFO - joeynmt.training - Epoch 1134: total training loss 9.88\n",
            "2021-02-01 11:53:41,514 - INFO - joeynmt.training - EPOCH 1135\n",
            "2021-02-01 11:53:54,078 - INFO - joeynmt.training - Epoch 1135: total training loss 9.90\n",
            "2021-02-01 11:53:54,079 - INFO - joeynmt.training - EPOCH 1136\n",
            "2021-02-01 11:54:06,445 - INFO - joeynmt.training - Epoch 1136: total training loss 9.88\n",
            "2021-02-01 11:54:06,445 - INFO - joeynmt.training - EPOCH 1137\n",
            "2021-02-01 11:54:18,900 - INFO - joeynmt.training - Epoch 1137: total training loss 9.92\n",
            "2021-02-01 11:54:18,900 - INFO - joeynmt.training - EPOCH 1138\n",
            "2021-02-01 11:54:22,172 - INFO - joeynmt.training - Epoch 1138, Step:    17000, Batch Loss:     0.678747, Tokens per Sec:    17751, Lr: 0.000103\n",
            "2021-02-01 11:54:50,303 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 11:54:50,304 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 11:54:50,304 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 11:54:50,304 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына ертіп қалаға жіберіп отырды.\n",
            "2021-02-01 11:54:50,305 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 11:54:50,305 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 11:54:50,305 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 11:54:50,305 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап беруге мәжбүрей деп айқайлап жатты.\n",
            "2021-02-01 11:54:50,305 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 11:54:50,306 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 11:54:50,306 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 11:54:50,306 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың ескі күнәкар болмыстарына беріліп, ең алдымен бар мал-мүлігін көрдім. Ол Тоқтының үстінде отырған жеті патшалық көрме! «Тақсыр, Құдайды мадақтады деп тапты!» — деп айқайлады. Олар айуанның есімі мен ашуланды.\n",
            "2021-02-01 11:54:50,307 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 11:54:50,307 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 11:54:50,307 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 11:54:50,307 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болыңдар: Ол жер бетіндегі барлық жерді оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 11:54:50,307 - INFO - joeynmt.training - Validation result (greedy) at epoch 1138, step    17000: bleu:   5.02, loss: 87417.6797, ppl:  77.4955, duration: 28.1355s\n",
            "2021-02-01 11:54:59,526 - INFO - joeynmt.training - Epoch 1138: total training loss 9.92\n",
            "2021-02-01 11:54:59,526 - INFO - joeynmt.training - EPOCH 1139\n",
            "2021-02-01 11:55:11,938 - INFO - joeynmt.training - Epoch 1139: total training loss 9.89\n",
            "2021-02-01 11:55:11,939 - INFO - joeynmt.training - EPOCH 1140\n",
            "2021-02-01 11:55:24,253 - INFO - joeynmt.training - Epoch 1140: total training loss 9.91\n",
            "2021-02-01 11:55:24,253 - INFO - joeynmt.training - EPOCH 1141\n",
            "2021-02-01 11:55:36,663 - INFO - joeynmt.training - Epoch 1141: total training loss 9.88\n",
            "2021-02-01 11:55:36,664 - INFO - joeynmt.training - EPOCH 1142\n",
            "2021-02-01 11:55:49,193 - INFO - joeynmt.training - Epoch 1142: total training loss 9.89\n",
            "2021-02-01 11:55:49,193 - INFO - joeynmt.training - EPOCH 1143\n",
            "2021-02-01 11:56:01,622 - INFO - joeynmt.training - Epoch 1143: total training loss 9.93\n",
            "2021-02-01 11:56:01,622 - INFO - joeynmt.training - EPOCH 1144\n",
            "2021-02-01 11:56:13,028 - INFO - joeynmt.training - Epoch 1144, Step:    17100, Batch Loss:     0.691941, Tokens per Sec:    18238, Lr: 0.000103\n",
            "2021-02-01 11:56:14,162 - INFO - joeynmt.training - Epoch 1144: total training loss 9.86\n",
            "2021-02-01 11:56:14,162 - INFO - joeynmt.training - EPOCH 1145\n",
            "2021-02-01 11:56:26,525 - INFO - joeynmt.training - Epoch 1145: total training loss 9.89\n",
            "2021-02-01 11:56:26,525 - INFO - joeynmt.training - EPOCH 1146\n",
            "2021-02-01 11:56:38,798 - INFO - joeynmt.training - Epoch 1146: total training loss 9.89\n",
            "2021-02-01 11:56:38,799 - INFO - joeynmt.training - EPOCH 1147\n",
            "2021-02-01 11:56:51,400 - INFO - joeynmt.training - Epoch 1147: total training loss 9.85\n",
            "2021-02-01 11:56:51,401 - INFO - joeynmt.training - EPOCH 1148\n",
            "2021-02-01 11:57:03,922 - INFO - joeynmt.training - Epoch 1148: total training loss 9.88\n",
            "2021-02-01 11:57:03,922 - INFO - joeynmt.training - EPOCH 1149\n",
            "2021-02-01 11:57:16,417 - INFO - joeynmt.training - Epoch 1149: total training loss 9.85\n",
            "2021-02-01 11:57:16,418 - INFO - joeynmt.training - EPOCH 1150\n",
            "2021-02-01 11:57:28,722 - INFO - joeynmt.training - Epoch 1150: total training loss 9.89\n",
            "2021-02-01 11:57:28,722 - INFO - joeynmt.training - EPOCH 1151\n",
            "2021-02-01 11:57:36,053 - INFO - joeynmt.training - Epoch 1151, Step:    17200, Batch Loss:     0.642853, Tokens per Sec:    17700, Lr: 0.000103\n",
            "2021-02-01 11:57:41,247 - INFO - joeynmt.training - Epoch 1151: total training loss 9.79\n",
            "2021-02-01 11:57:41,248 - INFO - joeynmt.training - EPOCH 1152\n",
            "2021-02-01 11:57:53,740 - INFO - joeynmt.training - Epoch 1152: total training loss 9.85\n",
            "2021-02-01 11:57:53,741 - INFO - joeynmt.training - EPOCH 1153\n",
            "2021-02-01 11:58:06,231 - INFO - joeynmt.training - Epoch 1153: total training loss 9.85\n",
            "2021-02-01 11:58:06,232 - INFO - joeynmt.training - EPOCH 1154\n",
            "2021-02-01 11:58:18,555 - INFO - joeynmt.training - Epoch 1154: total training loss 9.91\n",
            "2021-02-01 11:58:18,555 - INFO - joeynmt.training - EPOCH 1155\n",
            "2021-02-01 11:58:30,961 - INFO - joeynmt.training - Epoch 1155: total training loss 9.85\n",
            "2021-02-01 11:58:30,962 - INFO - joeynmt.training - EPOCH 1156\n",
            "2021-02-01 11:58:43,186 - INFO - joeynmt.training - Epoch 1156: total training loss 9.85\n",
            "2021-02-01 11:58:43,187 - INFO - joeynmt.training - EPOCH 1157\n",
            "2021-02-01 11:58:55,485 - INFO - joeynmt.training - Epoch 1157: total training loss 9.87\n",
            "2021-02-01 11:58:55,485 - INFO - joeynmt.training - EPOCH 1158\n",
            "2021-02-01 11:58:58,795 - INFO - joeynmt.training - Epoch 1158, Step:    17300, Batch Loss:     0.714833, Tokens per Sec:    18640, Lr: 0.000103\n",
            "2021-02-01 11:59:07,811 - INFO - joeynmt.training - Epoch 1158: total training loss 9.84\n",
            "2021-02-01 11:59:07,812 - INFO - joeynmt.training - EPOCH 1159\n",
            "2021-02-01 11:59:20,202 - INFO - joeynmt.training - Epoch 1159: total training loss 9.80\n",
            "2021-02-01 11:59:20,203 - INFO - joeynmt.training - EPOCH 1160\n",
            "2021-02-01 11:59:32,548 - INFO - joeynmt.training - Epoch 1160: total training loss 9.83\n",
            "2021-02-01 11:59:32,549 - INFO - joeynmt.training - EPOCH 1161\n",
            "2021-02-01 11:59:45,133 - INFO - joeynmt.training - Epoch 1161: total training loss 9.83\n",
            "2021-02-01 11:59:45,134 - INFO - joeynmt.training - EPOCH 1162\n",
            "2021-02-01 11:59:57,439 - INFO - joeynmt.training - Epoch 1162: total training loss 9.83\n",
            "2021-02-01 11:59:57,439 - INFO - joeynmt.training - EPOCH 1163\n",
            "2021-02-01 12:00:09,796 - INFO - joeynmt.training - Epoch 1163: total training loss 9.85\n",
            "2021-02-01 12:00:09,796 - INFO - joeynmt.training - EPOCH 1164\n",
            "2021-02-01 12:00:21,139 - INFO - joeynmt.training - Epoch 1164, Step:    17400, Batch Loss:     0.658156, Tokens per Sec:    18972, Lr: 0.000103\n",
            "2021-02-01 12:00:22,047 - INFO - joeynmt.training - Epoch 1164: total training loss 9.83\n",
            "2021-02-01 12:00:22,048 - INFO - joeynmt.training - EPOCH 1165\n",
            "2021-02-01 12:00:34,593 - INFO - joeynmt.training - Epoch 1165: total training loss 9.84\n",
            "2021-02-01 12:00:34,593 - INFO - joeynmt.training - EPOCH 1166\n",
            "2021-02-01 12:00:46,909 - INFO - joeynmt.training - Epoch 1166: total training loss 9.82\n",
            "2021-02-01 12:00:46,909 - INFO - joeynmt.training - EPOCH 1167\n",
            "2021-02-01 12:00:59,208 - INFO - joeynmt.training - Epoch 1167: total training loss 9.83\n",
            "2021-02-01 12:00:59,209 - INFO - joeynmt.training - EPOCH 1168\n",
            "2021-02-01 12:01:11,712 - INFO - joeynmt.training - Epoch 1168: total training loss 9.84\n",
            "2021-02-01 12:01:11,712 - INFO - joeynmt.training - EPOCH 1169\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:01:24,187 - INFO - joeynmt.training - Epoch 1169: total training loss 9.78\n",
            "2021-02-01 12:01:24,187 - INFO - joeynmt.training - EPOCH 1170\n",
            "2021-02-01 12:01:36,472 - INFO - joeynmt.training - Epoch 1170: total training loss 9.82\n",
            "2021-02-01 12:01:36,472 - INFO - joeynmt.training - EPOCH 1171\n",
            "2021-02-01 12:01:43,872 - INFO - joeynmt.training - Epoch 1171, Step:    17500, Batch Loss:     0.666394, Tokens per Sec:    18675, Lr: 0.000103\n",
            "2021-02-01 12:02:10,706 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:02:10,707 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға жіберіп отырғанда, сол жерде демалыс күні мәжілісханаларға жіберді.※\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу оятты. Сол жерде Оны өлтіргісі деп үміттенді.\n",
            "2021-02-01 12:02:10,708 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, оны ең алдымен бар деп санаймын. Тоқтының үстінде отырған Құдай осы дүниені көрдім. Олар қатты ашылыс болып, Құдайды мадақтап жүрдім.\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:02:10,709 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене болыңдар: Ол жер бетінде орналасқан жердің оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 12:02:10,710 - INFO - joeynmt.training - Validation result (greedy) at epoch 1171, step    17500: bleu:   5.33, loss: 87484.5000, ppl:  77.7537, duration: 26.8366s\n",
            "2021-02-01 12:02:15,645 - INFO - joeynmt.training - Epoch 1171: total training loss 9.80\n",
            "2021-02-01 12:02:15,645 - INFO - joeynmt.training - EPOCH 1172\n",
            "2021-02-01 12:02:28,108 - INFO - joeynmt.training - Epoch 1172: total training loss 9.80\n",
            "2021-02-01 12:02:28,109 - INFO - joeynmt.training - EPOCH 1173\n",
            "2021-02-01 12:02:40,576 - INFO - joeynmt.training - Epoch 1173: total training loss 9.75\n",
            "2021-02-01 12:02:40,576 - INFO - joeynmt.training - EPOCH 1174\n",
            "2021-02-01 12:02:52,972 - INFO - joeynmt.training - Epoch 1174: total training loss 9.76\n",
            "2021-02-01 12:02:52,972 - INFO - joeynmt.training - EPOCH 1175\n",
            "2021-02-01 12:03:05,058 - INFO - joeynmt.training - Epoch 1175: total training loss 9.23\n",
            "2021-02-01 12:03:05,058 - INFO - joeynmt.training - EPOCH 1176\n",
            "2021-02-01 12:03:17,547 - INFO - joeynmt.training - Epoch 1176: total training loss 9.74\n",
            "2021-02-01 12:03:17,547 - INFO - joeynmt.training - EPOCH 1177\n",
            "2021-02-01 12:03:29,821 - INFO - joeynmt.training - Epoch 1177: total training loss 9.79\n",
            "2021-02-01 12:03:29,821 - INFO - joeynmt.training - EPOCH 1178\n",
            "2021-02-01 12:03:33,901 - INFO - joeynmt.training - Epoch 1178, Step:    17600, Batch Loss:     0.642704, Tokens per Sec:    18378, Lr: 0.000103\n",
            "2021-02-01 12:03:42,167 - INFO - joeynmt.training - Epoch 1178: total training loss 9.78\n",
            "2021-02-01 12:03:42,167 - INFO - joeynmt.training - EPOCH 1179\n",
            "2021-02-01 12:03:54,494 - INFO - joeynmt.training - Epoch 1179: total training loss 9.78\n",
            "2021-02-01 12:03:54,494 - INFO - joeynmt.training - EPOCH 1180\n",
            "2021-02-01 12:04:06,794 - INFO - joeynmt.training - Epoch 1180: total training loss 9.79\n",
            "2021-02-01 12:04:06,794 - INFO - joeynmt.training - EPOCH 1181\n",
            "2021-02-01 12:04:18,926 - INFO - joeynmt.training - Epoch 1181: total training loss 9.04\n",
            "2021-02-01 12:04:18,927 - INFO - joeynmt.training - EPOCH 1182\n",
            "2021-02-01 12:04:31,271 - INFO - joeynmt.training - Epoch 1182: total training loss 9.76\n",
            "2021-02-01 12:04:31,272 - INFO - joeynmt.training - EPOCH 1183\n",
            "2021-02-01 12:04:43,608 - INFO - joeynmt.training - Epoch 1183: total training loss 9.75\n",
            "2021-02-01 12:04:43,608 - INFO - joeynmt.training - EPOCH 1184\n",
            "2021-02-01 12:04:55,710 - INFO - joeynmt.training - Epoch 1184: total training loss 9.14\n",
            "2021-02-01 12:04:55,710 - INFO - joeynmt.training - EPOCH 1185\n",
            "2021-02-01 12:04:57,360 - INFO - joeynmt.training - Epoch 1185, Step:    17700, Batch Loss:     0.651373, Tokens per Sec:    18281, Lr: 0.000103\n",
            "2021-02-01 12:05:07,998 - INFO - joeynmt.training - Epoch 1185: total training loss 9.72\n",
            "2021-02-01 12:05:07,999 - INFO - joeynmt.training - EPOCH 1186\n",
            "2021-02-01 12:05:20,252 - INFO - joeynmt.training - Epoch 1186: total training loss 9.76\n",
            "2021-02-01 12:05:20,253 - INFO - joeynmt.training - EPOCH 1187\n",
            "2021-02-01 12:05:32,620 - INFO - joeynmt.training - Epoch 1187: total training loss 9.73\n",
            "2021-02-01 12:05:32,621 - INFO - joeynmt.training - EPOCH 1188\n",
            "2021-02-01 12:05:44,940 - INFO - joeynmt.training - Epoch 1188: total training loss 9.77\n",
            "2021-02-01 12:05:44,941 - INFO - joeynmt.training - EPOCH 1189\n",
            "2021-02-01 12:05:57,221 - INFO - joeynmt.training - Epoch 1189: total training loss 9.72\n",
            "2021-02-01 12:05:57,221 - INFO - joeynmt.training - EPOCH 1190\n",
            "2021-02-01 12:06:09,475 - INFO - joeynmt.training - Epoch 1190: total training loss 9.78\n",
            "2021-02-01 12:06:09,476 - INFO - joeynmt.training - EPOCH 1191\n",
            "2021-02-01 12:06:19,165 - INFO - joeynmt.training - Epoch 1191, Step:    17800, Batch Loss:     0.621099, Tokens per Sec:    18213, Lr: 0.000103\n",
            "2021-02-01 12:06:21,832 - INFO - joeynmt.training - Epoch 1191: total training loss 9.72\n",
            "2021-02-01 12:06:21,832 - INFO - joeynmt.training - EPOCH 1192\n",
            "2021-02-01 12:06:34,283 - INFO - joeynmt.training - Epoch 1192: total training loss 9.76\n",
            "2021-02-01 12:06:34,284 - INFO - joeynmt.training - EPOCH 1193\n",
            "2021-02-01 12:06:46,582 - INFO - joeynmt.training - Epoch 1193: total training loss 9.75\n",
            "2021-02-01 12:06:46,582 - INFO - joeynmt.training - EPOCH 1194\n",
            "2021-02-01 12:06:58,836 - INFO - joeynmt.training - Epoch 1194: total training loss 9.72\n",
            "2021-02-01 12:06:58,836 - INFO - joeynmt.training - EPOCH 1195\n",
            "2021-02-01 12:07:11,201 - INFO - joeynmt.training - Epoch 1195: total training loss 9.69\n",
            "2021-02-01 12:07:11,201 - INFO - joeynmt.training - EPOCH 1196\n",
            "2021-02-01 12:07:23,510 - INFO - joeynmt.training - Epoch 1196: total training loss 9.71\n",
            "2021-02-01 12:07:23,511 - INFO - joeynmt.training - EPOCH 1197\n",
            "2021-02-01 12:07:35,772 - INFO - joeynmt.training - Epoch 1197: total training loss 9.72\n",
            "2021-02-01 12:07:35,772 - INFO - joeynmt.training - EPOCH 1198\n",
            "2021-02-01 12:07:41,531 - INFO - joeynmt.training - Epoch 1198, Step:    17900, Batch Loss:     0.653593, Tokens per Sec:    19006, Lr: 0.000103\n",
            "2021-02-01 12:07:48,150 - INFO - joeynmt.training - Epoch 1198: total training loss 9.75\n",
            "2021-02-01 12:07:48,151 - INFO - joeynmt.training - EPOCH 1199\n",
            "2021-02-01 12:08:00,511 - INFO - joeynmt.training - Epoch 1199: total training loss 9.73\n",
            "2021-02-01 12:08:00,511 - INFO - joeynmt.training - EPOCH 1200\n",
            "2021-02-01 12:08:12,911 - INFO - joeynmt.training - Epoch 1200: total training loss 9.70\n",
            "2021-02-01 12:08:12,912 - INFO - joeynmt.training - EPOCH 1201\n",
            "2021-02-01 12:08:25,587 - INFO - joeynmt.training - Epoch 1201: total training loss 9.73\n",
            "2021-02-01 12:08:25,588 - INFO - joeynmt.training - EPOCH 1202\n",
            "2021-02-01 12:08:37,853 - INFO - joeynmt.training - Epoch 1202: total training loss 9.73\n",
            "2021-02-01 12:08:37,853 - INFO - joeynmt.training - EPOCH 1203\n",
            "2021-02-01 12:08:50,393 - INFO - joeynmt.training - Epoch 1203: total training loss 9.70\n",
            "2021-02-01 12:08:50,394 - INFO - joeynmt.training - EPOCH 1204\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 12:09:02,650 - INFO - joeynmt.training - Epoch 1204: total training loss 9.64\n",
            "2021-02-01 12:09:02,650 - INFO - joeynmt.training - EPOCH 1205\n",
            "2021-02-01 12:09:04,323 - INFO - joeynmt.training - Epoch 1205, Step:    18000, Batch Loss:     0.650351, Tokens per Sec:    17956, Lr: 0.000103\n",
            "2021-02-01 12:09:33,717 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:09:33,719 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:09:33,719 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:09:33,719 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы алдын ала айтылған демалыс күні мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 12:09:33,719 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:09:33,720 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:09:33,720 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:09:33,720 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап түсіп, Оны оятты. Сол жерде оған Исаны ұстап беруге жасақшылардың мәйітін деп ойлап қалды.\n",
            "2021-02-01 12:09:33,720 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:09:33,727 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:09:33,727 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:09:33,728 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқтайтын адам сылдым. Ол Тоқтының үстінде отырған Құдайды көрмей қатты ашты,※ Құдайды мадақтап тілген жеті періште қатты аштым.\n",
            "2021-02-01 12:09:33,728 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:09:33,728 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:09:33,728 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:09:33,728 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 12:09:33,729 - INFO - joeynmt.training - Validation result (greedy) at epoch 1205, step    18000: bleu:   5.02, loss: 87415.6797, ppl:  77.4878, duration: 29.4050s\n",
            "2021-02-01 12:09:44,461 - INFO - joeynmt.training - Epoch 1205: total training loss 9.77\n",
            "2021-02-01 12:09:44,462 - INFO - joeynmt.training - EPOCH 1206\n",
            "2021-02-01 12:09:56,975 - INFO - joeynmt.training - Epoch 1206: total training loss 9.66\n",
            "2021-02-01 12:09:56,975 - INFO - joeynmt.training - EPOCH 1207\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:10:09,333 - INFO - joeynmt.training - Epoch 1207: total training loss 9.68\n",
            "2021-02-01 12:10:09,333 - INFO - joeynmt.training - EPOCH 1208\n",
            "2021-02-01 12:10:21,768 - INFO - joeynmt.training - Epoch 1208: total training loss 9.66\n",
            "2021-02-01 12:10:21,768 - INFO - joeynmt.training - EPOCH 1209\n",
            "2021-02-01 12:10:34,171 - INFO - joeynmt.training - Epoch 1209: total training loss 9.65\n",
            "2021-02-01 12:10:34,172 - INFO - joeynmt.training - EPOCH 1210\n",
            "2021-02-01 12:10:46,474 - INFO - joeynmt.training - Epoch 1210: total training loss 9.64\n",
            "2021-02-01 12:10:46,474 - INFO - joeynmt.training - EPOCH 1211\n",
            "2021-02-01 12:10:56,259 - INFO - joeynmt.training - Epoch 1211, Step:    18100, Batch Loss:     0.620649, Tokens per Sec:    18886, Lr: 0.000072\n",
            "2021-02-01 12:10:58,710 - INFO - joeynmt.training - Epoch 1211: total training loss 9.68\n",
            "2021-02-01 12:10:58,711 - INFO - joeynmt.training - EPOCH 1212\n",
            "2021-02-01 12:11:10,946 - INFO - joeynmt.training - Epoch 1212: total training loss 9.67\n",
            "2021-02-01 12:11:10,947 - INFO - joeynmt.training - EPOCH 1213\n",
            "2021-02-01 12:11:23,231 - INFO - joeynmt.training - Epoch 1213: total training loss 9.63\n",
            "2021-02-01 12:11:23,231 - INFO - joeynmt.training - EPOCH 1214\n",
            "2021-02-01 12:11:35,639 - INFO - joeynmt.training - Epoch 1214: total training loss 9.64\n",
            "2021-02-01 12:11:35,639 - INFO - joeynmt.training - EPOCH 1215\n",
            "2021-02-01 12:11:48,088 - INFO - joeynmt.training - Epoch 1215: total training loss 9.64\n",
            "2021-02-01 12:11:48,088 - INFO - joeynmt.training - EPOCH 1216\n",
            "2021-02-01 12:12:00,303 - INFO - joeynmt.training - Epoch 1216: total training loss 9.64\n",
            "2021-02-01 12:12:00,304 - INFO - joeynmt.training - EPOCH 1217\n",
            "2021-02-01 12:12:12,578 - INFO - joeynmt.training - Epoch 1217: total training loss 9.66\n",
            "2021-02-01 12:12:12,578 - INFO - joeynmt.training - EPOCH 1218\n",
            "2021-02-01 12:12:18,441 - INFO - joeynmt.training - Epoch 1218, Step:    18200, Batch Loss:     0.649824, Tokens per Sec:    18353, Lr: 0.000072\n",
            "2021-02-01 12:12:24,999 - INFO - joeynmt.training - Epoch 1218: total training loss 9.63\n",
            "2021-02-01 12:12:24,999 - INFO - joeynmt.training - EPOCH 1219\n",
            "2021-02-01 12:12:37,366 - INFO - joeynmt.training - Epoch 1219: total training loss 9.62\n",
            "2021-02-01 12:12:37,367 - INFO - joeynmt.training - EPOCH 1220\n",
            "2021-02-01 12:12:49,547 - INFO - joeynmt.training - Epoch 1220: total training loss 8.97\n",
            "2021-02-01 12:12:49,547 - INFO - joeynmt.training - EPOCH 1221\n",
            "2021-02-01 12:13:01,981 - INFO - joeynmt.training - Epoch 1221: total training loss 9.63\n",
            "2021-02-01 12:13:01,981 - INFO - joeynmt.training - EPOCH 1222\n",
            "2021-02-01 12:13:14,177 - INFO - joeynmt.training - Epoch 1222: total training loss 8.96\n",
            "2021-02-01 12:13:14,178 - INFO - joeynmt.training - EPOCH 1223\n",
            "2021-02-01 12:13:26,631 - INFO - joeynmt.training - Epoch 1223: total training loss 9.55\n",
            "2021-02-01 12:13:26,631 - INFO - joeynmt.training - EPOCH 1224\n",
            "2021-02-01 12:13:39,041 - INFO - joeynmt.training - Epoch 1224: total training loss 9.64\n",
            "2021-02-01 12:13:39,041 - INFO - joeynmt.training - EPOCH 1225\n",
            "2021-02-01 12:13:42,285 - INFO - joeynmt.training - Epoch 1225, Step:    18300, Batch Loss:     0.602627, Tokens per Sec:    18576, Lr: 0.000072\n",
            "2021-02-01 12:13:51,186 - INFO - joeynmt.training - Epoch 1225: total training loss 8.94\n",
            "2021-02-01 12:13:51,187 - INFO - joeynmt.training - EPOCH 1226\n",
            "2021-02-01 12:14:03,531 - INFO - joeynmt.training - Epoch 1226: total training loss 9.59\n",
            "2021-02-01 12:14:03,531 - INFO - joeynmt.training - EPOCH 1227\n",
            "2021-02-01 12:14:15,777 - INFO - joeynmt.training - Epoch 1227: total training loss 9.59\n",
            "2021-02-01 12:14:15,777 - INFO - joeynmt.training - EPOCH 1228\n",
            "2021-02-01 12:14:28,140 - INFO - joeynmt.training - Epoch 1228: total training loss 9.62\n",
            "2021-02-01 12:14:28,140 - INFO - joeynmt.training - EPOCH 1229\n",
            "2021-02-01 12:14:40,344 - INFO - joeynmt.training - Epoch 1229: total training loss 9.59\n",
            "2021-02-01 12:14:40,344 - INFO - joeynmt.training - EPOCH 1230\n",
            "2021-02-01 12:14:52,740 - INFO - joeynmt.training - Epoch 1230: total training loss 9.57\n",
            "2021-02-01 12:14:52,741 - INFO - joeynmt.training - EPOCH 1231\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:15:04,809 - INFO - joeynmt.training - Epoch 1231, Step:    18400, Batch Loss:     0.681188, Tokens per Sec:    18483, Lr: 0.000072\n",
            "2021-02-01 12:15:05,103 - INFO - joeynmt.training - Epoch 1231: total training loss 9.57\n",
            "2021-02-01 12:15:05,104 - INFO - joeynmt.training - EPOCH 1232\n",
            "2021-02-01 12:15:17,169 - INFO - joeynmt.training - Epoch 1232: total training loss 8.96\n",
            "2021-02-01 12:15:17,169 - INFO - joeynmt.training - EPOCH 1233\n",
            "2021-02-01 12:15:29,784 - INFO - joeynmt.training - Epoch 1233: total training loss 9.61\n",
            "2021-02-01 12:15:29,785 - INFO - joeynmt.training - EPOCH 1234\n",
            "2021-02-01 12:15:42,341 - INFO - joeynmt.training - Epoch 1234: total training loss 9.58\n",
            "2021-02-01 12:15:42,341 - INFO - joeynmt.training - EPOCH 1235\n",
            "2021-02-01 12:15:54,826 - INFO - joeynmt.training - Epoch 1235: total training loss 9.56\n",
            "2021-02-01 12:15:54,827 - INFO - joeynmt.training - EPOCH 1236\n",
            "2021-02-01 12:16:07,347 - INFO - joeynmt.training - Epoch 1236: total training loss 9.56\n",
            "2021-02-01 12:16:07,347 - INFO - joeynmt.training - EPOCH 1237\n",
            "2021-02-01 12:16:19,678 - INFO - joeynmt.training - Epoch 1237: total training loss 9.58\n",
            "2021-02-01 12:16:19,678 - INFO - joeynmt.training - EPOCH 1238\n",
            "2021-02-01 12:16:28,654 - INFO - joeynmt.training - Epoch 1238, Step:    18500, Batch Loss:     0.630125, Tokens per Sec:    18607, Lr: 0.000072\n",
            "2021-02-01 12:16:55,925 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:16:55,926 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:16:55,926 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:16:55,926 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына ертіп келді. Олар мәжілісхананың мәжілісханаларында дүре соғып келді.※\n",
            "2021-02-01 12:16:55,926 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:16:55,927 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:16:55,927 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:16:55,927 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 12:16:55,927 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есіне (яғни Құдай халқын) ең алдымен бас тартқан оған ілескен періштесі көрдім. Олар қатты ашты) Құдайды мадақтап отырған жеті періште көрініп, Құдайды мадақтап күкірмей алмайтын ешқандай кіністі заттарын※ ұғындым.\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 12:16:55,928 - INFO - joeynmt.training - Validation result (greedy) at epoch 1238, step    18500: bleu:   4.89, loss: 87493.6094, ppl:  77.7889, duration: 27.2744s\n",
            "2021-02-01 12:16:59,187 - INFO - joeynmt.training - Epoch 1238: total training loss 9.60\n",
            "2021-02-01 12:16:59,188 - INFO - joeynmt.training - EPOCH 1239\n",
            "2021-02-01 12:17:11,446 - INFO - joeynmt.training - Epoch 1239: total training loss 9.59\n",
            "2021-02-01 12:17:11,446 - INFO - joeynmt.training - EPOCH 1240\n",
            "2021-02-01 12:17:23,914 - INFO - joeynmt.training - Epoch 1240: total training loss 9.55\n",
            "2021-02-01 12:17:23,914 - INFO - joeynmt.training - EPOCH 1241\n",
            "2021-02-01 12:17:36,188 - INFO - joeynmt.training - Epoch 1241: total training loss 9.59\n",
            "2021-02-01 12:17:36,188 - INFO - joeynmt.training - EPOCH 1242\n",
            "2021-02-01 12:17:48,535 - INFO - joeynmt.training - Epoch 1242: total training loss 9.55\n",
            "2021-02-01 12:17:48,535 - INFO - joeynmt.training - EPOCH 1243\n",
            "2021-02-01 12:18:00,855 - INFO - joeynmt.training - Epoch 1243: total training loss 9.56\n",
            "2021-02-01 12:18:00,856 - INFO - joeynmt.training - EPOCH 1244\n",
            "2021-02-01 12:18:13,323 - INFO - joeynmt.training - Epoch 1244: total training loss 9.53\n",
            "2021-02-01 12:18:13,323 - INFO - joeynmt.training - EPOCH 1245\n",
            "2021-02-01 12:18:18,215 - INFO - joeynmt.training - Epoch 1245, Step:    18600, Batch Loss:     0.640144, Tokens per Sec:    18066, Lr: 0.000072\n",
            "2021-02-01 12:18:25,680 - INFO - joeynmt.training - Epoch 1245: total training loss 9.56\n",
            "2021-02-01 12:18:25,681 - INFO - joeynmt.training - EPOCH 1246\n",
            "2021-02-01 12:18:37,967 - INFO - joeynmt.training - Epoch 1246: total training loss 9.56\n",
            "2021-02-01 12:18:37,968 - INFO - joeynmt.training - EPOCH 1247\n",
            "2021-02-01 12:18:50,295 - INFO - joeynmt.training - Epoch 1247: total training loss 9.51\n",
            "2021-02-01 12:18:50,296 - INFO - joeynmt.training - EPOCH 1248\n",
            "2021-02-01 12:19:02,622 - INFO - joeynmt.training - Epoch 1248: total training loss 9.52\n",
            "2021-02-01 12:19:02,622 - INFO - joeynmt.training - EPOCH 1249\n",
            "2021-02-01 12:19:14,930 - INFO - joeynmt.training - Epoch 1249: total training loss 9.53\n",
            "2021-02-01 12:19:14,931 - INFO - joeynmt.training - EPOCH 1250\n",
            "2021-02-01 12:19:27,361 - INFO - joeynmt.training - Epoch 1250: total training loss 9.57\n",
            "2021-02-01 12:19:27,362 - INFO - joeynmt.training - EPOCH 1251\n",
            "2021-02-01 12:19:39,595 - INFO - joeynmt.training - Epoch 1251: total training loss 9.55\n",
            "2021-02-01 12:19:39,596 - INFO - joeynmt.training - EPOCH 1252\n",
            "2021-02-01 12:19:40,470 - INFO - joeynmt.training - Epoch 1252, Step:    18700, Batch Loss:     0.661555, Tokens per Sec:    18800, Lr: 0.000072\n",
            "2021-02-01 12:19:52,139 - INFO - joeynmt.training - Epoch 1252: total training loss 9.52\n",
            "2021-02-01 12:19:52,139 - INFO - joeynmt.training - EPOCH 1253\n",
            "2021-02-01 12:20:04,536 - INFO - joeynmt.training - Epoch 1253: total training loss 9.54\n",
            "2021-02-01 12:20:04,536 - INFO - joeynmt.training - EPOCH 1254\n",
            "2021-02-01 12:20:16,933 - INFO - joeynmt.training - Epoch 1254: total training loss 9.56\n",
            "2021-02-01 12:20:16,933 - INFO - joeynmt.training - EPOCH 1255\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:20:29,231 - INFO - joeynmt.training - Epoch 1255: total training loss 9.55\n",
            "2021-02-01 12:20:29,231 - INFO - joeynmt.training - EPOCH 1256\n",
            "2021-02-01 12:20:41,533 - INFO - joeynmt.training - Epoch 1256: total training loss 9.49\n",
            "2021-02-01 12:20:41,533 - INFO - joeynmt.training - EPOCH 1257\n",
            "2021-02-01 12:20:54,061 - INFO - joeynmt.training - Epoch 1257: total training loss 9.52\n",
            "2021-02-01 12:20:54,061 - INFO - joeynmt.training - EPOCH 1258\n",
            "2021-02-01 12:21:03,043 - INFO - joeynmt.training - Epoch 1258, Step:    18800, Batch Loss:     0.579447, Tokens per Sec:    18592, Lr: 0.000072\n",
            "2021-02-01 12:21:06,421 - INFO - joeynmt.training - Epoch 1258: total training loss 9.50\n",
            "2021-02-01 12:21:06,421 - INFO - joeynmt.training - EPOCH 1259\n",
            "2021-02-01 12:21:18,925 - INFO - joeynmt.training - Epoch 1259: total training loss 9.54\n",
            "2021-02-01 12:21:18,926 - INFO - joeynmt.training - EPOCH 1260\n",
            "2021-02-01 12:21:31,333 - INFO - joeynmt.training - Epoch 1260: total training loss 9.52\n",
            "2021-02-01 12:21:31,333 - INFO - joeynmt.training - EPOCH 1261\n",
            "2021-02-01 12:21:43,529 - INFO - joeynmt.training - Epoch 1261: total training loss 9.55\n",
            "2021-02-01 12:21:43,530 - INFO - joeynmt.training - EPOCH 1262\n",
            "2021-02-01 12:21:55,764 - INFO - joeynmt.training - Epoch 1262: total training loss 9.53\n",
            "2021-02-01 12:21:55,765 - INFO - joeynmt.training - EPOCH 1263\n",
            "2021-02-01 12:22:08,108 - INFO - joeynmt.training - Epoch 1263: total training loss 9.52\n",
            "2021-02-01 12:22:08,108 - INFO - joeynmt.training - EPOCH 1264\n",
            "2021-02-01 12:22:20,445 - INFO - joeynmt.training - Epoch 1264: total training loss 9.49\n",
            "2021-02-01 12:22:20,446 - INFO - joeynmt.training - EPOCH 1265\n",
            "2021-02-01 12:22:25,374 - INFO - joeynmt.training - Epoch 1265, Step:    18900, Batch Loss:     0.619114, Tokens per Sec:    18848, Lr: 0.000072\n",
            "2021-02-01 12:22:32,768 - INFO - joeynmt.training - Epoch 1265: total training loss 9.52\n",
            "2021-02-01 12:22:32,769 - INFO - joeynmt.training - EPOCH 1266\n",
            "2021-02-01 12:22:45,273 - INFO - joeynmt.training - Epoch 1266: total training loss 9.54\n",
            "2021-02-01 12:22:45,273 - INFO - joeynmt.training - EPOCH 1267\n",
            "2021-02-01 12:22:57,467 - INFO - joeynmt.training - Epoch 1267: total training loss 9.47\n",
            "2021-02-01 12:22:57,467 - INFO - joeynmt.training - EPOCH 1268\n",
            "2021-02-01 12:23:09,899 - INFO - joeynmt.training - Epoch 1268: total training loss 9.51\n",
            "2021-02-01 12:23:09,899 - INFO - joeynmt.training - EPOCH 1269\n",
            "2021-02-01 12:23:22,261 - INFO - joeynmt.training - Epoch 1269: total training loss 9.49\n",
            "2021-02-01 12:23:22,262 - INFO - joeynmt.training - EPOCH 1270\n",
            "2021-02-01 12:23:34,441 - INFO - joeynmt.training - Epoch 1270: total training loss 8.88\n",
            "2021-02-01 12:23:34,441 - INFO - joeynmt.training - EPOCH 1271\n",
            "2021-02-01 12:23:46,810 - INFO - joeynmt.training - Epoch 1271: total training loss 9.52\n",
            "2021-02-01 12:23:46,810 - INFO - joeynmt.training - EPOCH 1272\n",
            "2021-02-01 12:23:48,485 - INFO - joeynmt.training - Epoch 1272, Step:    19000, Batch Loss:     0.653724, Tokens per Sec:    19763, Lr: 0.000072\n",
            "2021-02-01 12:24:16,808 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:24:16,808 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 12:24:16,809 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, адам сылта көрдім. Ол Тоқтының үстінде отырған Құдайды көрмей қатты ашты! «Тәңір Ие — шын мәнінде әйелдің балағайды етіп жүрдім.\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене де төсек теп, мекендердің оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 12:24:16,810 - INFO - joeynmt.training - Validation result (greedy) at epoch 1272, step    19000: bleu:   5.17, loss: 87407.6406, ppl:  77.4568, duration: 28.3250s\n",
            "2021-02-01 12:24:27,407 - INFO - joeynmt.training - Epoch 1272: total training loss 9.51\n",
            "2021-02-01 12:24:27,407 - INFO - joeynmt.training - EPOCH 1273\n",
            "2021-02-01 12:24:39,616 - INFO - joeynmt.training - Epoch 1273: total training loss 9.51\n",
            "2021-02-01 12:24:39,617 - INFO - joeynmt.training - EPOCH 1274\n",
            "2021-02-01 12:24:52,052 - INFO - joeynmt.training - Epoch 1274: total training loss 9.43\n",
            "2021-02-01 12:24:52,052 - INFO - joeynmt.training - EPOCH 1275\n",
            "2021-02-01 12:25:04,306 - INFO - joeynmt.training - Epoch 1275: total training loss 9.48\n",
            "2021-02-01 12:25:04,306 - INFO - joeynmt.training - EPOCH 1276\n",
            "2021-02-01 12:25:16,686 - INFO - joeynmt.training - Epoch 1276: total training loss 9.45\n",
            "2021-02-01 12:25:16,687 - INFO - joeynmt.training - EPOCH 1277\n",
            "2021-02-01 12:25:29,192 - INFO - joeynmt.training - Epoch 1277: total training loss 9.50\n",
            "2021-02-01 12:25:29,192 - INFO - joeynmt.training - EPOCH 1278\n",
            "2021-02-01 12:25:38,897 - INFO - joeynmt.training - Epoch 1278, Step:    19100, Batch Loss:     0.638288, Tokens per Sec:    18473, Lr: 0.000072\n",
            "2021-02-01 12:25:41,495 - INFO - joeynmt.training - Epoch 1278: total training loss 9.45\n",
            "2021-02-01 12:25:41,496 - INFO - joeynmt.training - EPOCH 1279\n",
            "2021-02-01 12:25:53,709 - INFO - joeynmt.training - Epoch 1279: total training loss 9.44\n",
            "2021-02-01 12:25:53,709 - INFO - joeynmt.training - EPOCH 1280\n",
            "2021-02-01 12:26:06,016 - INFO - joeynmt.training - Epoch 1280: total training loss 9.54\n",
            "2021-02-01 12:26:06,017 - INFO - joeynmt.training - EPOCH 1281\n",
            "2021-02-01 12:26:18,075 - INFO - joeynmt.training - Epoch 1281: total training loss 8.82\n",
            "2021-02-01 12:26:18,075 - INFO - joeynmt.training - EPOCH 1282\n",
            "2021-02-01 12:26:30,475 - INFO - joeynmt.training - Epoch 1282: total training loss 9.51\n",
            "2021-02-01 12:26:30,475 - INFO - joeynmt.training - EPOCH 1283\n",
            "2021-02-01 12:26:42,803 - INFO - joeynmt.training - Epoch 1283: total training loss 9.47\n",
            "2021-02-01 12:26:42,803 - INFO - joeynmt.training - EPOCH 1284\n",
            "2021-02-01 12:26:55,247 - INFO - joeynmt.training - Epoch 1284: total training loss 9.48\n",
            "2021-02-01 12:26:55,247 - INFO - joeynmt.training - EPOCH 1285\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:27:01,825 - INFO - joeynmt.training - Epoch 1285, Step:    19200, Batch Loss:     0.652518, Tokens per Sec:    18805, Lr: 0.000072\n",
            "2021-02-01 12:27:07,559 - INFO - joeynmt.training - Epoch 1285: total training loss 9.45\n",
            "2021-02-01 12:27:07,560 - INFO - joeynmt.training - EPOCH 1286\n",
            "2021-02-01 12:27:19,838 - INFO - joeynmt.training - Epoch 1286: total training loss 9.42\n",
            "2021-02-01 12:27:19,839 - INFO - joeynmt.training - EPOCH 1287\n",
            "2021-02-01 12:27:32,248 - INFO - joeynmt.training - Epoch 1287: total training loss 9.46\n",
            "2021-02-01 12:27:32,249 - INFO - joeynmt.training - EPOCH 1288\n",
            "2021-02-01 12:27:44,715 - INFO - joeynmt.training - Epoch 1288: total training loss 9.47\n",
            "2021-02-01 12:27:44,716 - INFO - joeynmt.training - EPOCH 1289\n",
            "2021-02-01 12:27:57,052 - INFO - joeynmt.training - Epoch 1289: total training loss 9.44\n",
            "2021-02-01 12:27:57,053 - INFO - joeynmt.training - EPOCH 1290\n",
            "2021-02-01 12:28:09,337 - INFO - joeynmt.training - Epoch 1290: total training loss 9.46\n",
            "2021-02-01 12:28:09,337 - INFO - joeynmt.training - EPOCH 1291\n",
            "2021-02-01 12:28:21,475 - INFO - joeynmt.training - Epoch 1291: total training loss 8.82\n",
            "2021-02-01 12:28:21,475 - INFO - joeynmt.training - EPOCH 1292\n",
            "2021-02-01 12:28:24,735 - INFO - joeynmt.training - Epoch 1292, Step:    19300, Batch Loss:     0.590873, Tokens per Sec:    18918, Lr: 0.000072\n",
            "2021-02-01 12:28:33,982 - INFO - joeynmt.training - Epoch 1292: total training loss 9.42\n",
            "2021-02-01 12:28:33,982 - INFO - joeynmt.training - EPOCH 1293\n",
            "2021-02-01 12:28:46,316 - INFO - joeynmt.training - Epoch 1293: total training loss 9.46\n",
            "2021-02-01 12:28:46,316 - INFO - joeynmt.training - EPOCH 1294\n",
            "2021-02-01 12:28:58,767 - INFO - joeynmt.training - Epoch 1294: total training loss 9.44\n",
            "2021-02-01 12:28:58,767 - INFO - joeynmt.training - EPOCH 1295\n",
            "2021-02-01 12:29:11,088 - INFO - joeynmt.training - Epoch 1295: total training loss 9.45\n",
            "2021-02-01 12:29:11,089 - INFO - joeynmt.training - EPOCH 1296\n",
            "2021-02-01 12:29:23,535 - INFO - joeynmt.training - Epoch 1296: total training loss 9.42\n",
            "2021-02-01 12:29:23,536 - INFO - joeynmt.training - EPOCH 1297\n",
            "2021-02-01 12:29:35,927 - INFO - joeynmt.training - Epoch 1297: total training loss 9.41\n",
            "2021-02-01 12:29:35,927 - INFO - joeynmt.training - EPOCH 1298\n",
            "2021-02-01 12:29:47,284 - INFO - joeynmt.training - Epoch 1298, Step:    19400, Batch Loss:     0.628707, Tokens per Sec:    18332, Lr: 0.000072\n",
            "2021-02-01 12:29:48,496 - INFO - joeynmt.training - Epoch 1298: total training loss 9.43\n",
            "2021-02-01 12:29:48,497 - INFO - joeynmt.training - EPOCH 1299\n",
            "2021-02-01 12:30:00,731 - INFO - joeynmt.training - Epoch 1299: total training loss 9.46\n",
            "2021-02-01 12:30:00,732 - INFO - joeynmt.training - EPOCH 1300\n",
            "2021-02-01 12:30:13,043 - INFO - joeynmt.training - Epoch 1300: total training loss 9.43\n",
            "2021-02-01 12:30:13,043 - INFO - joeynmt.training - EPOCH 1301\n",
            "2021-02-01 12:30:25,409 - INFO - joeynmt.training - Epoch 1301: total training loss 9.44\n",
            "2021-02-01 12:30:25,409 - INFO - joeynmt.training - EPOCH 1302\n",
            "2021-02-01 12:30:37,716 - INFO - joeynmt.training - Epoch 1302: total training loss 9.41\n",
            "2021-02-01 12:30:37,716 - INFO - joeynmt.training - EPOCH 1303\n",
            "2021-02-01 12:30:50,095 - INFO - joeynmt.training - Epoch 1303: total training loss 9.42\n",
            "2021-02-01 12:30:50,096 - INFO - joeynmt.training - EPOCH 1304\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:31:02,495 - INFO - joeynmt.training - Epoch 1304: total training loss 9.40\n",
            "2021-02-01 12:31:02,495 - INFO - joeynmt.training - EPOCH 1305\n",
            "2021-02-01 12:31:09,749 - INFO - joeynmt.training - Epoch 1305, Step:    19500, Batch Loss:     0.591760, Tokens per Sec:    18137, Lr: 0.000072\n",
            "2021-02-01 12:31:36,356 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:31:36,357 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:31:36,357 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:31:36,357 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 12:31:36,357 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құйып өлтіртті.\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:31:36,358 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, оған ілескен кен ештеңе көрдім. Ол қатты ашты※ күнәкар дүниеде Құдайды мадақтап жүрдім. Оның мәнінде әйелдің үстінде мағынасы ұсталдығына келді!\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене де төсек теп, мекендердің оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 12:31:36,359 - INFO - joeynmt.training - Validation result (greedy) at epoch 1305, step    19500: bleu:   4.93, loss: 87488.0000, ppl:  77.7672, duration: 26.6094s\n",
            "2021-02-01 12:31:41,432 - INFO - joeynmt.training - Epoch 1305: total training loss 9.39\n",
            "2021-02-01 12:31:41,432 - INFO - joeynmt.training - EPOCH 1306\n",
            "2021-02-01 12:31:53,880 - INFO - joeynmt.training - Epoch 1306: total training loss 9.42\n",
            "2021-02-01 12:31:53,880 - INFO - joeynmt.training - EPOCH 1307\n",
            "2021-02-01 12:32:06,223 - INFO - joeynmt.training - Epoch 1307: total training loss 9.41\n",
            "2021-02-01 12:32:06,224 - INFO - joeynmt.training - EPOCH 1308\n",
            "2021-02-01 12:32:18,530 - INFO - joeynmt.training - Epoch 1308: total training loss 9.43\n",
            "2021-02-01 12:32:18,531 - INFO - joeynmt.training - EPOCH 1309\n",
            "2021-02-01 12:32:30,925 - INFO - joeynmt.training - Epoch 1309: total training loss 9.44\n",
            "2021-02-01 12:32:30,926 - INFO - joeynmt.training - EPOCH 1310\n",
            "2021-02-01 12:32:43,273 - INFO - joeynmt.training - Epoch 1310: total training loss 9.40\n",
            "2021-02-01 12:32:43,274 - INFO - joeynmt.training - EPOCH 1311\n",
            "2021-02-01 12:32:55,525 - INFO - joeynmt.training - Epoch 1311: total training loss 9.44\n",
            "2021-02-01 12:32:55,526 - INFO - joeynmt.training - EPOCH 1312\n",
            "2021-02-01 12:32:58,842 - INFO - joeynmt.training - Epoch 1312, Step:    19600, Batch Loss:     0.641514, Tokens per Sec:    19585, Lr: 0.000072\n",
            "2021-02-01 12:33:08,031 - INFO - joeynmt.training - Epoch 1312: total training loss 9.44\n",
            "2021-02-01 12:33:08,031 - INFO - joeynmt.training - EPOCH 1313\n",
            "2021-02-01 12:33:20,425 - INFO - joeynmt.training - Epoch 1313: total training loss 9.43\n",
            "2021-02-01 12:33:20,425 - INFO - joeynmt.training - EPOCH 1314\n",
            "2021-02-01 12:33:32,754 - INFO - joeynmt.training - Epoch 1314: total training loss 9.41\n",
            "2021-02-01 12:33:32,755 - INFO - joeynmt.training - EPOCH 1315\n",
            "2021-02-01 12:33:45,254 - INFO - joeynmt.training - Epoch 1315: total training loss 9.39\n",
            "2021-02-01 12:33:45,254 - INFO - joeynmt.training - EPOCH 1316\n",
            "2021-02-01 12:33:57,644 - INFO - joeynmt.training - Epoch 1316: total training loss 9.42\n",
            "2021-02-01 12:33:57,644 - INFO - joeynmt.training - EPOCH 1317\n",
            "2021-02-01 12:34:10,019 - INFO - joeynmt.training - Epoch 1317: total training loss 9.38\n",
            "2021-02-01 12:34:10,020 - INFO - joeynmt.training - EPOCH 1318\n",
            "2021-02-01 12:34:21,491 - INFO - joeynmt.training - Epoch 1318, Step:    19700, Batch Loss:     0.606963, Tokens per Sec:    18400, Lr: 0.000072\n",
            "2021-02-01 12:34:22,406 - INFO - joeynmt.training - Epoch 1318: total training loss 9.40\n",
            "2021-02-01 12:34:22,406 - INFO - joeynmt.training - EPOCH 1319\n",
            "2021-02-01 12:34:34,610 - INFO - joeynmt.training - Epoch 1319: total training loss 9.42\n",
            "2021-02-01 12:34:34,611 - INFO - joeynmt.training - EPOCH 1320\n",
            "2021-02-01 12:34:47,100 - INFO - joeynmt.training - Epoch 1320: total training loss 9.40\n",
            "2021-02-01 12:34:47,100 - INFO - joeynmt.training - EPOCH 1321\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:34:59,316 - INFO - joeynmt.training - Epoch 1321: total training loss 9.43\n",
            "2021-02-01 12:34:59,316 - INFO - joeynmt.training - EPOCH 1322\n",
            "2021-02-01 12:35:11,691 - INFO - joeynmt.training - Epoch 1322: total training loss 9.38\n",
            "2021-02-01 12:35:11,692 - INFO - joeynmt.training - EPOCH 1323\n",
            "2021-02-01 12:35:24,093 - INFO - joeynmt.training - Epoch 1323: total training loss 9.41\n",
            "2021-02-01 12:35:24,094 - INFO - joeynmt.training - EPOCH 1324\n",
            "2021-02-01 12:35:36,460 - INFO - joeynmt.training - Epoch 1324: total training loss 9.39\n",
            "2021-02-01 12:35:36,461 - INFO - joeynmt.training - EPOCH 1325\n",
            "2021-02-01 12:35:43,754 - INFO - joeynmt.training - Epoch 1325, Step:    19800, Batch Loss:     0.611309, Tokens per Sec:    18344, Lr: 0.000072\n",
            "2021-02-01 12:35:48,906 - INFO - joeynmt.training - Epoch 1325: total training loss 9.36\n",
            "2021-02-01 12:35:48,906 - INFO - joeynmt.training - EPOCH 1326\n",
            "2021-02-01 12:36:01,366 - INFO - joeynmt.training - Epoch 1326: total training loss 9.40\n",
            "2021-02-01 12:36:01,366 - INFO - joeynmt.training - EPOCH 1327\n",
            "2021-02-01 12:36:13,584 - INFO - joeynmt.training - Epoch 1327: total training loss 9.39\n",
            "2021-02-01 12:36:13,584 - INFO - joeynmt.training - EPOCH 1328\n",
            "2021-02-01 12:36:25,879 - INFO - joeynmt.training - Epoch 1328: total training loss 9.42\n",
            "2021-02-01 12:36:25,879 - INFO - joeynmt.training - EPOCH 1329\n",
            "2021-02-01 12:36:38,176 - INFO - joeynmt.training - Epoch 1329: total training loss 9.40\n",
            "2021-02-01 12:36:38,176 - INFO - joeynmt.training - EPOCH 1330\n",
            "2021-02-01 12:36:50,371 - INFO - joeynmt.training - Epoch 1330: total training loss 9.40\n",
            "2021-02-01 12:36:50,371 - INFO - joeynmt.training - EPOCH 1331\n",
            "2021-02-01 12:37:02,725 - INFO - joeynmt.training - Epoch 1331: total training loss 9.40\n",
            "2021-02-01 12:37:02,726 - INFO - joeynmt.training - EPOCH 1332\n",
            "2021-02-01 12:37:05,940 - INFO - joeynmt.training - Epoch 1332, Step:    19900, Batch Loss:     0.639306, Tokens per Sec:    18257, Lr: 0.000072\n",
            "2021-02-01 12:37:15,145 - INFO - joeynmt.training - Epoch 1332: total training loss 9.33\n",
            "2021-02-01 12:37:15,145 - INFO - joeynmt.training - EPOCH 1333\n",
            "2021-02-01 12:37:27,464 - INFO - joeynmt.training - Epoch 1333: total training loss 9.44\n",
            "2021-02-01 12:37:27,465 - INFO - joeynmt.training - EPOCH 1334\n",
            "2021-02-01 12:37:39,721 - INFO - joeynmt.training - Epoch 1334: total training loss 9.39\n",
            "2021-02-01 12:37:39,721 - INFO - joeynmt.training - EPOCH 1335\n",
            "2021-02-01 12:37:52,061 - INFO - joeynmt.training - Epoch 1335: total training loss 9.34\n",
            "2021-02-01 12:37:52,062 - INFO - joeynmt.training - EPOCH 1336\n",
            "2021-02-01 12:38:04,369 - INFO - joeynmt.training - Epoch 1336: total training loss 9.42\n",
            "2021-02-01 12:38:04,370 - INFO - joeynmt.training - EPOCH 1337\n",
            "2021-02-01 12:38:16,632 - INFO - joeynmt.training - Epoch 1337: total training loss 9.38\n",
            "2021-02-01 12:38:16,633 - INFO - joeynmt.training - EPOCH 1338\n",
            "2021-02-01 12:38:28,029 - INFO - joeynmt.training - Epoch 1338, Step:    20000, Batch Loss:     0.665441, Tokens per Sec:    18496, Lr: 0.000072\n",
            "2021-02-01 12:38:55,324 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:38:55,325 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:38:55,325 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:38:55,325 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы бір демалыс күні※ мәжілісханаларға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 12:38:55,325 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:38:55,326 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:38:55,326 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:38:55,326 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құятқа деп ойлап келген еді.\n",
            "2021-02-01 12:38:55,326 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:38:55,327 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:38:55,327 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:38:55,327 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, адам сылта көрдім. Ол Тоқтының үстінде отырған Құдайды көрмей қатты ашты! Жер бетінде әйелдің шын жүректен тілі мен Құдайды балағаттандырдым.\n",
            "2021-02-01 12:38:55,327 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:38:55,328 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:38:55,328 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:38:55,328 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алдыңдар.※ Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 12:38:55,328 - INFO - joeynmt.training - Validation result (greedy) at epoch 1338, step    20000: bleu:   5.18, loss: 87523.1250, ppl:  77.9033, duration: 27.2990s\n",
            "2021-02-01 12:38:56,409 - INFO - joeynmt.training - Epoch 1338: total training loss 9.37\n",
            "2021-02-01 12:38:56,409 - INFO - joeynmt.training - EPOCH 1339\n",
            "2021-02-01 12:39:08,858 - INFO - joeynmt.training - Epoch 1339: total training loss 9.37\n",
            "2021-02-01 12:39:08,859 - INFO - joeynmt.training - EPOCH 1340\n",
            "2021-02-01 12:39:21,413 - INFO - joeynmt.training - Epoch 1340: total training loss 9.38\n",
            "2021-02-01 12:39:21,414 - INFO - joeynmt.training - EPOCH 1341\n",
            "2021-02-01 12:39:33,853 - INFO - joeynmt.training - Epoch 1341: total training loss 9.30\n",
            "2021-02-01 12:39:33,853 - INFO - joeynmt.training - EPOCH 1342\n",
            "2021-02-01 12:39:46,338 - INFO - joeynmt.training - Epoch 1342: total training loss 9.33\n",
            "2021-02-01 12:39:46,339 - INFO - joeynmt.training - EPOCH 1343\n",
            "2021-02-01 12:39:58,661 - INFO - joeynmt.training - Epoch 1343: total training loss 9.35\n",
            "2021-02-01 12:39:58,662 - INFO - joeynmt.training - EPOCH 1344\n",
            "2021-02-01 12:40:11,096 - INFO - joeynmt.training - Epoch 1344: total training loss 9.32\n",
            "2021-02-01 12:40:11,097 - INFO - joeynmt.training - EPOCH 1345\n",
            "2021-02-01 12:40:18,477 - INFO - joeynmt.training - Epoch 1345, Step:    20100, Batch Loss:     0.633737, Tokens per Sec:    18352, Lr: 0.000072\n",
            "2021-02-01 12:40:23,462 - INFO - joeynmt.training - Epoch 1345: total training loss 9.36\n",
            "2021-02-01 12:40:23,462 - INFO - joeynmt.training - EPOCH 1346\n",
            "2021-02-01 12:40:35,694 - INFO - joeynmt.training - Epoch 1346: total training loss 9.36\n",
            "2021-02-01 12:40:35,694 - INFO - joeynmt.training - EPOCH 1347\n",
            "2021-02-01 12:40:48,167 - INFO - joeynmt.training - Epoch 1347: total training loss 9.38\n",
            "2021-02-01 12:40:48,167 - INFO - joeynmt.training - EPOCH 1348\n",
            "2021-02-01 12:41:00,476 - INFO - joeynmt.training - Epoch 1348: total training loss 9.35\n",
            "2021-02-01 12:41:00,476 - INFO - joeynmt.training - EPOCH 1349\n",
            "2021-02-01 12:41:12,810 - INFO - joeynmt.training - Epoch 1349: total training loss 9.32\n",
            "2021-02-01 12:41:12,811 - INFO - joeynmt.training - EPOCH 1350\n",
            "2021-02-01 12:41:25,084 - INFO - joeynmt.training - Epoch 1350: total training loss 9.34\n",
            "2021-02-01 12:41:25,084 - INFO - joeynmt.training - EPOCH 1351\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:41:37,193 - INFO - joeynmt.training - Epoch 1351: total training loss 8.64\n",
            "2021-02-01 12:41:37,193 - INFO - joeynmt.training - EPOCH 1352\n",
            "2021-02-01 12:41:41,268 - INFO - joeynmt.training - Epoch 1352, Step:    20200, Batch Loss:     0.603742, Tokens per Sec:    19227, Lr: 0.000072\n",
            "2021-02-01 12:41:49,423 - INFO - joeynmt.training - Epoch 1352: total training loss 9.32\n",
            "2021-02-01 12:41:49,423 - INFO - joeynmt.training - EPOCH 1353\n",
            "2021-02-01 12:42:01,791 - INFO - joeynmt.training - Epoch 1353: total training loss 9.33\n",
            "2021-02-01 12:42:01,792 - INFO - joeynmt.training - EPOCH 1354\n",
            "2021-02-01 12:42:14,244 - INFO - joeynmt.training - Epoch 1354: total training loss 9.33\n",
            "2021-02-01 12:42:14,244 - INFO - joeynmt.training - EPOCH 1355\n",
            "2021-02-01 12:42:26,628 - INFO - joeynmt.training - Epoch 1355: total training loss 9.36\n",
            "2021-02-01 12:42:26,628 - INFO - joeynmt.training - EPOCH 1356\n",
            "2021-02-01 12:42:38,905 - INFO - joeynmt.training - Epoch 1356: total training loss 9.35\n",
            "2021-02-01 12:42:38,906 - INFO - joeynmt.training - EPOCH 1357\n",
            "2021-02-01 12:42:51,214 - INFO - joeynmt.training - Epoch 1357: total training loss 9.35\n",
            "2021-02-01 12:42:51,214 - INFO - joeynmt.training - EPOCH 1358\n",
            "2021-02-01 12:43:03,499 - INFO - joeynmt.training - Epoch 1358, Step:    20300, Batch Loss:     0.618908, Tokens per Sec:    18474, Lr: 0.000072\n",
            "2021-02-01 12:43:03,597 - INFO - joeynmt.training - Epoch 1358: total training loss 9.32\n",
            "2021-02-01 12:43:03,597 - INFO - joeynmt.training - EPOCH 1359\n",
            "2021-02-01 12:43:16,078 - INFO - joeynmt.training - Epoch 1359: total training loss 9.32\n",
            "2021-02-01 12:43:16,078 - INFO - joeynmt.training - EPOCH 1360\n",
            "2021-02-01 12:43:28,413 - INFO - joeynmt.training - Epoch 1360: total training loss 9.30\n",
            "2021-02-01 12:43:28,413 - INFO - joeynmt.training - EPOCH 1361\n",
            "2021-02-01 12:43:40,757 - INFO - joeynmt.training - Epoch 1361: total training loss 9.36\n",
            "2021-02-01 12:43:40,757 - INFO - joeynmt.training - EPOCH 1362\n",
            "2021-02-01 12:43:53,375 - INFO - joeynmt.training - Epoch 1362: total training loss 9.29\n",
            "2021-02-01 12:43:53,375 - INFO - joeynmt.training - EPOCH 1363\n",
            "2021-02-01 12:44:05,890 - INFO - joeynmt.training - Epoch 1363: total training loss 9.30\n",
            "2021-02-01 12:44:05,890 - INFO - joeynmt.training - EPOCH 1364\n",
            "2021-02-01 12:44:18,285 - INFO - joeynmt.training - Epoch 1364: total training loss 9.32\n",
            "2021-02-01 12:44:18,286 - INFO - joeynmt.training - EPOCH 1365\n",
            "2021-02-01 12:44:26,526 - INFO - joeynmt.training - Epoch 1365, Step:    20400, Batch Loss:     0.616705, Tokens per Sec:    18842, Lr: 0.000072\n",
            "2021-02-01 12:44:30,612 - INFO - joeynmt.training - Epoch 1365: total training loss 9.32\n",
            "2021-02-01 12:44:30,612 - INFO - joeynmt.training - EPOCH 1366\n",
            "2021-02-01 12:44:43,046 - INFO - joeynmt.training - Epoch 1366: total training loss 9.32\n",
            "2021-02-01 12:44:43,047 - INFO - joeynmt.training - EPOCH 1367\n",
            "2021-02-01 12:44:55,418 - INFO - joeynmt.training - Epoch 1367: total training loss 9.28\n",
            "2021-02-01 12:44:55,419 - INFO - joeynmt.training - EPOCH 1368\n",
            "2021-02-01 12:45:07,627 - INFO - joeynmt.training - Epoch 1368: total training loss 9.34\n",
            "2021-02-01 12:45:07,628 - INFO - joeynmt.training - EPOCH 1369\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:45:19,955 - INFO - joeynmt.training - Epoch 1369: total training loss 9.33\n",
            "2021-02-01 12:45:19,956 - INFO - joeynmt.training - EPOCH 1370\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 12:45:32,213 - INFO - joeynmt.training - Epoch 1370: total training loss 9.25\n",
            "2021-02-01 12:45:32,214 - INFO - joeynmt.training - EPOCH 1371\n",
            "2021-02-01 12:45:44,557 - INFO - joeynmt.training - Epoch 1371: total training loss 9.32\n",
            "2021-02-01 12:45:44,557 - INFO - joeynmt.training - EPOCH 1372\n",
            "2021-02-01 12:45:48,645 - INFO - joeynmt.training - Epoch 1372, Step:    20500, Batch Loss:     0.627028, Tokens per Sec:    18357, Lr: 0.000072\n",
            "2021-02-01 12:46:16,154 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:46:16,155 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:46:16,155 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:46:16,155 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 12:46:16,155 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:46:16,155 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:46:16,156 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:46:16,156 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құйып өлтіртті.\n",
            "2021-02-01 12:46:16,156 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:46:16,156 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:46:16,157 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:46:16,157 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқан Ізгі хабарды бұрмалайға жіберіп, оған ілесті. Тоқтының үстінде отырған жеті патшалық көрмей қатты ашты,※ Құдайды мадақтап күткен, айуанның есімі мен көрдім.\n",
            "2021-02-01 12:46:16,157 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:46:16,157 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:46:16,157 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:46:16,158 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермекендерің жайылмай тұра, Құдайдың оң жағындағы құрметті орнына отыр.※\n",
            "2021-02-01 12:46:16,158 - INFO - joeynmt.training - Validation result (greedy) at epoch 1372, step    20500: bleu:   5.01, loss: 87560.2188, ppl:  78.0472, duration: 27.5124s\n",
            "2021-02-01 12:46:24,587 - INFO - joeynmt.training - Epoch 1372: total training loss 9.32\n",
            "2021-02-01 12:46:24,588 - INFO - joeynmt.training - EPOCH 1373\n",
            "2021-02-01 12:46:36,867 - INFO - joeynmt.training - Epoch 1373: total training loss 9.30\n",
            "2021-02-01 12:46:36,867 - INFO - joeynmt.training - EPOCH 1374\n",
            "2021-02-01 12:46:49,331 - INFO - joeynmt.training - Epoch 1374: total training loss 9.28\n",
            "2021-02-01 12:46:49,332 - INFO - joeynmt.training - EPOCH 1375\n",
            "2021-02-01 12:47:01,752 - INFO - joeynmt.training - Epoch 1375: total training loss 9.29\n",
            "2021-02-01 12:47:01,752 - INFO - joeynmt.training - EPOCH 1376\n",
            "2021-02-01 12:47:13,993 - INFO - joeynmt.training - Epoch 1376: total training loss 9.29\n",
            "2021-02-01 12:47:13,993 - INFO - joeynmt.training - EPOCH 1377\n",
            "2021-02-01 12:47:26,475 - INFO - joeynmt.training - Epoch 1377: total training loss 9.29\n",
            "2021-02-01 12:47:26,476 - INFO - joeynmt.training - EPOCH 1378\n",
            "2021-02-01 12:47:38,694 - INFO - joeynmt.training - Epoch 1378, Step:    20600, Batch Loss:     0.652683, Tokens per Sec:    18430, Lr: 0.000072\n",
            "2021-02-01 12:47:38,887 - INFO - joeynmt.training - Epoch 1378: total training loss 9.31\n",
            "2021-02-01 12:47:38,887 - INFO - joeynmt.training - EPOCH 1379\n",
            "2021-02-01 12:47:51,288 - INFO - joeynmt.training - Epoch 1379: total training loss 9.21\n",
            "2021-02-01 12:47:51,288 - INFO - joeynmt.training - EPOCH 1380\n",
            "2021-02-01 12:48:03,865 - INFO - joeynmt.training - Epoch 1380: total training loss 9.26\n",
            "2021-02-01 12:48:03,865 - INFO - joeynmt.training - EPOCH 1381\n",
            "2021-02-01 12:48:16,143 - INFO - joeynmt.training - Epoch 1381: total training loss 9.31\n",
            "2021-02-01 12:48:16,144 - INFO - joeynmt.training - EPOCH 1382\n",
            "2021-02-01 12:48:28,475 - INFO - joeynmt.training - Epoch 1382: total training loss 9.30\n",
            "2021-02-01 12:48:28,475 - INFO - joeynmt.training - EPOCH 1383\n",
            "2021-02-01 12:48:40,769 - INFO - joeynmt.training - Epoch 1383: total training loss 9.27\n",
            "2021-02-01 12:48:40,769 - INFO - joeynmt.training - EPOCH 1384\n",
            "2021-02-01 12:48:53,098 - INFO - joeynmt.training - Epoch 1384: total training loss 9.26\n",
            "2021-02-01 12:48:53,098 - INFO - joeynmt.training - EPOCH 1385\n",
            "2021-02-01 12:49:01,356 - INFO - joeynmt.training - Epoch 1385, Step:    20700, Batch Loss:     0.640068, Tokens per Sec:    18396, Lr: 0.000072\n",
            "2021-02-01 12:49:05,418 - INFO - joeynmt.training - Epoch 1385: total training loss 9.29\n",
            "2021-02-01 12:49:05,418 - INFO - joeynmt.training - EPOCH 1386\n",
            "2021-02-01 12:49:17,735 - INFO - joeynmt.training - Epoch 1386: total training loss 9.31\n",
            "2021-02-01 12:49:17,735 - INFO - joeynmt.training - EPOCH 1387\n",
            "2021-02-01 12:49:30,137 - INFO - joeynmt.training - Epoch 1387: total training loss 9.27\n",
            "2021-02-01 12:49:30,137 - INFO - joeynmt.training - EPOCH 1388\n",
            "2021-02-01 12:49:42,532 - INFO - joeynmt.training - Epoch 1388: total training loss 9.28\n",
            "2021-02-01 12:49:42,532 - INFO - joeynmt.training - EPOCH 1389\n",
            "2021-02-01 12:49:54,876 - INFO - joeynmt.training - Epoch 1389: total training loss 9.24\n",
            "2021-02-01 12:49:54,876 - INFO - joeynmt.training - EPOCH 1390\n",
            "2021-02-01 12:50:07,348 - INFO - joeynmt.training - Epoch 1390: total training loss 9.27\n",
            "2021-02-01 12:50:07,348 - INFO - joeynmt.training - EPOCH 1391\n",
            "2021-02-01 12:50:19,704 - INFO - joeynmt.training - Epoch 1391: total training loss 9.26\n",
            "2021-02-01 12:50:19,704 - INFO - joeynmt.training - EPOCH 1392\n",
            "2021-02-01 12:50:23,770 - INFO - joeynmt.training - Epoch 1392, Step:    20800, Batch Loss:     0.602101, Tokens per Sec:    17951, Lr: 0.000072\n",
            "2021-02-01 12:50:32,008 - INFO - joeynmt.training - Epoch 1392: total training loss 9.25\n",
            "2021-02-01 12:50:32,009 - INFO - joeynmt.training - EPOCH 1393\n",
            "2021-02-01 12:50:44,293 - INFO - joeynmt.training - Epoch 1393: total training loss 9.32\n",
            "2021-02-01 12:50:44,294 - INFO - joeynmt.training - EPOCH 1394\n",
            "2021-02-01 12:50:56,572 - INFO - joeynmt.training - Epoch 1394: total training loss 9.29\n",
            "2021-02-01 12:50:56,573 - INFO - joeynmt.training - EPOCH 1395\n",
            "2021-02-01 12:51:08,983 - INFO - joeynmt.training - Epoch 1395: total training loss 9.26\n",
            "2021-02-01 12:51:08,984 - INFO - joeynmt.training - EPOCH 1396\n",
            "2021-02-01 12:51:21,359 - INFO - joeynmt.training - Epoch 1396: total training loss 9.27\n",
            "2021-02-01 12:51:21,359 - INFO - joeynmt.training - EPOCH 1397\n",
            "2021-02-01 12:51:33,440 - INFO - joeynmt.training - Epoch 1397: total training loss 8.66\n",
            "2021-02-01 12:51:33,441 - INFO - joeynmt.training - EPOCH 1398\n",
            "2021-02-01 12:51:45,990 - INFO - joeynmt.training - Epoch 1398: total training loss 9.23\n",
            "2021-02-01 12:51:45,990 - INFO - joeynmt.training - EPOCH 1399\n",
            "2021-02-01 12:51:46,824 - INFO - joeynmt.training - Epoch 1399, Step:    20900, Batch Loss:     0.633004, Tokens per Sec:    18307, Lr: 0.000072\n",
            "2021-02-01 12:51:58,224 - INFO - joeynmt.training - Epoch 1399: total training loss 9.26\n",
            "2021-02-01 12:51:58,224 - INFO - joeynmt.training - EPOCH 1400\n",
            "2021-02-01 12:52:10,608 - INFO - joeynmt.training - Epoch 1400: total training loss 9.22\n",
            "2021-02-01 12:52:10,609 - INFO - joeynmt.training - EPOCH 1401\n",
            "2021-02-01 12:52:22,695 - INFO - joeynmt.training - Epoch 1401: total training loss 8.62\n",
            "2021-02-01 12:52:22,695 - INFO - joeynmt.training - EPOCH 1402\n",
            "2021-02-01 12:52:35,080 - INFO - joeynmt.training - Epoch 1402: total training loss 9.29\n",
            "2021-02-01 12:52:35,081 - INFO - joeynmt.training - EPOCH 1403\n",
            "2021-02-01 12:52:47,575 - INFO - joeynmt.training - Epoch 1403: total training loss 9.24\n",
            "2021-02-01 12:52:47,575 - INFO - joeynmt.training - EPOCH 1404\n",
            "2021-02-01 12:52:59,924 - INFO - joeynmt.training - Epoch 1404: total training loss 9.23\n",
            "2021-02-01 12:52:59,925 - INFO - joeynmt.training - EPOCH 1405\n",
            "2021-02-01 12:53:09,669 - INFO - joeynmt.training - Epoch 1405, Step:    21000, Batch Loss:     0.593474, Tokens per Sec:    18693, Lr: 0.000072\n",
            "2021-02-01 12:53:38,318 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 12:53:38,319 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 12:53:38,319 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 12:53:38,319 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 12:53:38,319 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 12:53:38,319 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, адам сылта көрдім. Ол Тоқтының үстінде отырған Құдайды көрмеген әйелді қатты ашты,※ Құдайды мадақтап жүрдім. Олар қатты ашкөзі көрген төрт бұрышпен тәжіндей көрдім.\n",
            "2021-02-01 12:53:38,320 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 12:53:38,321 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 12:53:38,321 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 12:53:38,321 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алайық, Құдайдың оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 12:53:38,321 - INFO - joeynmt.training - Validation result (greedy) at epoch 1405, step    21000: bleu:   5.07, loss: 87546.1094, ppl:  77.9924, duration: 28.6511s\n",
            "2021-02-01 12:53:40,871 - INFO - joeynmt.training - Epoch 1405: total training loss 9.28\n",
            "2021-02-01 12:53:40,871 - INFO - joeynmt.training - EPOCH 1406\n",
            "2021-02-01 12:53:53,294 - INFO - joeynmt.training - Epoch 1406: total training loss 9.23\n",
            "2021-02-01 12:53:53,294 - INFO - joeynmt.training - EPOCH 1407\n",
            "2021-02-01 12:54:05,759 - INFO - joeynmt.training - Epoch 1407: total training loss 9.26\n",
            "2021-02-01 12:54:05,759 - INFO - joeynmt.training - EPOCH 1408\n",
            "2021-02-01 12:54:18,066 - INFO - joeynmt.training - Epoch 1408: total training loss 9.23\n",
            "2021-02-01 12:54:18,067 - INFO - joeynmt.training - EPOCH 1409\n",
            "2021-02-01 12:54:30,433 - INFO - joeynmt.training - Epoch 1409: total training loss 9.23\n",
            "2021-02-01 12:54:30,433 - INFO - joeynmt.training - EPOCH 1410\n",
            "2021-02-01 12:54:42,712 - INFO - joeynmt.training - Epoch 1410: total training loss 9.27\n",
            "2021-02-01 12:54:42,712 - INFO - joeynmt.training - EPOCH 1411\n",
            "2021-02-01 12:54:55,070 - INFO - joeynmt.training - Epoch 1411: total training loss 9.27\n",
            "2021-02-01 12:54:55,071 - INFO - joeynmt.training - EPOCH 1412\n",
            "2021-02-01 12:55:00,746 - INFO - joeynmt.training - Epoch 1412, Step:    21100, Batch Loss:     0.585202, Tokens per Sec:    18148, Lr: 0.000072\n",
            "2021-02-01 12:55:07,534 - INFO - joeynmt.training - Epoch 1412: total training loss 9.24\n",
            "2021-02-01 12:55:07,534 - INFO - joeynmt.training - EPOCH 1413\n",
            "2021-02-01 12:55:19,884 - INFO - joeynmt.training - Epoch 1413: total training loss 9.20\n",
            "2021-02-01 12:55:19,884 - INFO - joeynmt.training - EPOCH 1414\n",
            "2021-02-01 12:55:32,097 - INFO - joeynmt.training - Epoch 1414: total training loss 9.22\n",
            "2021-02-01 12:55:32,097 - INFO - joeynmt.training - EPOCH 1415\n",
            "2021-02-01 12:55:44,375 - INFO - joeynmt.training - Epoch 1415: total training loss 9.23\n",
            "2021-02-01 12:55:44,376 - INFO - joeynmt.training - EPOCH 1416\n",
            "2021-02-01 12:55:56,532 - INFO - joeynmt.training - Epoch 1416: total training loss 9.22\n",
            "2021-02-01 12:55:56,532 - INFO - joeynmt.training - EPOCH 1417\n",
            "2021-02-01 12:56:08,755 - INFO - joeynmt.training - Epoch 1417: total training loss 9.25\n",
            "2021-02-01 12:56:08,755 - INFO - joeynmt.training - EPOCH 1418\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 12:56:21,195 - INFO - joeynmt.training - Epoch 1418: total training loss 9.21\n",
            "2021-02-01 12:56:21,196 - INFO - joeynmt.training - EPOCH 1419\n",
            "2021-02-01 12:56:22,889 - INFO - joeynmt.training - Epoch 1419, Step:    21200, Batch Loss:     0.651963, Tokens per Sec:    20011, Lr: 0.000072\n",
            "2021-02-01 12:56:33,582 - INFO - joeynmt.training - Epoch 1419: total training loss 9.23\n",
            "2021-02-01 12:56:33,583 - INFO - joeynmt.training - EPOCH 1420\n",
            "2021-02-01 12:56:45,942 - INFO - joeynmt.training - Epoch 1420: total training loss 9.22\n",
            "2021-02-01 12:56:45,943 - INFO - joeynmt.training - EPOCH 1421\n",
            "2021-02-01 12:56:58,202 - INFO - joeynmt.training - Epoch 1421: total training loss 9.21\n",
            "2021-02-01 12:56:58,203 - INFO - joeynmt.training - EPOCH 1422\n",
            "2021-02-01 12:57:10,488 - INFO - joeynmt.training - Epoch 1422: total training loss 9.22\n",
            "2021-02-01 12:57:10,488 - INFO - joeynmt.training - EPOCH 1423\n",
            "2021-02-01 12:57:22,729 - INFO - joeynmt.training - Epoch 1423: total training loss 9.21\n",
            "2021-02-01 12:57:22,730 - INFO - joeynmt.training - EPOCH 1424\n",
            "2021-02-01 12:57:35,261 - INFO - joeynmt.training - Epoch 1424: total training loss 9.18\n",
            "2021-02-01 12:57:35,261 - INFO - joeynmt.training - EPOCH 1425\n",
            "2021-02-01 12:57:44,972 - INFO - joeynmt.training - Epoch 1425, Step:    21300, Batch Loss:     0.576825, Tokens per Sec:    18253, Lr: 0.000072\n",
            "2021-02-01 12:57:47,652 - INFO - joeynmt.training - Epoch 1425: total training loss 9.23\n",
            "2021-02-01 12:57:47,652 - INFO - joeynmt.training - EPOCH 1426\n",
            "2021-02-01 12:57:59,794 - INFO - joeynmt.training - Epoch 1426: total training loss 8.66\n",
            "2021-02-01 12:57:59,795 - INFO - joeynmt.training - EPOCH 1427\n",
            "2021-02-01 12:58:12,151 - INFO - joeynmt.training - Epoch 1427: total training loss 9.17\n",
            "2021-02-01 12:58:12,152 - INFO - joeynmt.training - EPOCH 1428\n",
            "2021-02-01 12:58:24,526 - INFO - joeynmt.training - Epoch 1428: total training loss 9.17\n",
            "2021-02-01 12:58:24,527 - INFO - joeynmt.training - EPOCH 1429\n",
            "2021-02-01 12:58:36,871 - INFO - joeynmt.training - Epoch 1429: total training loss 9.24\n",
            "2021-02-01 12:58:36,872 - INFO - joeynmt.training - EPOCH 1430\n",
            "2021-02-01 12:58:49,312 - INFO - joeynmt.training - Epoch 1430: total training loss 9.20\n",
            "2021-02-01 12:58:49,312 - INFO - joeynmt.training - EPOCH 1431\n",
            "2021-02-01 12:59:01,822 - INFO - joeynmt.training - Epoch 1431: total training loss 9.20\n",
            "2021-02-01 12:59:01,822 - INFO - joeynmt.training - EPOCH 1432\n",
            "2021-02-01 12:59:08,313 - INFO - joeynmt.training - Epoch 1432, Step:    21400, Batch Loss:     0.610397, Tokens per Sec:    17615, Lr: 0.000072\n",
            "2021-02-01 12:59:14,242 - INFO - joeynmt.training - Epoch 1432: total training loss 9.21\n",
            "2021-02-01 12:59:14,243 - INFO - joeynmt.training - EPOCH 1433\n",
            "2021-02-01 12:59:26,409 - INFO - joeynmt.training - Epoch 1433: total training loss 8.58\n",
            "2021-02-01 12:59:26,409 - INFO - joeynmt.training - EPOCH 1434\n",
            "2021-02-01 12:59:38,709 - INFO - joeynmt.training - Epoch 1434: total training loss 9.22\n",
            "2021-02-01 12:59:38,709 - INFO - joeynmt.training - EPOCH 1435\n",
            "2021-02-01 12:59:51,048 - INFO - joeynmt.training - Epoch 1435: total training loss 9.15\n",
            "2021-02-01 12:59:51,048 - INFO - joeynmt.training - EPOCH 1436\n",
            "2021-02-01 13:00:03,597 - INFO - joeynmt.training - Epoch 1436: total training loss 9.16\n",
            "2021-02-01 13:00:03,598 - INFO - joeynmt.training - EPOCH 1437\n",
            "2021-02-01 13:00:15,967 - INFO - joeynmt.training - Epoch 1437: total training loss 9.19\n",
            "2021-02-01 13:00:15,967 - INFO - joeynmt.training - EPOCH 1438\n",
            "2021-02-01 13:00:28,496 - INFO - joeynmt.training - Epoch 1438: total training loss 9.17\n",
            "2021-02-01 13:00:28,497 - INFO - joeynmt.training - EPOCH 1439\n",
            "2021-02-01 13:00:31,748 - INFO - joeynmt.training - Epoch 1439, Step:    21500, Batch Loss:     0.621100, Tokens per Sec:    18815, Lr: 0.000072\n",
            "2021-02-01 13:01:00,994 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:01:00,994 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға жіберіп отыра бастады.\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:01:00,995 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, адам сылта рет еніп, оған ілесіп тұрған әйелді көрдім. Оның есімі айдаған «Тежелде көрінбейтін әйелдің қатты ашты»※ деп санайды.\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:01:00,996 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:01:00,997 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алайық, Құдайдың оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:01:00,997 - INFO - joeynmt.training - Validation result (greedy) at epoch 1439, step    21500: bleu:   5.50, loss: 87590.8125, ppl:  78.1661, duration: 29.2485s\n",
            "2021-02-01 13:01:10,130 - INFO - joeynmt.training - Epoch 1439: total training loss 9.21\n",
            "2021-02-01 13:01:10,131 - INFO - joeynmt.training - EPOCH 1440\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:01:22,411 - INFO - joeynmt.training - Epoch 1440: total training loss 9.21\n",
            "2021-02-01 13:01:22,412 - INFO - joeynmt.training - EPOCH 1441\n",
            "2021-02-01 13:01:34,868 - INFO - joeynmt.training - Epoch 1441: total training loss 9.17\n",
            "2021-02-01 13:01:34,868 - INFO - joeynmt.training - EPOCH 1442\n",
            "2021-02-01 13:01:47,203 - INFO - joeynmt.training - Epoch 1442: total training loss 9.22\n",
            "2021-02-01 13:01:47,204 - INFO - joeynmt.training - EPOCH 1443\n",
            "2021-02-01 13:01:59,532 - INFO - joeynmt.training - Epoch 1443: total training loss 9.19\n",
            "2021-02-01 13:01:59,532 - INFO - joeynmt.training - EPOCH 1444\n",
            "2021-02-01 13:02:11,816 - INFO - joeynmt.training - Epoch 1444: total training loss 9.21\n",
            "2021-02-01 13:02:11,817 - INFO - joeynmt.training - EPOCH 1445\n",
            "2021-02-01 13:02:23,153 - INFO - joeynmt.training - Epoch 1445, Step:    21600, Batch Loss:     0.617823, Tokens per Sec:    18113, Lr: 0.000072\n",
            "2021-02-01 13:02:24,375 - INFO - joeynmt.training - Epoch 1445: total training loss 9.14\n",
            "2021-02-01 13:02:24,375 - INFO - joeynmt.training - EPOCH 1446\n",
            "2021-02-01 13:02:36,677 - INFO - joeynmt.training - Epoch 1446: total training loss 9.25\n",
            "2021-02-01 13:02:36,677 - INFO - joeynmt.training - EPOCH 1447\n",
            "2021-02-01 13:02:49,149 - INFO - joeynmt.training - Epoch 1447: total training loss 9.13\n",
            "2021-02-01 13:02:49,149 - INFO - joeynmt.training - EPOCH 1448\n",
            "2021-02-01 13:03:01,649 - INFO - joeynmt.training - Epoch 1448: total training loss 9.18\n",
            "2021-02-01 13:03:01,650 - INFO - joeynmt.training - EPOCH 1449\n",
            "2021-02-01 13:03:14,075 - INFO - joeynmt.training - Epoch 1449: total training loss 9.14\n",
            "2021-02-01 13:03:14,075 - INFO - joeynmt.training - EPOCH 1450\n",
            "2021-02-01 13:03:26,521 - INFO - joeynmt.training - Epoch 1450: total training loss 9.21\n",
            "2021-02-01 13:03:26,521 - INFO - joeynmt.training - EPOCH 1451\n",
            "2021-02-01 13:03:38,950 - INFO - joeynmt.training - Epoch 1451: total training loss 9.17\n",
            "2021-02-01 13:03:38,950 - INFO - joeynmt.training - EPOCH 1452\n",
            "2021-02-01 13:03:46,305 - INFO - joeynmt.training - Epoch 1452, Step:    21700, Batch Loss:     0.568942, Tokens per Sec:    17954, Lr: 0.000072\n",
            "2021-02-01 13:03:51,362 - INFO - joeynmt.training - Epoch 1452: total training loss 9.12\n",
            "2021-02-01 13:03:51,363 - INFO - joeynmt.training - EPOCH 1453\n",
            "2021-02-01 13:04:03,664 - INFO - joeynmt.training - Epoch 1453: total training loss 9.12\n",
            "2021-02-01 13:04:03,664 - INFO - joeynmt.training - EPOCH 1454\n",
            "2021-02-01 13:04:15,970 - INFO - joeynmt.training - Epoch 1454: total training loss 9.18\n",
            "2021-02-01 13:04:15,970 - INFO - joeynmt.training - EPOCH 1455\n",
            "2021-02-01 13:04:28,235 - INFO - joeynmt.training - Epoch 1455: total training loss 8.57\n",
            "2021-02-01 13:04:28,235 - INFO - joeynmt.training - EPOCH 1456\n",
            "2021-02-01 13:04:40,543 - INFO - joeynmt.training - Epoch 1456: total training loss 9.17\n",
            "2021-02-01 13:04:40,543 - INFO - joeynmt.training - EPOCH 1457\n",
            "2021-02-01 13:04:52,936 - INFO - joeynmt.training - Epoch 1457: total training loss 9.13\n",
            "2021-02-01 13:04:52,937 - INFO - joeynmt.training - EPOCH 1458\n",
            "2021-02-01 13:05:05,173 - INFO - joeynmt.training - Epoch 1458: total training loss 9.20\n",
            "2021-02-01 13:05:05,174 - INFO - joeynmt.training - EPOCH 1459\n",
            "2021-02-01 13:05:09,244 - INFO - joeynmt.training - Epoch 1459, Step:    21800, Batch Loss:     0.628703, Tokens per Sec:    18490, Lr: 0.000072\n",
            "2021-02-01 13:05:17,573 - INFO - joeynmt.training - Epoch 1459: total training loss 9.12\n",
            "2021-02-01 13:05:17,574 - INFO - joeynmt.training - EPOCH 1460\n",
            "2021-02-01 13:05:29,804 - INFO - joeynmt.training - Epoch 1460: total training loss 8.55\n",
            "2021-02-01 13:05:29,805 - INFO - joeynmt.training - EPOCH 1461\n",
            "2021-02-01 13:05:42,087 - INFO - joeynmt.training - Epoch 1461: total training loss 9.21\n",
            "2021-02-01 13:05:42,087 - INFO - joeynmt.training - EPOCH 1462\n",
            "2021-02-01 13:05:54,425 - INFO - joeynmt.training - Epoch 1462: total training loss 9.16\n",
            "2021-02-01 13:05:54,426 - INFO - joeynmt.training - EPOCH 1463\n",
            "2021-02-01 13:06:06,771 - INFO - joeynmt.training - Epoch 1463: total training loss 9.13\n",
            "2021-02-01 13:06:06,771 - INFO - joeynmt.training - EPOCH 1464\n",
            "2021-02-01 13:06:19,132 - INFO - joeynmt.training - Epoch 1464: total training loss 9.15\n",
            "2021-02-01 13:06:19,133 - INFO - joeynmt.training - EPOCH 1465\n",
            "2021-02-01 13:06:31,195 - INFO - joeynmt.training - Epoch 1465: total training loss 8.51\n",
            "2021-02-01 13:06:31,195 - INFO - joeynmt.training - EPOCH 1466\n",
            "2021-02-01 13:06:32,840 - INFO - joeynmt.training - Epoch 1466, Step:    21900, Batch Loss:     0.619345, Tokens per Sec:    18919, Lr: 0.000072\n",
            "2021-02-01 13:06:43,554 - INFO - joeynmt.training - Epoch 1466: total training loss 9.14\n",
            "2021-02-01 13:06:43,555 - INFO - joeynmt.training - EPOCH 1467\n",
            "2021-02-01 13:06:55,859 - INFO - joeynmt.training - Epoch 1467: total training loss 9.17\n",
            "2021-02-01 13:06:55,860 - INFO - joeynmt.training - EPOCH 1468\n",
            "2021-02-01 13:07:08,087 - INFO - joeynmt.training - Epoch 1468: total training loss 9.15\n",
            "2021-02-01 13:07:08,087 - INFO - joeynmt.training - EPOCH 1469\n",
            "2021-02-01 13:07:20,204 - INFO - joeynmt.training - Epoch 1469: total training loss 9.13\n",
            "2021-02-01 13:07:20,204 - INFO - joeynmt.training - EPOCH 1470\n",
            "2021-02-01 13:07:32,429 - INFO - joeynmt.training - Epoch 1470: total training loss 9.11\n",
            "2021-02-01 13:07:32,429 - INFO - joeynmt.training - EPOCH 1471\n",
            "2021-02-01 13:07:44,715 - INFO - joeynmt.training - Epoch 1471: total training loss 9.15\n",
            "2021-02-01 13:07:44,715 - INFO - joeynmt.training - EPOCH 1472\n",
            "2021-02-01 13:07:54,411 - INFO - joeynmt.training - Epoch 1472, Step:    22000, Batch Loss:     0.599917, Tokens per Sec:    18812, Lr: 0.000072\n",
            "2021-02-01 13:08:22,268 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына ертіп қалаға жіберіп отырды.\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:08:22,269 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:08:22,270 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:08:22,270 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:08:22,270 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, әлемді көрдім. Ол Тиянды тәнінде тұра билігі бар Құдайды көрмеген жеті періште қатты ашты; оның қатты ашылып, Құдайды мадақтап күткен, айуанның есімі мен жыр көрдім.\n",
            "2021-02-01 13:08:22,270 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:08:22,270 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:08:22,271 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:08:22,271 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене де төсек болыңдар: Оның оң жағындағы жеріміздің Иесіне сай орналасқан кісінің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:08:22,271 - INFO - joeynmt.training - Validation result (greedy) at epoch 1472, step    22000: bleu:   4.97, loss: 87567.9766, ppl:  78.0773, duration: 27.8594s\n",
            "2021-02-01 13:08:24,811 - INFO - joeynmt.training - Epoch 1472: total training loss 9.14\n",
            "2021-02-01 13:08:24,812 - INFO - joeynmt.training - EPOCH 1473\n",
            "2021-02-01 13:08:36,836 - INFO - joeynmt.training - Epoch 1473: total training loss 8.54\n",
            "2021-02-01 13:08:36,836 - INFO - joeynmt.training - EPOCH 1474\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 13:08:49,118 - INFO - joeynmt.training - Epoch 1474: total training loss 9.09\n",
            "2021-02-01 13:08:49,119 - INFO - joeynmt.training - EPOCH 1475\n",
            "2021-02-01 13:09:01,392 - INFO - joeynmt.training - Epoch 1475: total training loss 9.13\n",
            "2021-02-01 13:09:01,393 - INFO - joeynmt.training - EPOCH 1476\n",
            "2021-02-01 13:09:13,749 - INFO - joeynmt.training - Epoch 1476: total training loss 9.13\n",
            "2021-02-01 13:09:13,750 - INFO - joeynmt.training - EPOCH 1477\n",
            "2021-02-01 13:09:26,009 - INFO - joeynmt.training - Epoch 1477: total training loss 9.10\n",
            "2021-02-01 13:09:26,009 - INFO - joeynmt.training - EPOCH 1478\n",
            "2021-02-01 13:09:38,267 - INFO - joeynmt.training - Epoch 1478: total training loss 9.13\n",
            "2021-02-01 13:09:38,267 - INFO - joeynmt.training - EPOCH 1479\n",
            "2021-02-01 13:09:44,789 - INFO - joeynmt.training - Epoch 1479, Step:    22100, Batch Loss:     0.621514, Tokens per Sec:    18867, Lr: 0.000050\n",
            "2021-02-01 13:09:50,490 - INFO - joeynmt.training - Epoch 1479: total training loss 9.09\n",
            "2021-02-01 13:09:50,491 - INFO - joeynmt.training - EPOCH 1480\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:10:02,908 - INFO - joeynmt.training - Epoch 1480: total training loss 9.03\n",
            "2021-02-01 13:10:02,909 - INFO - joeynmt.training - EPOCH 1481\n",
            "2021-02-01 13:10:15,210 - INFO - joeynmt.training - Epoch 1481: total training loss 9.06\n",
            "2021-02-01 13:10:15,210 - INFO - joeynmt.training - EPOCH 1482\n",
            "2021-02-01 13:10:27,671 - INFO - joeynmt.training - Epoch 1482: total training loss 9.09\n",
            "2021-02-01 13:10:27,671 - INFO - joeynmt.training - EPOCH 1483\n",
            "2021-02-01 13:10:40,085 - INFO - joeynmt.training - Epoch 1483: total training loss 9.09\n",
            "2021-02-01 13:10:40,085 - INFO - joeynmt.training - EPOCH 1484\n",
            "2021-02-01 13:10:52,508 - INFO - joeynmt.training - Epoch 1484: total training loss 9.06\n",
            "2021-02-01 13:10:52,509 - INFO - joeynmt.training - EPOCH 1485\n",
            "2021-02-01 13:11:04,883 - INFO - joeynmt.training - Epoch 1485: total training loss 9.10\n",
            "2021-02-01 13:11:04,884 - INFO - joeynmt.training - EPOCH 1486\n",
            "2021-02-01 13:11:07,311 - INFO - joeynmt.training - Epoch 1486, Step:    22200, Batch Loss:     0.603913, Tokens per Sec:    18593, Lr: 0.000050\n",
            "2021-02-01 13:11:17,170 - INFO - joeynmt.training - Epoch 1486: total training loss 9.06\n",
            "2021-02-01 13:11:17,171 - INFO - joeynmt.training - EPOCH 1487\n",
            "2021-02-01 13:11:29,394 - INFO - joeynmt.training - Epoch 1487: total training loss 9.11\n",
            "2021-02-01 13:11:29,394 - INFO - joeynmt.training - EPOCH 1488\n",
            "2021-02-01 13:11:41,562 - INFO - joeynmt.training - Epoch 1488: total training loss 9.10\n",
            "2021-02-01 13:11:41,562 - INFO - joeynmt.training - EPOCH 1489\n",
            "2021-02-01 13:11:53,796 - INFO - joeynmt.training - Epoch 1489: total training loss 9.06\n",
            "2021-02-01 13:11:53,797 - INFO - joeynmt.training - EPOCH 1490\n",
            "2021-02-01 13:12:06,076 - INFO - joeynmt.training - Epoch 1490: total training loss 9.09\n",
            "2021-02-01 13:12:06,076 - INFO - joeynmt.training - EPOCH 1491\n",
            "2021-02-01 13:12:18,292 - INFO - joeynmt.training - Epoch 1491: total training loss 9.06\n",
            "2021-02-01 13:12:18,292 - INFO - joeynmt.training - EPOCH 1492\n",
            "2021-02-01 13:12:28,782 - INFO - joeynmt.training - Epoch 1492, Step:    22300, Batch Loss:     0.595912, Tokens per Sec:    18681, Lr: 0.000050\n",
            "2021-02-01 13:12:30,414 - INFO - joeynmt.training - Epoch 1492: total training loss 9.07\n",
            "2021-02-01 13:12:30,415 - INFO - joeynmt.training - EPOCH 1493\n",
            "2021-02-01 13:12:42,776 - INFO - joeynmt.training - Epoch 1493: total training loss 9.09\n",
            "2021-02-01 13:12:42,776 - INFO - joeynmt.training - EPOCH 1494\n",
            "2021-02-01 13:12:54,780 - INFO - joeynmt.training - Epoch 1494: total training loss 8.45\n",
            "2021-02-01 13:12:54,780 - INFO - joeynmt.training - EPOCH 1495\n",
            "2021-02-01 13:13:06,993 - INFO - joeynmt.training - Epoch 1495: total training loss 9.06\n",
            "2021-02-01 13:13:06,994 - INFO - joeynmt.training - EPOCH 1496\n",
            "2021-02-01 13:13:19,337 - INFO - joeynmt.training - Epoch 1496: total training loss 9.07\n",
            "2021-02-01 13:13:19,337 - INFO - joeynmt.training - EPOCH 1497\n",
            "2021-02-01 13:13:31,756 - INFO - joeynmt.training - Epoch 1497: total training loss 9.12\n",
            "2021-02-01 13:13:31,757 - INFO - joeynmt.training - EPOCH 1498\n",
            "2021-02-01 13:13:43,937 - INFO - joeynmt.training - Epoch 1498: total training loss 9.09\n",
            "2021-02-01 13:13:43,937 - INFO - joeynmt.training - EPOCH 1499\n",
            "2021-02-01 13:13:51,239 - INFO - joeynmt.training - Epoch 1499, Step:    22400, Batch Loss:     0.613686, Tokens per Sec:    18249, Lr: 0.000050\n",
            "2021-02-01 13:13:56,362 - INFO - joeynmt.training - Epoch 1499: total training loss 9.03\n",
            "2021-02-01 13:13:56,363 - INFO - joeynmt.training - EPOCH 1500\n",
            "2021-02-01 13:14:08,598 - INFO - joeynmt.training - Epoch 1500: total training loss 9.05\n",
            "2021-02-01 13:14:08,599 - INFO - joeynmt.training - EPOCH 1501\n",
            "2021-02-01 13:14:21,109 - INFO - joeynmt.training - Epoch 1501: total training loss 9.07\n",
            "2021-02-01 13:14:21,109 - INFO - joeynmt.training - EPOCH 1502\n",
            "2021-02-01 13:14:33,512 - INFO - joeynmt.training - Epoch 1502: total training loss 9.00\n",
            "2021-02-01 13:14:33,513 - INFO - joeynmt.training - EPOCH 1503\n",
            "2021-02-01 13:14:45,604 - INFO - joeynmt.training - Epoch 1503: total training loss 8.48\n",
            "2021-02-01 13:14:45,605 - INFO - joeynmt.training - EPOCH 1504\n",
            "2021-02-01 13:14:57,907 - INFO - joeynmt.training - Epoch 1504: total training loss 9.07\n",
            "2021-02-01 13:14:57,907 - INFO - joeynmt.training - EPOCH 1505\n",
            "2021-02-01 13:15:10,260 - INFO - joeynmt.training - Epoch 1505: total training loss 9.03\n",
            "2021-02-01 13:15:10,260 - INFO - joeynmt.training - EPOCH 1506\n",
            "2021-02-01 13:15:14,329 - INFO - joeynmt.training - Epoch 1506, Step:    22500, Batch Loss:     0.592617, Tokens per Sec:    18324, Lr: 0.000050\n",
            "2021-02-01 13:15:42,731 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:15:42,732 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:15:42,732 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы алдын ала айтылған демалыс күні мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:15:42,733 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, әлемді көрдім. Ол Тиянды тәнінде тұрған, мәңгі тірі Құдайдың бүкіл әлемді жаратылыс болып, Құдайды мадақтала алдым.\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермеңдер, көкке көтеріліп, Құдайдың оң жағындағы құрметті орнына жайғасты. Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:15:42,734 - INFO - joeynmt.training - Validation result (greedy) at epoch 1506, step    22500: bleu:   4.98, loss: 87635.6797, ppl:  78.3408, duration: 28.4047s\n",
            "2021-02-01 13:15:51,136 - INFO - joeynmt.training - Epoch 1506: total training loss 9.05\n",
            "2021-02-01 13:15:51,136 - INFO - joeynmt.training - EPOCH 1507\n",
            "2021-02-01 13:16:03,505 - INFO - joeynmt.training - Epoch 1507: total training loss 9.05\n",
            "2021-02-01 13:16:03,506 - INFO - joeynmt.training - EPOCH 1508\n",
            "2021-02-01 13:16:16,031 - INFO - joeynmt.training - Epoch 1508: total training loss 9.03\n",
            "2021-02-01 13:16:16,031 - INFO - joeynmt.training - EPOCH 1509\n",
            "2021-02-01 13:16:28,260 - INFO - joeynmt.training - Epoch 1509: total training loss 9.03\n",
            "2021-02-01 13:16:28,260 - INFO - joeynmt.training - EPOCH 1510\n",
            "2021-02-01 13:16:40,605 - INFO - joeynmt.training - Epoch 1510: total training loss 9.06\n",
            "2021-02-01 13:16:40,605 - INFO - joeynmt.training - EPOCH 1511\n",
            "2021-02-01 13:16:52,944 - INFO - joeynmt.training - Epoch 1511: total training loss 9.07\n",
            "2021-02-01 13:16:52,944 - INFO - joeynmt.training - EPOCH 1512\n",
            "2021-02-01 13:17:05,100 - INFO - joeynmt.training - Epoch 1512, Step:    22600, Batch Loss:     0.640046, Tokens per Sec:    18527, Lr: 0.000050\n",
            "2021-02-01 13:17:05,292 - INFO - joeynmt.training - Epoch 1512: total training loss 9.06\n",
            "2021-02-01 13:17:05,292 - INFO - joeynmt.training - EPOCH 1513\n",
            "2021-02-01 13:17:17,473 - INFO - joeynmt.training - Epoch 1513: total training loss 9.03\n",
            "2021-02-01 13:17:17,473 - INFO - joeynmt.training - EPOCH 1514\n",
            "2021-02-01 13:17:29,678 - INFO - joeynmt.training - Epoch 1514: total training loss 9.05\n",
            "2021-02-01 13:17:29,678 - INFO - joeynmt.training - EPOCH 1515\n",
            "2021-02-01 13:17:42,069 - INFO - joeynmt.training - Epoch 1515: total training loss 9.02\n",
            "2021-02-01 13:17:42,070 - INFO - joeynmt.training - EPOCH 1516\n",
            "2021-02-01 13:17:54,480 - INFO - joeynmt.training - Epoch 1516: total training loss 9.00\n",
            "2021-02-01 13:17:54,480 - INFO - joeynmt.training - EPOCH 1517\n",
            "2021-02-01 13:18:06,799 - INFO - joeynmt.training - Epoch 1517: total training loss 9.02\n",
            "2021-02-01 13:18:06,799 - INFO - joeynmt.training - EPOCH 1518\n",
            "2021-02-01 13:18:19,182 - INFO - joeynmt.training - Epoch 1518: total training loss 9.01\n",
            "2021-02-01 13:18:19,182 - INFO - joeynmt.training - EPOCH 1519\n",
            "2021-02-01 13:18:27,290 - INFO - joeynmt.training - Epoch 1519, Step:    22700, Batch Loss:     0.631915, Tokens per Sec:    18733, Lr: 0.000050\n",
            "2021-02-01 13:18:31,639 - INFO - joeynmt.training - Epoch 1519: total training loss 8.99\n",
            "2021-02-01 13:18:31,640 - INFO - joeynmt.training - EPOCH 1520\n",
            "2021-02-01 13:18:43,791 - INFO - joeynmt.training - Epoch 1520: total training loss 8.42\n",
            "2021-02-01 13:18:43,792 - INFO - joeynmt.training - EPOCH 1521\n",
            "2021-02-01 13:18:56,297 - INFO - joeynmt.training - Epoch 1521: total training loss 9.02\n",
            "2021-02-01 13:18:56,298 - INFO - joeynmt.training - EPOCH 1522\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 13:19:08,495 - INFO - joeynmt.training - Epoch 1522: total training loss 9.02\n",
            "2021-02-01 13:19:08,495 - INFO - joeynmt.training - EPOCH 1523\n",
            "2021-02-01 13:19:20,894 - INFO - joeynmt.training - Epoch 1523: total training loss 9.03\n",
            "2021-02-01 13:19:20,894 - INFO - joeynmt.training - EPOCH 1524\n",
            "2021-02-01 13:19:33,382 - INFO - joeynmt.training - Epoch 1524: total training loss 9.04\n",
            "2021-02-01 13:19:33,382 - INFO - joeynmt.training - EPOCH 1525\n",
            "2021-02-01 13:19:45,610 - INFO - joeynmt.training - Epoch 1525: total training loss 9.04\n",
            "2021-02-01 13:19:45,610 - INFO - joeynmt.training - EPOCH 1526\n",
            "2021-02-01 13:19:50,502 - INFO - joeynmt.training - Epoch 1526, Step:    22800, Batch Loss:     0.611240, Tokens per Sec:    18329, Lr: 0.000050\n",
            "2021-02-01 13:19:57,974 - INFO - joeynmt.training - Epoch 1526: total training loss 9.04\n",
            "2021-02-01 13:19:57,975 - INFO - joeynmt.training - EPOCH 1527\n",
            "2021-02-01 13:20:10,306 - INFO - joeynmt.training - Epoch 1527: total training loss 9.02\n",
            "2021-02-01 13:20:10,306 - INFO - joeynmt.training - EPOCH 1528\n",
            "2021-02-01 13:20:22,570 - INFO - joeynmt.training - Epoch 1528: total training loss 9.03\n",
            "2021-02-01 13:20:22,570 - INFO - joeynmt.training - EPOCH 1529\n",
            "2021-02-01 13:20:35,057 - INFO - joeynmt.training - Epoch 1529: total training loss 8.97\n",
            "2021-02-01 13:20:35,058 - INFO - joeynmt.training - EPOCH 1530\n",
            "2021-02-01 13:20:47,503 - INFO - joeynmt.training - Epoch 1530: total training loss 9.01\n",
            "2021-02-01 13:20:47,503 - INFO - joeynmt.training - EPOCH 1531\n",
            "2021-02-01 13:20:59,945 - INFO - joeynmt.training - Epoch 1531: total training loss 9.07\n",
            "2021-02-01 13:20:59,946 - INFO - joeynmt.training - EPOCH 1532\n",
            "2021-02-01 13:21:12,398 - INFO - joeynmt.training - Epoch 1532: total training loss 9.01\n",
            "2021-02-01 13:21:12,398 - INFO - joeynmt.training - EPOCH 1533\n",
            "2021-02-01 13:21:13,255 - INFO - joeynmt.training - Epoch 1533, Step:    22900, Batch Loss:     0.606582, Tokens per Sec:    18003, Lr: 0.000050\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:21:24,670 - INFO - joeynmt.training - Epoch 1533: total training loss 8.98\n",
            "2021-02-01 13:21:24,671 - INFO - joeynmt.training - EPOCH 1534\n",
            "2021-02-01 13:21:37,101 - INFO - joeynmt.training - Epoch 1534: total training loss 9.02\n",
            "2021-02-01 13:21:37,102 - INFO - joeynmt.training - EPOCH 1535\n",
            "2021-02-01 13:21:49,357 - INFO - joeynmt.training - Epoch 1535: total training loss 9.00\n",
            "2021-02-01 13:21:49,357 - INFO - joeynmt.training - EPOCH 1536\n",
            "2021-02-01 13:22:01,692 - INFO - joeynmt.training - Epoch 1536: total training loss 9.01\n",
            "2021-02-01 13:22:01,692 - INFO - joeynmt.training - EPOCH 1537\n",
            "2021-02-01 13:22:14,091 - INFO - joeynmt.training - Epoch 1537: total training loss 9.07\n",
            "2021-02-01 13:22:14,092 - INFO - joeynmt.training - EPOCH 1538\n",
            "2021-02-01 13:22:26,358 - INFO - joeynmt.training - Epoch 1538: total training loss 9.03\n",
            "2021-02-01 13:22:26,358 - INFO - joeynmt.training - EPOCH 1539\n",
            "2021-02-01 13:22:35,352 - INFO - joeynmt.training - Epoch 1539, Step:    23000, Batch Loss:     0.641018, Tokens per Sec:    18765, Lr: 0.000050\n",
            "2021-02-01 13:23:04,985 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:23:04,986 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:23:04,986 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:23:04,986 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға барып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 13:23:04,986 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еткізіп, оған ілесіп тұрған әйелді көрдім. Қалмыс тәнінде ешине, Құдайды мадақтап жүрдім. Олар қатты ашуландырды.\n",
            "2021-02-01 13:23:04,987 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:23:04,988 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:23:04,988 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:23:04,988 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермекендерің де Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 13:23:04,988 - INFO - joeynmt.training - Validation result (greedy) at epoch 1539, step    23000: bleu:   4.96, loss: 87536.3516, ppl:  77.9545, duration: 29.6356s\n",
            "2021-02-01 13:23:08,112 - INFO - joeynmt.training - Epoch 1539: total training loss 8.39\n",
            "2021-02-01 13:23:08,112 - INFO - joeynmt.training - EPOCH 1540\n",
            "2021-02-01 13:23:20,470 - INFO - joeynmt.training - Epoch 1540: total training loss 9.04\n",
            "2021-02-01 13:23:20,470 - INFO - joeynmt.training - EPOCH 1541\n",
            "2021-02-01 13:23:32,752 - INFO - joeynmt.training - Epoch 1541: total training loss 8.98\n",
            "2021-02-01 13:23:32,753 - INFO - joeynmt.training - EPOCH 1542\n",
            "2021-02-01 13:23:45,102 - INFO - joeynmt.training - Epoch 1542: total training loss 9.03\n",
            "2021-02-01 13:23:45,103 - INFO - joeynmt.training - EPOCH 1543\n",
            "2021-02-01 13:23:57,414 - INFO - joeynmt.training - Epoch 1543: total training loss 8.99\n",
            "2021-02-01 13:23:57,415 - INFO - joeynmt.training - EPOCH 1544\n",
            "2021-02-01 13:24:09,702 - INFO - joeynmt.training - Epoch 1544: total training loss 9.02\n",
            "2021-02-01 13:24:09,703 - INFO - joeynmt.training - EPOCH 1545\n",
            "2021-02-01 13:24:22,122 - INFO - joeynmt.training - Epoch 1545: total training loss 9.04\n",
            "2021-02-01 13:24:22,122 - INFO - joeynmt.training - EPOCH 1546\n",
            "2021-02-01 13:24:27,784 - INFO - joeynmt.training - Epoch 1546, Step:    23100, Batch Loss:     0.601126, Tokens per Sec:    18470, Lr: 0.000050\n",
            "2021-02-01 13:24:34,427 - INFO - joeynmt.training - Epoch 1546: total training loss 9.03\n",
            "2021-02-01 13:24:34,428 - INFO - joeynmt.training - EPOCH 1547\n",
            "2021-02-01 13:24:46,652 - INFO - joeynmt.training - Epoch 1547: total training loss 9.00\n",
            "2021-02-01 13:24:46,652 - INFO - joeynmt.training - EPOCH 1548\n",
            "2021-02-01 13:24:58,885 - INFO - joeynmt.training - Epoch 1548: total training loss 8.99\n",
            "2021-02-01 13:24:58,885 - INFO - joeynmt.training - EPOCH 1549\n",
            "2021-02-01 13:25:11,138 - INFO - joeynmt.training - Epoch 1549: total training loss 9.02\n",
            "2021-02-01 13:25:11,139 - INFO - joeynmt.training - EPOCH 1550\n",
            "2021-02-01 13:25:23,313 - INFO - joeynmt.training - Epoch 1550: total training loss 9.00\n",
            "2021-02-01 13:25:23,313 - INFO - joeynmt.training - EPOCH 1551\n",
            "2021-02-01 13:25:35,809 - INFO - joeynmt.training - Epoch 1551: total training loss 8.96\n",
            "2021-02-01 13:25:35,809 - INFO - joeynmt.training - EPOCH 1552\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:25:48,080 - INFO - joeynmt.training - Epoch 1552: total training loss 8.97\n",
            "2021-02-01 13:25:48,081 - INFO - joeynmt.training - EPOCH 1553\n",
            "2021-02-01 13:25:49,679 - INFO - joeynmt.training - Epoch 1553, Step:    23200, Batch Loss:     0.586985, Tokens per Sec:    15987, Lr: 0.000050\n",
            "2021-02-01 13:26:00,550 - INFO - joeynmt.training - Epoch 1553: total training loss 8.98\n",
            "2021-02-01 13:26:00,551 - INFO - joeynmt.training - EPOCH 1554\n",
            "2021-02-01 13:26:13,026 - INFO - joeynmt.training - Epoch 1554: total training loss 8.99\n",
            "2021-02-01 13:26:13,026 - INFO - joeynmt.training - EPOCH 1555\n",
            "2021-02-01 13:26:25,197 - INFO - joeynmt.training - Epoch 1555: total training loss 8.98\n",
            "2021-02-01 13:26:25,198 - INFO - joeynmt.training - EPOCH 1556\n",
            "2021-02-01 13:26:37,600 - INFO - joeynmt.training - Epoch 1556: total training loss 9.02\n",
            "2021-02-01 13:26:37,601 - INFO - joeynmt.training - EPOCH 1557\n",
            "2021-02-01 13:26:49,793 - INFO - joeynmt.training - Epoch 1557: total training loss 8.98\n",
            "2021-02-01 13:26:49,793 - INFO - joeynmt.training - EPOCH 1558\n",
            "2021-02-01 13:27:02,094 - INFO - joeynmt.training - Epoch 1558: total training loss 8.98\n",
            "2021-02-01 13:27:02,095 - INFO - joeynmt.training - EPOCH 1559\n",
            "2021-02-01 13:27:11,818 - INFO - joeynmt.training - Epoch 1559, Step:    23300, Batch Loss:     0.591291, Tokens per Sec:    18487, Lr: 0.000050\n",
            "2021-02-01 13:27:14,420 - INFO - joeynmt.training - Epoch 1559: total training loss 8.95\n",
            "2021-02-01 13:27:14,420 - INFO - joeynmt.training - EPOCH 1560\n",
            "2021-02-01 13:27:26,686 - INFO - joeynmt.training - Epoch 1560: total training loss 9.00\n",
            "2021-02-01 13:27:26,687 - INFO - joeynmt.training - EPOCH 1561\n",
            "2021-02-01 13:27:38,946 - INFO - joeynmt.training - Epoch 1561: total training loss 9.01\n",
            "2021-02-01 13:27:38,947 - INFO - joeynmt.training - EPOCH 1562\n",
            "2021-02-01 13:27:51,166 - INFO - joeynmt.training - Epoch 1562: total training loss 8.97\n",
            "2021-02-01 13:27:51,166 - INFO - joeynmt.training - EPOCH 1563\n",
            "2021-02-01 13:28:03,418 - INFO - joeynmt.training - Epoch 1563: total training loss 8.96\n",
            "2021-02-01 13:28:03,418 - INFO - joeynmt.training - EPOCH 1564\n",
            "2021-02-01 13:28:15,851 - INFO - joeynmt.training - Epoch 1564: total training loss 9.01\n",
            "2021-02-01 13:28:15,851 - INFO - joeynmt.training - EPOCH 1565\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 13:28:28,233 - INFO - joeynmt.training - Epoch 1565: total training loss 8.97\n",
            "2021-02-01 13:28:28,233 - INFO - joeynmt.training - EPOCH 1566\n",
            "2021-02-01 13:28:33,961 - INFO - joeynmt.training - Epoch 1566, Step:    23400, Batch Loss:     0.593191, Tokens per Sec:    18823, Lr: 0.000050\n",
            "2021-02-01 13:28:40,564 - INFO - joeynmt.training - Epoch 1566: total training loss 8.97\n",
            "2021-02-01 13:28:40,564 - INFO - joeynmt.training - EPOCH 1567\n",
            "2021-02-01 13:28:53,084 - INFO - joeynmt.training - Epoch 1567: total training loss 8.98\n",
            "2021-02-01 13:28:53,085 - INFO - joeynmt.training - EPOCH 1568\n",
            "2021-02-01 13:29:05,364 - INFO - joeynmt.training - Epoch 1568: total training loss 8.92\n",
            "2021-02-01 13:29:05,364 - INFO - joeynmt.training - EPOCH 1569\n",
            "2021-02-01 13:29:17,618 - INFO - joeynmt.training - Epoch 1569: total training loss 8.97\n",
            "2021-02-01 13:29:17,619 - INFO - joeynmt.training - EPOCH 1570\n",
            "2021-02-01 13:29:29,950 - INFO - joeynmt.training - Epoch 1570: total training loss 8.95\n",
            "2021-02-01 13:29:29,951 - INFO - joeynmt.training - EPOCH 1571\n",
            "2021-02-01 13:29:42,407 - INFO - joeynmt.training - Epoch 1571: total training loss 8.96\n",
            "2021-02-01 13:29:42,407 - INFO - joeynmt.training - EPOCH 1572\n",
            "2021-02-01 13:29:54,844 - INFO - joeynmt.training - Epoch 1572: total training loss 8.94\n",
            "2021-02-01 13:29:54,845 - INFO - joeynmt.training - EPOCH 1573\n",
            "2021-02-01 13:29:56,508 - INFO - joeynmt.training - Epoch 1573, Step:    23500, Batch Loss:     0.560825, Tokens per Sec:    17469, Lr: 0.000050\n",
            "2021-02-01 13:30:24,869 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:30:24,870 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:30:24,870 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:30:24,871 - INFO - joeynmt.training - \tHypothesis: Себебі Таурат заңында Мұса арқылы берілген Таурат заңы мен ғауыр мәжілісханаларға жіберіп отырғанда, сол жерде мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 13:30:24,871 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:30:24,871 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:30:24,871 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:30:24,871 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:30:24,872 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:30:24,872 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:30:24,872 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:30:24,873 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, әлемді көрдім. Ол Тиянды тәнінде тұра билігі бар Құдайды мадақтап жүрдім. Оның мәнінде әйелдің үстінде тұрған, алтын жүзі мен Құдайды мадақтап күтіп тұрды.※\n",
            "2021-02-01 13:30:24,873 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:30:24,873 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:30:24,874 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:30:24,874 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне арналған рухани жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:30:24,874 - INFO - joeynmt.training - Validation result (greedy) at epoch 1573, step    23500: bleu:   5.28, loss: 87558.6875, ppl:  78.0412, duration: 28.3653s\n",
            "2021-02-01 13:30:35,589 - INFO - joeynmt.training - Epoch 1573: total training loss 8.95\n",
            "2021-02-01 13:30:35,589 - INFO - joeynmt.training - EPOCH 1574\n",
            "2021-02-01 13:30:47,921 - INFO - joeynmt.training - Epoch 1574: total training loss 8.99\n",
            "2021-02-01 13:30:47,922 - INFO - joeynmt.training - EPOCH 1575\n",
            "2021-02-01 13:31:00,044 - INFO - joeynmt.training - Epoch 1575: total training loss 8.37\n",
            "2021-02-01 13:31:00,045 - INFO - joeynmt.training - EPOCH 1576\n",
            "2021-02-01 13:31:12,392 - INFO - joeynmt.training - Epoch 1576: total training loss 9.00\n",
            "2021-02-01 13:31:12,393 - INFO - joeynmt.training - EPOCH 1577\n",
            "2021-02-01 13:31:24,735 - INFO - joeynmt.training - Epoch 1577: total training loss 8.95\n",
            "2021-02-01 13:31:24,735 - INFO - joeynmt.training - EPOCH 1578\n",
            "2021-02-01 13:31:36,926 - INFO - joeynmt.training - Epoch 1578: total training loss 8.95\n",
            "2021-02-01 13:31:36,927 - INFO - joeynmt.training - EPOCH 1579\n",
            "2021-02-01 13:31:47,401 - INFO - joeynmt.training - Epoch 1579, Step:    23600, Batch Loss:     0.544048, Tokens per Sec:    18403, Lr: 0.000050\n",
            "2021-02-01 13:31:49,252 - INFO - joeynmt.training - Epoch 1579: total training loss 8.93\n",
            "2021-02-01 13:31:49,253 - INFO - joeynmt.training - EPOCH 1580\n",
            "2021-02-01 13:32:01,579 - INFO - joeynmt.training - Epoch 1580: total training loss 8.92\n",
            "2021-02-01 13:32:01,579 - INFO - joeynmt.training - EPOCH 1581\n",
            "2021-02-01 13:32:13,648 - INFO - joeynmt.training - Epoch 1581: total training loss 8.38\n",
            "2021-02-01 13:32:13,648 - INFO - joeynmt.training - EPOCH 1582\n",
            "2021-02-01 13:32:26,125 - INFO - joeynmt.training - Epoch 1582: total training loss 8.97\n",
            "2021-02-01 13:32:26,125 - INFO - joeynmt.training - EPOCH 1583\n",
            "2021-02-01 13:32:38,552 - INFO - joeynmt.training - Epoch 1583: total training loss 8.98\n",
            "2021-02-01 13:32:38,552 - INFO - joeynmt.training - EPOCH 1584\n",
            "2021-02-01 13:32:50,817 - INFO - joeynmt.training - Epoch 1584: total training loss 8.99\n",
            "2021-02-01 13:32:50,818 - INFO - joeynmt.training - EPOCH 1585\n",
            "2021-02-01 13:33:03,146 - INFO - joeynmt.training - Epoch 1585: total training loss 8.95\n",
            "2021-02-01 13:33:03,147 - INFO - joeynmt.training - EPOCH 1586\n",
            "2021-02-01 13:33:10,475 - INFO - joeynmt.training - Epoch 1586, Step:    23700, Batch Loss:     0.648954, Tokens per Sec:    18716, Lr: 0.000050\n",
            "2021-02-01 13:33:15,535 - INFO - joeynmt.training - Epoch 1586: total training loss 8.94\n",
            "2021-02-01 13:33:15,536 - INFO - joeynmt.training - EPOCH 1587\n",
            "2021-02-01 13:33:27,957 - INFO - joeynmt.training - Epoch 1587: total training loss 8.88\n",
            "2021-02-01 13:33:27,958 - INFO - joeynmt.training - EPOCH 1588\n",
            "2021-02-01 13:33:40,113 - INFO - joeynmt.training - Epoch 1588: total training loss 8.38\n",
            "2021-02-01 13:33:40,114 - INFO - joeynmt.training - EPOCH 1589\n",
            "2021-02-01 13:33:52,452 - INFO - joeynmt.training - Epoch 1589: total training loss 8.97\n",
            "2021-02-01 13:33:52,453 - INFO - joeynmt.training - EPOCH 1590\n",
            "2021-02-01 13:34:04,741 - INFO - joeynmt.training - Epoch 1590: total training loss 8.95\n",
            "2021-02-01 13:34:04,742 - INFO - joeynmt.training - EPOCH 1591\n",
            "2021-02-01 13:34:17,175 - INFO - joeynmt.training - Epoch 1591: total training loss 8.92\n",
            "2021-02-01 13:34:17,175 - INFO - joeynmt.training - EPOCH 1592\n",
            "2021-02-01 13:34:29,569 - INFO - joeynmt.training - Epoch 1592: total training loss 8.90\n",
            "2021-02-01 13:34:29,570 - INFO - joeynmt.training - EPOCH 1593\n",
            "2021-02-01 13:34:33,654 - INFO - joeynmt.training - Epoch 1593, Step:    23800, Batch Loss:     0.603177, Tokens per Sec:    18955, Lr: 0.000050\n",
            "2021-02-01 13:34:41,773 - INFO - joeynmt.training - Epoch 1593: total training loss 8.94\n",
            "2021-02-01 13:34:41,774 - INFO - joeynmt.training - EPOCH 1594\n",
            "2021-02-01 13:34:54,101 - INFO - joeynmt.training - Epoch 1594: total training loss 8.93\n",
            "2021-02-01 13:34:54,101 - INFO - joeynmt.training - EPOCH 1595\n",
            "2021-02-01 13:35:06,546 - INFO - joeynmt.training - Epoch 1595: total training loss 8.91\n",
            "2021-02-01 13:35:06,546 - INFO - joeynmt.training - EPOCH 1596\n",
            "2021-02-01 13:35:18,882 - INFO - joeynmt.training - Epoch 1596: total training loss 8.93\n",
            "2021-02-01 13:35:18,882 - INFO - joeynmt.training - EPOCH 1597\n",
            "2021-02-01 13:35:31,381 - INFO - joeynmt.training - Epoch 1597: total training loss 8.99\n",
            "2021-02-01 13:35:31,381 - INFO - joeynmt.training - EPOCH 1598\n",
            "2021-02-01 13:35:43,773 - INFO - joeynmt.training - Epoch 1598: total training loss 8.90\n",
            "2021-02-01 13:35:43,774 - INFO - joeynmt.training - EPOCH 1599\n",
            "2021-02-01 13:35:56,087 - INFO - joeynmt.training - Epoch 1599, Step:    23900, Batch Loss:     0.575898, Tokens per Sec:    18570, Lr: 0.000050\n",
            "2021-02-01 13:35:56,087 - INFO - joeynmt.training - Epoch 1599: total training loss 8.95\n",
            "2021-02-01 13:35:56,088 - INFO - joeynmt.training - EPOCH 1600\n",
            "2021-02-01 13:36:08,460 - INFO - joeynmt.training - Epoch 1600: total training loss 8.92\n",
            "2021-02-01 13:36:08,460 - INFO - joeynmt.training - EPOCH 1601\n",
            "2021-02-01 13:36:20,739 - INFO - joeynmt.training - Epoch 1601: total training loss 8.93\n",
            "2021-02-01 13:36:20,739 - INFO - joeynmt.training - EPOCH 1602\n",
            "2021-02-01 13:36:33,026 - INFO - joeynmt.training - Epoch 1602: total training loss 8.93\n",
            "2021-02-01 13:36:33,027 - INFO - joeynmt.training - EPOCH 1603\n",
            "2021-02-01 13:36:45,360 - INFO - joeynmt.training - Epoch 1603: total training loss 8.94\n",
            "2021-02-01 13:36:45,361 - INFO - joeynmt.training - EPOCH 1604\n",
            "2021-02-01 13:36:57,752 - INFO - joeynmt.training - Epoch 1604: total training loss 8.91\n",
            "2021-02-01 13:36:57,752 - INFO - joeynmt.training - EPOCH 1605\n",
            "2021-02-01 13:37:10,272 - INFO - joeynmt.training - Epoch 1605: total training loss 8.91\n",
            "2021-02-01 13:37:10,272 - INFO - joeynmt.training - EPOCH 1606\n",
            "2021-02-01 13:37:18,383 - INFO - joeynmt.training - Epoch 1606, Step:    24000, Batch Loss:     0.598303, Tokens per Sec:    17908, Lr: 0.000050\n",
            "2021-02-01 13:37:48,760 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:37:48,761 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:37:48,761 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:37:48,761 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына ертіп қалаға ие болды. Олар мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 13:37:48,761 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:37:48,762 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартуларын оқтының астына жіберіп, оған ілемекентай етіп тағайындаған Құдай Тоқты※ көрдім. Ол қатты дауыспен Құдайды қастерлейтін әйелдер ! Олар қатты ашылықты болмаған※ ештеңе болмадым.\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне арналған рухани жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:37:48,763 - INFO - joeynmt.training - Validation result (greedy) at epoch 1606, step    24000: bleu:   4.94, loss: 87660.6016, ppl:  78.4380, duration: 30.3798s\n",
            "2021-02-01 13:37:53,143 - INFO - joeynmt.training - Epoch 1606: total training loss 8.90\n",
            "2021-02-01 13:37:53,143 - INFO - joeynmt.training - EPOCH 1607\n",
            "2021-02-01 13:38:05,610 - INFO - joeynmt.training - Epoch 1607: total training loss 8.93\n",
            "2021-02-01 13:38:05,610 - INFO - joeynmt.training - EPOCH 1608\n",
            "2021-02-01 13:38:17,946 - INFO - joeynmt.training - Epoch 1608: total training loss 8.90\n",
            "2021-02-01 13:38:17,947 - INFO - joeynmt.training - EPOCH 1609\n",
            "2021-02-01 13:38:30,136 - INFO - joeynmt.training - Epoch 1609: total training loss 8.36\n",
            "2021-02-01 13:38:30,137 - INFO - joeynmt.training - EPOCH 1610\n",
            "2021-02-01 13:38:42,689 - INFO - joeynmt.training - Epoch 1610: total training loss 8.92\n",
            "2021-02-01 13:38:42,690 - INFO - joeynmt.training - EPOCH 1611\n",
            "2021-02-01 13:38:55,045 - INFO - joeynmt.training - Epoch 1611: total training loss 8.93\n",
            "2021-02-01 13:38:55,045 - INFO - joeynmt.training - EPOCH 1612\n",
            "2021-02-01 13:39:07,604 - INFO - joeynmt.training - Epoch 1612: total training loss 8.88\n",
            "2021-02-01 13:39:07,605 - INFO - joeynmt.training - EPOCH 1613\n",
            "2021-02-01 13:39:12,560 - INFO - joeynmt.training - Epoch 1613, Step:    24100, Batch Loss:     0.600321, Tokens per Sec:    17680, Lr: 0.000050\n",
            "2021-02-01 13:39:19,962 - INFO - joeynmt.training - Epoch 1613: total training loss 8.91\n",
            "2021-02-01 13:39:19,962 - INFO - joeynmt.training - EPOCH 1614\n",
            "2021-02-01 13:39:32,349 - INFO - joeynmt.training - Epoch 1614: total training loss 8.91\n",
            "2021-02-01 13:39:32,349 - INFO - joeynmt.training - EPOCH 1615\n",
            "2021-02-01 13:39:44,570 - INFO - joeynmt.training - Epoch 1615: total training loss 8.94\n",
            "2021-02-01 13:39:44,571 - INFO - joeynmt.training - EPOCH 1616\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 13:39:57,103 - INFO - joeynmt.training - Epoch 1616: total training loss 8.85\n",
            "2021-02-01 13:39:57,104 - INFO - joeynmt.training - EPOCH 1617\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:40:09,518 - INFO - joeynmt.training - Epoch 1617: total training loss 8.93\n",
            "2021-02-01 13:40:09,518 - INFO - joeynmt.training - EPOCH 1618\n",
            "2021-02-01 13:40:22,006 - INFO - joeynmt.training - Epoch 1618: total training loss 8.91\n",
            "2021-02-01 13:40:22,007 - INFO - joeynmt.training - EPOCH 1619\n",
            "2021-02-01 13:40:34,472 - INFO - joeynmt.training - Epoch 1619: total training loss 8.89\n",
            "2021-02-01 13:40:34,472 - INFO - joeynmt.training - EPOCH 1620\n",
            "2021-02-01 13:40:35,341 - INFO - joeynmt.training - Epoch 1620, Step:    24200, Batch Loss:     0.602867, Tokens per Sec:    18216, Lr: 0.000050\n",
            "2021-02-01 13:40:47,013 - INFO - joeynmt.training - Epoch 1620: total training loss 8.92\n",
            "2021-02-01 13:40:47,014 - INFO - joeynmt.training - EPOCH 1621\n",
            "2021-02-01 13:40:59,193 - INFO - joeynmt.training - Epoch 1621: total training loss 8.32\n",
            "2021-02-01 13:40:59,193 - INFO - joeynmt.training - EPOCH 1622\n",
            "2021-02-01 13:41:11,630 - INFO - joeynmt.training - Epoch 1622: total training loss 8.88\n",
            "2021-02-01 13:41:11,631 - INFO - joeynmt.training - EPOCH 1623\n",
            "2021-02-01 13:41:24,072 - INFO - joeynmt.training - Epoch 1623: total training loss 8.89\n",
            "2021-02-01 13:41:24,073 - INFO - joeynmt.training - EPOCH 1624\n",
            "2021-02-01 13:41:36,441 - INFO - joeynmt.training - Epoch 1624: total training loss 8.87\n",
            "2021-02-01 13:41:36,442 - INFO - joeynmt.training - EPOCH 1625\n",
            "2021-02-01 13:41:48,769 - INFO - joeynmt.training - Epoch 1625: total training loss 8.91\n",
            "2021-02-01 13:41:48,770 - INFO - joeynmt.training - EPOCH 1626\n",
            "2021-02-01 13:41:58,566 - INFO - joeynmt.training - Epoch 1626, Step:    24300, Batch Loss:     0.619035, Tokens per Sec:    18507, Lr: 0.000050\n",
            "2021-02-01 13:42:01,244 - INFO - joeynmt.training - Epoch 1626: total training loss 8.87\n",
            "2021-02-01 13:42:01,244 - INFO - joeynmt.training - EPOCH 1627\n",
            "2021-02-01 13:42:13,550 - INFO - joeynmt.training - Epoch 1627: total training loss 8.88\n",
            "2021-02-01 13:42:13,551 - INFO - joeynmt.training - EPOCH 1628\n",
            "2021-02-01 13:42:25,871 - INFO - joeynmt.training - Epoch 1628: total training loss 8.90\n",
            "2021-02-01 13:42:25,872 - INFO - joeynmt.training - EPOCH 1629\n",
            "2021-02-01 13:42:38,325 - INFO - joeynmt.training - Epoch 1629: total training loss 8.89\n",
            "2021-02-01 13:42:38,325 - INFO - joeynmt.training - EPOCH 1630\n",
            "2021-02-01 13:42:50,839 - INFO - joeynmt.training - Epoch 1630: total training loss 8.89\n",
            "2021-02-01 13:42:50,839 - INFO - joeynmt.training - EPOCH 1631\n",
            "2021-02-01 13:43:03,104 - INFO - joeynmt.training - Epoch 1631: total training loss 8.91\n",
            "2021-02-01 13:43:03,105 - INFO - joeynmt.training - EPOCH 1632\n",
            "2021-02-01 13:43:15,502 - INFO - joeynmt.training - Epoch 1632: total training loss 8.88\n",
            "2021-02-01 13:43:15,502 - INFO - joeynmt.training - EPOCH 1633\n",
            "2021-02-01 13:43:21,186 - INFO - joeynmt.training - Epoch 1633, Step:    24400, Batch Loss:     0.582017, Tokens per Sec:    19027, Lr: 0.000050\n",
            "2021-02-01 13:43:27,808 - INFO - joeynmt.training - Epoch 1633: total training loss 8.91\n",
            "2021-02-01 13:43:27,809 - INFO - joeynmt.training - EPOCH 1634\n",
            "2021-02-01 13:43:40,087 - INFO - joeynmt.training - Epoch 1634: total training loss 8.90\n",
            "2021-02-01 13:43:40,087 - INFO - joeynmt.training - EPOCH 1635\n",
            "2021-02-01 13:43:52,506 - INFO - joeynmt.training - Epoch 1635: total training loss 8.91\n",
            "2021-02-01 13:43:52,506 - INFO - joeynmt.training - EPOCH 1636\n",
            "2021-02-01 13:44:04,892 - INFO - joeynmt.training - Epoch 1636: total training loss 8.87\n",
            "2021-02-01 13:44:04,893 - INFO - joeynmt.training - EPOCH 1637\n",
            "2021-02-01 13:44:17,128 - INFO - joeynmt.training - Epoch 1637: total training loss 8.90\n",
            "2021-02-01 13:44:17,128 - INFO - joeynmt.training - EPOCH 1638\n",
            "2021-02-01 13:44:29,408 - INFO - joeynmt.training - Epoch 1638: total training loss 8.84\n",
            "2021-02-01 13:44:29,408 - INFO - joeynmt.training - EPOCH 1639\n",
            "2021-02-01 13:44:41,782 - INFO - joeynmt.training - Epoch 1639: total training loss 8.91\n",
            "2021-02-01 13:44:41,782 - INFO - joeynmt.training - EPOCH 1640\n",
            "2021-02-01 13:44:43,452 - INFO - joeynmt.training - Epoch 1640, Step:    24500, Batch Loss:     0.583177, Tokens per Sec:    17436, Lr: 0.000050\n",
            "2021-02-01 13:45:11,853 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:45:11,854 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:45:11,854 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:45:11,854 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 13:45:11,855 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:45:11,855 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:45:11,855 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:45:11,855 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:45:11,856 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:45:11,856 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:45:11,856 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:45:11,856 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартуларын оқымап жіберіп, әлемді көрдім. Ол Тиянды тәнінде ешкімге патшалық көрме! Құдайды мадақтап отырған жеті мың қатты ашылым болмаған※ ештеңе болмадым.\n",
            "2021-02-01 13:45:11,857 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:45:11,857 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:45:11,857 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:45:11,857 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне арналған рухани жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 13:45:11,858 - INFO - joeynmt.training - Validation result (greedy) at epoch 1640, step    24500: bleu:   4.69, loss: 87601.2344, ppl:  78.2066, duration: 28.4050s\n",
            "2021-02-01 13:45:22,474 - INFO - joeynmt.training - Epoch 1640: total training loss 8.90\n",
            "2021-02-01 13:45:22,474 - INFO - joeynmt.training - EPOCH 1641\n",
            "2021-02-01 13:45:34,852 - INFO - joeynmt.training - Epoch 1641: total training loss 8.90\n",
            "2021-02-01 13:45:34,852 - INFO - joeynmt.training - EPOCH 1642\n",
            "2021-02-01 13:45:47,239 - INFO - joeynmt.training - Epoch 1642: total training loss 8.89\n",
            "2021-02-01 13:45:47,239 - INFO - joeynmt.training - EPOCH 1643\n",
            "2021-02-01 13:45:59,626 - INFO - joeynmt.training - Epoch 1643: total training loss 8.88\n",
            "2021-02-01 13:45:59,626 - INFO - joeynmt.training - EPOCH 1644\n",
            "2021-02-01 13:46:11,962 - INFO - joeynmt.training - Epoch 1644: total training loss 8.86\n",
            "2021-02-01 13:46:11,963 - INFO - joeynmt.training - EPOCH 1645\n",
            "2021-02-01 13:46:24,273 - INFO - joeynmt.training - Epoch 1645: total training loss 8.90\n",
            "2021-02-01 13:46:24,274 - INFO - joeynmt.training - EPOCH 1646\n",
            "2021-02-01 13:46:34,189 - INFO - joeynmt.training - Epoch 1646, Step:    24600, Batch Loss:     0.618177, Tokens per Sec:    18561, Lr: 0.000050\n",
            "2021-02-01 13:46:36,712 - INFO - joeynmt.training - Epoch 1646: total training loss 8.93\n",
            "2021-02-01 13:46:36,712 - INFO - joeynmt.training - EPOCH 1647\n",
            "2021-02-01 13:46:49,182 - INFO - joeynmt.training - Epoch 1647: total training loss 8.89\n",
            "2021-02-01 13:46:49,182 - INFO - joeynmt.training - EPOCH 1648\n",
            "2021-02-01 13:47:01,606 - INFO - joeynmt.training - Epoch 1648: total training loss 8.84\n",
            "2021-02-01 13:47:01,607 - INFO - joeynmt.training - EPOCH 1649\n",
            "2021-02-01 13:47:13,943 - INFO - joeynmt.training - Epoch 1649: total training loss 8.85\n",
            "2021-02-01 13:47:13,944 - INFO - joeynmt.training - EPOCH 1650\n",
            "2021-02-01 13:47:26,172 - INFO - joeynmt.training - Epoch 1650: total training loss 8.89\n",
            "2021-02-01 13:47:26,172 - INFO - joeynmt.training - EPOCH 1651\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 13:47:38,484 - INFO - joeynmt.training - Epoch 1651: total training loss 8.91\n",
            "2021-02-01 13:47:38,484 - INFO - joeynmt.training - EPOCH 1652\n",
            "2021-02-01 13:47:50,782 - INFO - joeynmt.training - Epoch 1652: total training loss 8.87\n",
            "2021-02-01 13:47:50,783 - INFO - joeynmt.training - EPOCH 1653\n",
            "2021-02-01 13:47:56,492 - INFO - joeynmt.training - Epoch 1653, Step:    24700, Batch Loss:     0.608811, Tokens per Sec:    17897, Lr: 0.000050\n",
            "2021-02-01 13:48:03,167 - INFO - joeynmt.training - Epoch 1653: total training loss 8.87\n",
            "2021-02-01 13:48:03,168 - INFO - joeynmt.training - EPOCH 1654\n",
            "2021-02-01 13:48:15,710 - INFO - joeynmt.training - Epoch 1654: total training loss 8.86\n",
            "2021-02-01 13:48:15,711 - INFO - joeynmt.training - EPOCH 1655\n",
            "2021-02-01 13:48:28,134 - INFO - joeynmt.training - Epoch 1655: total training loss 8.85\n",
            "2021-02-01 13:48:28,134 - INFO - joeynmt.training - EPOCH 1656\n",
            "2021-02-01 13:48:40,440 - INFO - joeynmt.training - Epoch 1656: total training loss 8.89\n",
            "2021-02-01 13:48:40,440 - INFO - joeynmt.training - EPOCH 1657\n",
            "2021-02-01 13:48:52,792 - INFO - joeynmt.training - Epoch 1657: total training loss 8.86\n",
            "2021-02-01 13:48:52,792 - INFO - joeynmt.training - EPOCH 1658\n",
            "2021-02-01 13:49:05,089 - INFO - joeynmt.training - Epoch 1658: total training loss 8.84\n",
            "2021-02-01 13:49:05,090 - INFO - joeynmt.training - EPOCH 1659\n",
            "2021-02-01 13:49:17,364 - INFO - joeynmt.training - Epoch 1659: total training loss 8.84\n",
            "2021-02-01 13:49:17,365 - INFO - joeynmt.training - EPOCH 1660\n",
            "2021-02-01 13:49:19,008 - INFO - joeynmt.training - Epoch 1660, Step:    24800, Batch Loss:     0.594584, Tokens per Sec:    18916, Lr: 0.000050\n",
            "2021-02-01 13:49:29,701 - INFO - joeynmt.training - Epoch 1660: total training loss 8.88\n",
            "2021-02-01 13:49:29,702 - INFO - joeynmt.training - EPOCH 1661\n",
            "2021-02-01 13:49:41,995 - INFO - joeynmt.training - Epoch 1661: total training loss 8.87\n",
            "2021-02-01 13:49:41,995 - INFO - joeynmt.training - EPOCH 1662\n",
            "2021-02-01 13:49:54,375 - INFO - joeynmt.training - Epoch 1662: total training loss 8.84\n",
            "2021-02-01 13:49:54,375 - INFO - joeynmt.training - EPOCH 1663\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:50:06,751 - INFO - joeynmt.training - Epoch 1663: total training loss 8.83\n",
            "2021-02-01 13:50:06,752 - INFO - joeynmt.training - EPOCH 1664\n",
            "2021-02-01 13:50:18,917 - INFO - joeynmt.training - Epoch 1664: total training loss 8.87\n",
            "2021-02-01 13:50:18,917 - INFO - joeynmt.training - EPOCH 1665\n",
            "2021-02-01 13:50:31,062 - INFO - joeynmt.training - Epoch 1665: total training loss 8.89\n",
            "2021-02-01 13:50:31,062 - INFO - joeynmt.training - EPOCH 1666\n",
            "2021-02-01 13:50:40,880 - INFO - joeynmt.training - Epoch 1666, Step:    24900, Batch Loss:     0.567125, Tokens per Sec:    18531, Lr: 0.000050\n",
            "2021-02-01 13:50:43,200 - INFO - joeynmt.training - Epoch 1666: total training loss 8.32\n",
            "2021-02-01 13:50:43,200 - INFO - joeynmt.training - EPOCH 1667\n",
            "2021-02-01 13:50:55,595 - INFO - joeynmt.training - Epoch 1667: total training loss 8.82\n",
            "2021-02-01 13:50:55,595 - INFO - joeynmt.training - EPOCH 1668\n",
            "2021-02-01 13:51:07,955 - INFO - joeynmt.training - Epoch 1668: total training loss 8.86\n",
            "2021-02-01 13:51:07,956 - INFO - joeynmt.training - EPOCH 1669\n",
            "2021-02-01 13:51:20,173 - INFO - joeynmt.training - Epoch 1669: total training loss 8.86\n",
            "2021-02-01 13:51:20,173 - INFO - joeynmt.training - EPOCH 1670\n",
            "2021-02-01 13:51:32,569 - INFO - joeynmt.training - Epoch 1670: total training loss 8.82\n",
            "2021-02-01 13:51:32,569 - INFO - joeynmt.training - EPOCH 1671\n",
            "2021-02-01 13:51:44,834 - INFO - joeynmt.training - Epoch 1671: total training loss 8.84\n",
            "2021-02-01 13:51:44,834 - INFO - joeynmt.training - EPOCH 1672\n",
            "2021-02-01 13:51:57,166 - INFO - joeynmt.training - Epoch 1672: total training loss 8.85\n",
            "2021-02-01 13:51:57,166 - INFO - joeynmt.training - EPOCH 1673\n",
            "2021-02-01 13:52:03,600 - INFO - joeynmt.training - Epoch 1673, Step:    25000, Batch Loss:     0.593685, Tokens per Sec:    18238, Lr: 0.000050\n",
            "2021-02-01 13:52:33,429 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:52:33,430 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:52:33,430 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:52:33,430 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға ертіп қаладан шығып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 13:52:33,431 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:52:33,431 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:52:33,431 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:52:33,431 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:52:33,431 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:52:33,432 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:52:33,432 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:52:33,432 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, оны жерге бас тарту ретінде берілді. Тоқтының үстінде отырған Құдай осы дүниені көрме! ! Ол қатты ашылықты айуанның есімі мен Құдайды мадақтап күткендерді (яғни Құдайды мадақтап отырды.※\n",
            "2021-02-01 13:52:33,432 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:52:33,433 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:52:33,433 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:52:33,433 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермекендерің де Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 13:52:33,433 - INFO - joeynmt.training - Validation result (greedy) at epoch 1673, step    25000: bleu:   5.16, loss: 87693.7031, ppl:  78.5673, duration: 29.8328s\n",
            "2021-02-01 13:52:39,256 - INFO - joeynmt.training - Epoch 1673: total training loss 8.87\n",
            "2021-02-01 13:52:39,256 - INFO - joeynmt.training - EPOCH 1674\n",
            "2021-02-01 13:52:51,626 - INFO - joeynmt.training - Epoch 1674: total training loss 8.83\n",
            "2021-02-01 13:52:51,627 - INFO - joeynmt.training - EPOCH 1675\n",
            "2021-02-01 13:53:04,056 - INFO - joeynmt.training - Epoch 1675: total training loss 8.83\n",
            "2021-02-01 13:53:04,056 - INFO - joeynmt.training - EPOCH 1676\n",
            "2021-02-01 13:53:16,429 - INFO - joeynmt.training - Epoch 1676: total training loss 8.84\n",
            "2021-02-01 13:53:16,430 - INFO - joeynmt.training - EPOCH 1677\n",
            "2021-02-01 13:53:28,745 - INFO - joeynmt.training - Epoch 1677: total training loss 8.84\n",
            "2021-02-01 13:53:28,746 - INFO - joeynmt.training - EPOCH 1678\n",
            "2021-02-01 13:53:40,906 - INFO - joeynmt.training - Epoch 1678: total training loss 8.87\n",
            "2021-02-01 13:53:40,907 - INFO - joeynmt.training - EPOCH 1679\n",
            "2021-02-01 13:53:53,322 - INFO - joeynmt.training - Epoch 1679: total training loss 8.85\n",
            "2021-02-01 13:53:53,322 - INFO - joeynmt.training - EPOCH 1680\n",
            "2021-02-01 13:53:55,789 - INFO - joeynmt.training - Epoch 1680, Step:    25100, Batch Loss:     0.613447, Tokens per Sec:    18648, Lr: 0.000050\n",
            "2021-02-01 13:54:05,433 - INFO - joeynmt.training - Epoch 1680: total training loss 8.30\n",
            "2021-02-01 13:54:05,434 - INFO - joeynmt.training - EPOCH 1681\n",
            "2021-02-01 13:54:17,737 - INFO - joeynmt.training - Epoch 1681: total training loss 8.89\n",
            "2021-02-01 13:54:17,737 - INFO - joeynmt.training - EPOCH 1682\n",
            "2021-02-01 13:54:29,944 - INFO - joeynmt.training - Epoch 1682: total training loss 8.84\n",
            "2021-02-01 13:54:29,944 - INFO - joeynmt.training - EPOCH 1683\n",
            "2021-02-01 13:54:42,126 - INFO - joeynmt.training - Epoch 1683: total training loss 8.88\n",
            "2021-02-01 13:54:42,127 - INFO - joeynmt.training - EPOCH 1684\n",
            "2021-02-01 13:54:54,265 - INFO - joeynmt.training - Epoch 1684: total training loss 8.82\n",
            "2021-02-01 13:54:54,266 - INFO - joeynmt.training - EPOCH 1685\n",
            "2021-02-01 13:55:06,765 - INFO - joeynmt.training - Epoch 1685: total training loss 8.83\n",
            "2021-02-01 13:55:06,766 - INFO - joeynmt.training - EPOCH 1686\n",
            "2021-02-01 13:55:18,159 - INFO - joeynmt.training - Epoch 1686, Step:    25200, Batch Loss:     0.585024, Tokens per Sec:    18851, Lr: 0.000050\n",
            "2021-02-01 13:55:18,823 - INFO - joeynmt.training - Epoch 1686: total training loss 8.26\n",
            "2021-02-01 13:55:18,824 - INFO - joeynmt.training - EPOCH 1687\n",
            "2021-02-01 13:55:31,178 - INFO - joeynmt.training - Epoch 1687: total training loss 8.85\n",
            "2021-02-01 13:55:31,178 - INFO - joeynmt.training - EPOCH 1688\n",
            "2021-02-01 13:55:43,412 - INFO - joeynmt.training - Epoch 1688: total training loss 8.84\n",
            "2021-02-01 13:55:43,412 - INFO - joeynmt.training - EPOCH 1689\n",
            "2021-02-01 13:55:55,591 - INFO - joeynmt.training - Epoch 1689: total training loss 8.81\n",
            "2021-02-01 13:55:55,592 - INFO - joeynmt.training - EPOCH 1690\n",
            "2021-02-01 13:56:07,894 - INFO - joeynmt.training - Epoch 1690: total training loss 8.83\n",
            "2021-02-01 13:56:07,894 - INFO - joeynmt.training - EPOCH 1691\n",
            "2021-02-01 13:56:19,987 - INFO - joeynmt.training - Epoch 1691: total training loss 8.84\n",
            "2021-02-01 13:56:19,988 - INFO - joeynmt.training - EPOCH 1692\n",
            "2021-02-01 13:56:32,365 - INFO - joeynmt.training - Epoch 1692: total training loss 8.85\n",
            "2021-02-01 13:56:32,366 - INFO - joeynmt.training - EPOCH 1693\n",
            "2021-02-01 13:56:40,494 - INFO - joeynmt.training - Epoch 1693, Step:    25300, Batch Loss:     0.593551, Tokens per Sec:    18381, Lr: 0.000050\n",
            "2021-02-01 13:56:44,661 - INFO - joeynmt.training - Epoch 1693: total training loss 8.85\n",
            "2021-02-01 13:56:44,661 - INFO - joeynmt.training - EPOCH 1694\n",
            "2021-02-01 13:56:56,907 - INFO - joeynmt.training - Epoch 1694: total training loss 8.85\n",
            "2021-02-01 13:56:56,907 - INFO - joeynmt.training - EPOCH 1695\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 13:57:09,246 - INFO - joeynmt.training - Epoch 1695: total training loss 8.83\n",
            "2021-02-01 13:57:09,246 - INFO - joeynmt.training - EPOCH 1696\n",
            "2021-02-01 13:57:21,532 - INFO - joeynmt.training - Epoch 1696: total training loss 8.85\n",
            "2021-02-01 13:57:21,532 - INFO - joeynmt.training - EPOCH 1697\n",
            "2021-02-01 13:57:33,861 - INFO - joeynmt.training - Epoch 1697: total training loss 8.84\n",
            "2021-02-01 13:57:33,861 - INFO - joeynmt.training - EPOCH 1698\n",
            "2021-02-01 13:57:46,036 - INFO - joeynmt.training - Epoch 1698: total training loss 8.83\n",
            "2021-02-01 13:57:46,036 - INFO - joeynmt.training - EPOCH 1699\n",
            "2021-02-01 13:57:58,320 - INFO - joeynmt.training - Epoch 1699: total training loss 8.82\n",
            "2021-02-01 13:57:58,320 - INFO - joeynmt.training - EPOCH 1700\n",
            "2021-02-01 13:58:02,334 - INFO - joeynmt.training - Epoch 1700, Step:    25400, Batch Loss:     0.596714, Tokens per Sec:    18325, Lr: 0.000050\n",
            "2021-02-01 13:58:10,601 - INFO - joeynmt.training - Epoch 1700: total training loss 8.84\n",
            "2021-02-01 13:58:10,601 - INFO - joeynmt.training - EPOCH 1701\n",
            "2021-02-01 13:58:22,766 - INFO - joeynmt.training - Epoch 1701: total training loss 8.81\n",
            "2021-02-01 13:58:22,766 - INFO - joeynmt.training - EPOCH 1702\n",
            "2021-02-01 13:58:34,937 - INFO - joeynmt.training - Epoch 1702: total training loss 8.83\n",
            "2021-02-01 13:58:34,938 - INFO - joeynmt.training - EPOCH 1703\n",
            "2021-02-01 13:58:47,187 - INFO - joeynmt.training - Epoch 1703: total training loss 8.83\n",
            "2021-02-01 13:58:47,188 - INFO - joeynmt.training - EPOCH 1704\n",
            "2021-02-01 13:58:59,524 - INFO - joeynmt.training - Epoch 1704: total training loss 8.79\n",
            "2021-02-01 13:58:59,524 - INFO - joeynmt.training - EPOCH 1705\n",
            "2021-02-01 13:59:11,981 - INFO - joeynmt.training - Epoch 1705: total training loss 8.82\n",
            "2021-02-01 13:59:11,981 - INFO - joeynmt.training - EPOCH 1706\n",
            "2021-02-01 13:59:24,112 - INFO - joeynmt.training - Epoch 1706, Step:    25500, Batch Loss:     0.563828, Tokens per Sec:    18496, Lr: 0.000050\n",
            "2021-02-01 13:59:51,495 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 13:59:51,496 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 13:59:51,496 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 13:59:51,497 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 13:59:51,497 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 13:59:51,497 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 13:59:51,497 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 13:59:51,498 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 13:59:51,498 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 13:59:51,498 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 13:59:51,498 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 13:59:51,498 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еткізіп, оған ілесіп тұрған кезімде көрдім. Ол мұны көрген ешқандай қылмыс таптыр екен. Үй етіп қатты ашуландырды.\n",
            "2021-02-01 13:59:51,499 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 13:59:51,499 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 13:59:51,499 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 13:59:51,499 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермеңдер, көкке көтеріліп, Құдайдың оң жағындағы құрметті орнына отырды.\n",
            "2021-02-01 13:59:51,500 - INFO - joeynmt.training - Validation result (greedy) at epoch 1706, step    25500: bleu:   5.18, loss: 87623.1797, ppl:  78.2921, duration: 27.3875s\n",
            "2021-02-01 13:59:51,719 - INFO - joeynmt.training - Epoch 1706: total training loss 8.81\n",
            "2021-02-01 13:59:51,720 - INFO - joeynmt.training - EPOCH 1707\n",
            "2021-02-01 14:00:04,098 - INFO - joeynmt.training - Epoch 1707: total training loss 8.81\n",
            "2021-02-01 14:00:04,099 - INFO - joeynmt.training - EPOCH 1708\n",
            "2021-02-01 14:00:16,399 - INFO - joeynmt.training - Epoch 1708: total training loss 8.78\n",
            "2021-02-01 14:00:16,400 - INFO - joeynmt.training - EPOCH 1709\n",
            "2021-02-01 14:00:28,818 - INFO - joeynmt.training - Epoch 1709: total training loss 8.85\n",
            "2021-02-01 14:00:28,818 - INFO - joeynmt.training - EPOCH 1710\n",
            "2021-02-01 14:00:41,085 - INFO - joeynmt.training - Epoch 1710: total training loss 8.80\n",
            "2021-02-01 14:00:41,085 - INFO - joeynmt.training - EPOCH 1711\n",
            "2021-02-01 14:00:53,354 - INFO - joeynmt.training - Epoch 1711: total training loss 8.82\n",
            "2021-02-01 14:00:53,354 - INFO - joeynmt.training - EPOCH 1712\n",
            "2021-02-01 14:01:05,628 - INFO - joeynmt.training - Epoch 1712: total training loss 8.82\n",
            "2021-02-01 14:01:05,628 - INFO - joeynmt.training - EPOCH 1713\n",
            "2021-02-01 14:01:13,859 - INFO - joeynmt.training - Epoch 1713, Step:    25600, Batch Loss:     0.570257, Tokens per Sec:    18539, Lr: 0.000050\n",
            "2021-02-01 14:01:17,977 - INFO - joeynmt.training - Epoch 1713: total training loss 8.79\n",
            "2021-02-01 14:01:17,977 - INFO - joeynmt.training - EPOCH 1714\n",
            "2021-02-01 14:01:30,237 - INFO - joeynmt.training - Epoch 1714: total training loss 8.82\n",
            "2021-02-01 14:01:30,238 - INFO - joeynmt.training - EPOCH 1715\n",
            "2021-02-01 14:01:42,522 - INFO - joeynmt.training - Epoch 1715: total training loss 8.83\n",
            "2021-02-01 14:01:42,522 - INFO - joeynmt.training - EPOCH 1716\n",
            "2021-02-01 14:01:54,900 - INFO - joeynmt.training - Epoch 1716: total training loss 8.78\n",
            "2021-02-01 14:01:54,900 - INFO - joeynmt.training - EPOCH 1717\n",
            "2021-02-01 14:02:07,144 - INFO - joeynmt.training - Epoch 1717: total training loss 8.79\n",
            "2021-02-01 14:02:07,144 - INFO - joeynmt.training - EPOCH 1718\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:02:19,534 - INFO - joeynmt.training - Epoch 1718: total training loss 8.79\n",
            "2021-02-01 14:02:19,535 - INFO - joeynmt.training - EPOCH 1719\n",
            "2021-02-01 14:02:31,719 - INFO - joeynmt.training - Epoch 1719: total training loss 8.79\n",
            "2021-02-01 14:02:31,719 - INFO - joeynmt.training - EPOCH 1720\n",
            "2021-02-01 14:02:35,797 - INFO - joeynmt.training - Epoch 1720, Step:    25700, Batch Loss:     0.612310, Tokens per Sec:    20041, Lr: 0.000050\n",
            "2021-02-01 14:02:43,833 - INFO - joeynmt.training - Epoch 1720: total training loss 8.80\n",
            "2021-02-01 14:02:43,833 - INFO - joeynmt.training - EPOCH 1721\n",
            "2021-02-01 14:02:56,089 - INFO - joeynmt.training - Epoch 1721: total training loss 8.79\n",
            "2021-02-01 14:02:56,090 - INFO - joeynmt.training - EPOCH 1722\n",
            "2021-02-01 14:03:08,223 - INFO - joeynmt.training - Epoch 1722: total training loss 8.76\n",
            "2021-02-01 14:03:08,224 - INFO - joeynmt.training - EPOCH 1723\n",
            "2021-02-01 14:03:20,614 - INFO - joeynmt.training - Epoch 1723: total training loss 8.79\n",
            "2021-02-01 14:03:20,615 - INFO - joeynmt.training - EPOCH 1724\n",
            "2021-02-01 14:03:33,028 - INFO - joeynmt.training - Epoch 1724: total training loss 8.79\n",
            "2021-02-01 14:03:33,028 - INFO - joeynmt.training - EPOCH 1725\n",
            "2021-02-01 14:03:45,466 - INFO - joeynmt.training - Epoch 1725: total training loss 8.80\n",
            "2021-02-01 14:03:45,466 - INFO - joeynmt.training - EPOCH 1726\n",
            "2021-02-01 14:03:57,504 - INFO - joeynmt.training - Epoch 1726, Step:    25800, Batch Loss:     0.604642, Tokens per Sec:    18529, Lr: 0.000050\n",
            "2021-02-01 14:03:57,803 - INFO - joeynmt.training - Epoch 1726: total training loss 8.82\n",
            "2021-02-01 14:03:57,803 - INFO - joeynmt.training - EPOCH 1727\n",
            "2021-02-01 14:04:09,745 - INFO - joeynmt.training - Epoch 1727: total training loss 8.25\n",
            "2021-02-01 14:04:09,745 - INFO - joeynmt.training - EPOCH 1728\n",
            "2021-02-01 14:04:22,186 - INFO - joeynmt.training - Epoch 1728: total training loss 8.81\n",
            "2021-02-01 14:04:22,187 - INFO - joeynmt.training - EPOCH 1729\n",
            "2021-02-01 14:04:34,518 - INFO - joeynmt.training - Epoch 1729: total training loss 8.78\n",
            "2021-02-01 14:04:34,519 - INFO - joeynmt.training - EPOCH 1730\n",
            "2021-02-01 14:04:46,756 - INFO - joeynmt.training - Epoch 1730: total training loss 8.80\n",
            "2021-02-01 14:04:46,757 - INFO - joeynmt.training - EPOCH 1731\n",
            "2021-02-01 14:04:59,081 - INFO - joeynmt.training - Epoch 1731: total training loss 8.78\n",
            "2021-02-01 14:04:59,081 - INFO - joeynmt.training - EPOCH 1732\n",
            "2021-02-01 14:05:11,307 - INFO - joeynmt.training - Epoch 1732: total training loss 8.78\n",
            "2021-02-01 14:05:11,307 - INFO - joeynmt.training - EPOCH 1733\n",
            "2021-02-01 14:05:20,285 - INFO - joeynmt.training - Epoch 1733, Step:    25900, Batch Loss:     0.591316, Tokens per Sec:    18810, Lr: 0.000050\n",
            "2021-02-01 14:05:23,375 - INFO - joeynmt.training - Epoch 1733: total training loss 8.19\n",
            "2021-02-01 14:05:23,376 - INFO - joeynmt.training - EPOCH 1734\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:05:35,608 - INFO - joeynmt.training - Epoch 1734: total training loss 8.74\n",
            "2021-02-01 14:05:35,608 - INFO - joeynmt.training - EPOCH 1735\n",
            "2021-02-01 14:05:47,917 - INFO - joeynmt.training - Epoch 1735: total training loss 8.75\n",
            "2021-02-01 14:05:47,918 - INFO - joeynmt.training - EPOCH 1736\n",
            "2021-02-01 14:06:00,171 - INFO - joeynmt.training - Epoch 1736: total training loss 8.79\n",
            "2021-02-01 14:06:00,171 - INFO - joeynmt.training - EPOCH 1737\n",
            "2021-02-01 14:06:12,424 - INFO - joeynmt.training - Epoch 1737: total training loss 8.77\n",
            "2021-02-01 14:06:12,425 - INFO - joeynmt.training - EPOCH 1738\n",
            "2021-02-01 14:06:24,552 - INFO - joeynmt.training - Epoch 1738: total training loss 8.81\n",
            "2021-02-01 14:06:24,552 - INFO - joeynmt.training - EPOCH 1739\n",
            "2021-02-01 14:06:36,878 - INFO - joeynmt.training - Epoch 1739: total training loss 8.76\n",
            "2021-02-01 14:06:36,878 - INFO - joeynmt.training - EPOCH 1740\n",
            "2021-02-01 14:06:42,476 - INFO - joeynmt.training - Epoch 1740, Step:    26000, Batch Loss:     0.572584, Tokens per Sec:    18017, Lr: 0.000050\n",
            "2021-02-01 14:07:11,081 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:07:11,082 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:07:11,082 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісханаларға жіберіп отыратын сол жерде қаладағы мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны дайтты. Сол кісі өлтіргісі деп аңдыды.\n",
            "2021-02-01 14:07:11,083 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еттін, оны көрдім. Ол Тиянды тәнінде тұра билігі беріліп, Құдайды мадақтап жүрмедім. Оның есімі мен қатты ашты!\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:07:11,084 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алмақтас іспетті болады.※ Құдайдың оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:07:11,085 - INFO - joeynmt.training - Validation result (greedy) at epoch 1740, step    26000: bleu:   4.99, loss: 87514.6641, ppl:  77.8705, duration: 28.6080s\n",
            "2021-02-01 14:07:17,879 - INFO - joeynmt.training - Epoch 1740: total training loss 8.79\n",
            "2021-02-01 14:07:17,879 - INFO - joeynmt.training - EPOCH 1741\n",
            "2021-02-01 14:07:30,176 - INFO - joeynmt.training - Epoch 1741: total training loss 8.71\n",
            "2021-02-01 14:07:30,176 - INFO - joeynmt.training - EPOCH 1742\n",
            "2021-02-01 14:07:42,275 - INFO - joeynmt.training - Epoch 1742: total training loss 8.77\n",
            "2021-02-01 14:07:42,275 - INFO - joeynmt.training - EPOCH 1743\n",
            "2021-02-01 14:07:54,382 - INFO - joeynmt.training - Epoch 1743: total training loss 8.77\n",
            "2021-02-01 14:07:54,382 - INFO - joeynmt.training - EPOCH 1744\n",
            "2021-02-01 14:08:06,539 - INFO - joeynmt.training - Epoch 1744: total training loss 8.80\n",
            "2021-02-01 14:08:06,540 - INFO - joeynmt.training - EPOCH 1745\n",
            "2021-02-01 14:08:18,784 - INFO - joeynmt.training - Epoch 1745: total training loss 8.79\n",
            "2021-02-01 14:08:18,785 - INFO - joeynmt.training - EPOCH 1746\n",
            "2021-02-01 14:08:31,062 - INFO - joeynmt.training - Epoch 1746: total training loss 8.76\n",
            "2021-02-01 14:08:31,063 - INFO - joeynmt.training - EPOCH 1747\n",
            "2021-02-01 14:08:32,698 - INFO - joeynmt.training - Epoch 1747, Step:    26100, Batch Loss:     0.598769, Tokens per Sec:    18793, Lr: 0.000035\n",
            "2021-02-01 14:08:43,238 - INFO - joeynmt.training - Epoch 1747: total training loss 8.76\n",
            "2021-02-01 14:08:43,239 - INFO - joeynmt.training - EPOCH 1748\n",
            "2021-02-01 14:08:55,506 - INFO - joeynmt.training - Epoch 1748: total training loss 8.75\n",
            "2021-02-01 14:08:55,507 - INFO - joeynmt.training - EPOCH 1749\n",
            "2021-02-01 14:09:07,828 - INFO - joeynmt.training - Epoch 1749: total training loss 8.76\n",
            "2021-02-01 14:09:07,829 - INFO - joeynmt.training - EPOCH 1750\n",
            "2021-02-01 14:09:20,069 - INFO - joeynmt.training - Epoch 1750: total training loss 8.77\n",
            "2021-02-01 14:09:20,069 - INFO - joeynmt.training - EPOCH 1751\n",
            "2021-02-01 14:09:32,313 - INFO - joeynmt.training - Epoch 1751: total training loss 8.73\n",
            "2021-02-01 14:09:32,313 - INFO - joeynmt.training - EPOCH 1752\n",
            "2021-02-01 14:09:44,584 - INFO - joeynmt.training - Epoch 1752: total training loss 8.76\n",
            "2021-02-01 14:09:44,584 - INFO - joeynmt.training - EPOCH 1753\n",
            "2021-02-01 14:09:54,356 - INFO - joeynmt.training - Epoch 1753, Step:    26200, Batch Loss:     0.601919, Tokens per Sec:    18887, Lr: 0.000035\n",
            "2021-02-01 14:09:56,678 - INFO - joeynmt.training - Epoch 1753: total training loss 8.15\n",
            "2021-02-01 14:09:56,679 - INFO - joeynmt.training - EPOCH 1754\n",
            "2021-02-01 14:10:08,998 - INFO - joeynmt.training - Epoch 1754: total training loss 8.74\n",
            "2021-02-01 14:10:08,999 - INFO - joeynmt.training - EPOCH 1755\n",
            "2021-02-01 14:10:21,078 - INFO - joeynmt.training - Epoch 1755: total training loss 8.74\n",
            "2021-02-01 14:10:21,079 - INFO - joeynmt.training - EPOCH 1756\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:10:33,420 - INFO - joeynmt.training - Epoch 1756: total training loss 8.68\n",
            "2021-02-01 14:10:33,421 - INFO - joeynmt.training - EPOCH 1757\n",
            "2021-02-01 14:10:45,614 - INFO - joeynmt.training - Epoch 1757: total training loss 8.75\n",
            "2021-02-01 14:10:45,615 - INFO - joeynmt.training - EPOCH 1758\n",
            "2021-02-01 14:10:57,901 - INFO - joeynmt.training - Epoch 1758: total training loss 8.71\n",
            "2021-02-01 14:10:57,902 - INFO - joeynmt.training - EPOCH 1759\n",
            "2021-02-01 14:11:10,177 - INFO - joeynmt.training - Epoch 1759: total training loss 8.77\n",
            "2021-02-01 14:11:10,177 - INFO - joeynmt.training - EPOCH 1760\n",
            "2021-02-01 14:11:16,652 - INFO - joeynmt.training - Epoch 1760, Step:    26300, Batch Loss:     0.565831, Tokens per Sec:    18440, Lr: 0.000035\n",
            "2021-02-01 14:11:22,595 - INFO - joeynmt.training - Epoch 1760: total training loss 8.76\n",
            "2021-02-01 14:11:22,595 - INFO - joeynmt.training - EPOCH 1761\n",
            "2021-02-01 14:11:34,918 - INFO - joeynmt.training - Epoch 1761: total training loss 8.69\n",
            "2021-02-01 14:11:34,918 - INFO - joeynmt.training - EPOCH 1762\n",
            "2021-02-01 14:11:47,072 - INFO - joeynmt.training - Epoch 1762: total training loss 8.71\n",
            "2021-02-01 14:11:47,072 - INFO - joeynmt.training - EPOCH 1763\n",
            "2021-02-01 14:11:59,228 - INFO - joeynmt.training - Epoch 1763: total training loss 8.74\n",
            "2021-02-01 14:11:59,228 - INFO - joeynmt.training - EPOCH 1764\n",
            "2021-02-01 14:12:11,778 - INFO - joeynmt.training - Epoch 1764: total training loss 8.74\n",
            "2021-02-01 14:12:11,778 - INFO - joeynmt.training - EPOCH 1765\n",
            "2021-02-01 14:12:23,993 - INFO - joeynmt.training - Epoch 1765: total training loss 8.76\n",
            "2021-02-01 14:12:23,994 - INFO - joeynmt.training - EPOCH 1766\n",
            "2021-02-01 14:12:36,490 - INFO - joeynmt.training - Epoch 1766: total training loss 8.75\n",
            "2021-02-01 14:12:36,491 - INFO - joeynmt.training - EPOCH 1767\n",
            "2021-02-01 14:12:38,952 - INFO - joeynmt.training - Epoch 1767, Step:    26400, Batch Loss:     0.580461, Tokens per Sec:    19188, Lr: 0.000035\n",
            "2021-02-01 14:12:48,716 - INFO - joeynmt.training - Epoch 1767: total training loss 8.74\n",
            "2021-02-01 14:12:48,717 - INFO - joeynmt.training - EPOCH 1768\n",
            "2021-02-01 14:13:01,147 - INFO - joeynmt.training - Epoch 1768: total training loss 8.73\n",
            "2021-02-01 14:13:01,148 - INFO - joeynmt.training - EPOCH 1769\n",
            "2021-02-01 14:13:13,587 - INFO - joeynmt.training - Epoch 1769: total training loss 8.74\n",
            "2021-02-01 14:13:13,587 - INFO - joeynmt.training - EPOCH 1770\n",
            "2021-02-01 14:13:25,904 - INFO - joeynmt.training - Epoch 1770: total training loss 8.77\n",
            "2021-02-01 14:13:25,904 - INFO - joeynmt.training - EPOCH 1771\n",
            "2021-02-01 14:13:38,271 - INFO - joeynmt.training - Epoch 1771: total training loss 8.68\n",
            "2021-02-01 14:13:38,271 - INFO - joeynmt.training - EPOCH 1772\n",
            "2021-02-01 14:13:50,565 - INFO - joeynmt.training - Epoch 1772: total training loss 8.74\n",
            "2021-02-01 14:13:50,565 - INFO - joeynmt.training - EPOCH 1773\n",
            "2021-02-01 14:14:01,148 - INFO - joeynmt.training - Epoch 1773, Step:    26500, Batch Loss:     0.543104, Tokens per Sec:    18805, Lr: 0.000035\n",
            "2021-02-01 14:14:30,091 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:14:30,091 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:14:30,091 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбардың ережелерін бұзып, мәжілісхананың мәжілісханаларына жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 14:14:30,092 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, оны жерге бас тарту ретінде берілді. Тоқтының үстінде отырған Құдай осы дүниені көрмедім. Ол қатты ашылыс болып, Құдайды мадақтап күш етіп жүрмін деп айды.※\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:14:30,093 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:14:30,094 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермекендерің де Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 14:14:30,094 - INFO - joeynmt.training - Validation result (greedy) at epoch 1773, step    26500: bleu:   5.26, loss: 87684.0703, ppl:  78.5297, duration: 28.9454s\n",
            "2021-02-01 14:14:31,748 - INFO - joeynmt.training - Epoch 1773: total training loss 8.72\n",
            "2021-02-01 14:14:31,748 - INFO - joeynmt.training - EPOCH 1774\n",
            "2021-02-01 14:14:44,115 - INFO - joeynmt.training - Epoch 1774: total training loss 8.71\n",
            "2021-02-01 14:14:44,115 - INFO - joeynmt.training - EPOCH 1775\n",
            "2021-02-01 14:14:56,468 - INFO - joeynmt.training - Epoch 1775: total training loss 8.74\n",
            "2021-02-01 14:14:56,469 - INFO - joeynmt.training - EPOCH 1776\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:15:08,641 - INFO - joeynmt.training - Epoch 1776: total training loss 8.74\n",
            "2021-02-01 14:15:08,641 - INFO - joeynmt.training - EPOCH 1777\n",
            "2021-02-01 14:15:21,017 - INFO - joeynmt.training - Epoch 1777: total training loss 8.71\n",
            "2021-02-01 14:15:21,018 - INFO - joeynmt.training - EPOCH 1778\n",
            "2021-02-01 14:15:33,428 - INFO - joeynmt.training - Epoch 1778: total training loss 8.69\n",
            "2021-02-01 14:15:33,428 - INFO - joeynmt.training - EPOCH 1779\n",
            "2021-02-01 14:15:45,848 - INFO - joeynmt.training - Epoch 1779: total training loss 8.72\n",
            "2021-02-01 14:15:45,848 - INFO - joeynmt.training - EPOCH 1780\n",
            "2021-02-01 14:15:52,302 - INFO - joeynmt.training - Epoch 1780, Step:    26600, Batch Loss:     0.584374, Tokens per Sec:    18870, Lr: 0.000035\n",
            "2021-02-01 14:15:58,142 - INFO - joeynmt.training - Epoch 1780: total training loss 8.72\n",
            "2021-02-01 14:15:58,142 - INFO - joeynmt.training - EPOCH 1781\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 14:16:10,418 - INFO - joeynmt.training - Epoch 1781: total training loss 8.70\n",
            "2021-02-01 14:16:10,418 - INFO - joeynmt.training - EPOCH 1782\n",
            "2021-02-01 14:16:22,635 - INFO - joeynmt.training - Epoch 1782: total training loss 8.70\n",
            "2021-02-01 14:16:22,635 - INFO - joeynmt.training - EPOCH 1783\n",
            "2021-02-01 14:16:34,976 - INFO - joeynmt.training - Epoch 1783: total training loss 8.74\n",
            "2021-02-01 14:16:34,976 - INFO - joeynmt.training - EPOCH 1784\n",
            "2021-02-01 14:16:47,392 - INFO - joeynmt.training - Epoch 1784: total training loss 8.69\n",
            "2021-02-01 14:16:47,392 - INFO - joeynmt.training - EPOCH 1785\n",
            "2021-02-01 14:16:59,554 - INFO - joeynmt.training - Epoch 1785: total training loss 8.72\n",
            "2021-02-01 14:16:59,554 - INFO - joeynmt.training - EPOCH 1786\n",
            "2021-02-01 14:17:11,753 - INFO - joeynmt.training - Epoch 1786: total training loss 8.74\n",
            "2021-02-01 14:17:11,754 - INFO - joeynmt.training - EPOCH 1787\n",
            "2021-02-01 14:17:14,245 - INFO - joeynmt.training - Epoch 1787, Step:    26700, Batch Loss:     0.583757, Tokens per Sec:    18863, Lr: 0.000035\n",
            "2021-02-01 14:17:24,176 - INFO - joeynmt.training - Epoch 1787: total training loss 8.73\n",
            "2021-02-01 14:17:24,177 - INFO - joeynmt.training - EPOCH 1788\n",
            "2021-02-01 14:17:36,445 - INFO - joeynmt.training - Epoch 1788: total training loss 8.71\n",
            "2021-02-01 14:17:36,445 - INFO - joeynmt.training - EPOCH 1789\n",
            "2021-02-01 14:17:48,707 - INFO - joeynmt.training - Epoch 1789: total training loss 8.68\n",
            "2021-02-01 14:17:48,708 - INFO - joeynmt.training - EPOCH 1790\n",
            "2021-02-01 14:18:00,972 - INFO - joeynmt.training - Epoch 1790: total training loss 8.72\n",
            "2021-02-01 14:18:00,972 - INFO - joeynmt.training - EPOCH 1791\n",
            "2021-02-01 14:18:13,286 - INFO - joeynmt.training - Epoch 1791: total training loss 8.70\n",
            "2021-02-01 14:18:13,287 - INFO - joeynmt.training - EPOCH 1792\n",
            "2021-02-01 14:18:25,588 - INFO - joeynmt.training - Epoch 1792: total training loss 8.72\n",
            "2021-02-01 14:18:25,588 - INFO - joeynmt.training - EPOCH 1793\n",
            "2021-02-01 14:18:36,125 - INFO - joeynmt.training - Epoch 1793, Step:    26800, Batch Loss:     0.562237, Tokens per Sec:    18506, Lr: 0.000035\n",
            "2021-02-01 14:18:37,852 - INFO - joeynmt.training - Epoch 1793: total training loss 8.69\n",
            "2021-02-01 14:18:37,852 - INFO - joeynmt.training - EPOCH 1794\n",
            "2021-02-01 14:18:50,092 - INFO - joeynmt.training - Epoch 1794: total training loss 8.71\n",
            "2021-02-01 14:18:50,093 - INFO - joeynmt.training - EPOCH 1795\n",
            "2021-02-01 14:19:02,377 - INFO - joeynmt.training - Epoch 1795: total training loss 8.71\n",
            "2021-02-01 14:19:02,377 - INFO - joeynmt.training - EPOCH 1796\n",
            "2021-02-01 14:19:14,634 - INFO - joeynmt.training - Epoch 1796: total training loss 8.66\n",
            "2021-02-01 14:19:14,635 - INFO - joeynmt.training - EPOCH 1797\n",
            "2021-02-01 14:19:26,955 - INFO - joeynmt.training - Epoch 1797: total training loss 8.72\n",
            "2021-02-01 14:19:26,955 - INFO - joeynmt.training - EPOCH 1798\n",
            "2021-02-01 14:19:39,154 - INFO - joeynmt.training - Epoch 1798: total training loss 8.73\n",
            "2021-02-01 14:19:39,155 - INFO - joeynmt.training - EPOCH 1799\n",
            "2021-02-01 14:19:51,454 - INFO - joeynmt.training - Epoch 1799: total training loss 8.66\n",
            "2021-02-01 14:19:51,455 - INFO - joeynmt.training - EPOCH 1800\n",
            "2021-02-01 14:19:57,977 - INFO - joeynmt.training - Epoch 1800, Step:    26900, Batch Loss:     0.592156, Tokens per Sec:    19049, Lr: 0.000035\n",
            "2021-02-01 14:20:03,726 - INFO - joeynmt.training - Epoch 1800: total training loss 8.70\n",
            "2021-02-01 14:20:03,727 - INFO - joeynmt.training - EPOCH 1801\n",
            "2021-02-01 14:20:16,106 - INFO - joeynmt.training - Epoch 1801: total training loss 8.70\n",
            "2021-02-01 14:20:16,107 - INFO - joeynmt.training - EPOCH 1802\n",
            "2021-02-01 14:20:28,409 - INFO - joeynmt.training - Epoch 1802: total training loss 8.71\n",
            "2021-02-01 14:20:28,409 - INFO - joeynmt.training - EPOCH 1803\n",
            "2021-02-01 14:20:40,756 - INFO - joeynmt.training - Epoch 1803: total training loss 8.73\n",
            "2021-02-01 14:20:40,756 - INFO - joeynmt.training - EPOCH 1804\n",
            "2021-02-01 14:20:53,047 - INFO - joeynmt.training - Epoch 1804: total training loss 8.75\n",
            "2021-02-01 14:20:53,047 - INFO - joeynmt.training - EPOCH 1805\n",
            "2021-02-01 14:21:05,123 - INFO - joeynmt.training - Epoch 1805: total training loss 8.13\n",
            "2021-02-01 14:21:05,123 - INFO - joeynmt.training - EPOCH 1806\n",
            "2021-02-01 14:21:17,399 - INFO - joeynmt.training - Epoch 1806: total training loss 8.70\n",
            "2021-02-01 14:21:17,400 - INFO - joeynmt.training - EPOCH 1807\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 14:21:20,600 - INFO - joeynmt.training - Epoch 1807, Step:    27000, Batch Loss:     0.604691, Tokens per Sec:    18483, Lr: 0.000035\n",
            "2021-02-01 14:21:48,923 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:21:48,924 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:21:48,924 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:21:48,924 - INFO - joeynmt.training - \tHypothesis: Себебі Таурат заңында Мұса арқылы берілген Таурат заңы мен ғауыр мәжілісханаларға жіберіп отырғанда, ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі жас есектен ұстап беру үшін адамдар да мәйітін қабірге қоюға батыс етті.\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:21:48,925 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам сол рет жақсы көрдім. Ол Тоқтының үстінде отырған Құдай Тоқтыр көрме! Жалт дүниеңді терең деп тапты; оның неші мен Құдайды мадақтап жілейтін болдым.\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алмақтас іспетті Бас діни қызметкердің Иесіне сай орналасқан болар еді.\n",
            "2021-02-01 14:21:48,926 - INFO - joeynmt.training - Validation result (greedy) at epoch 1807, step    27000: bleu:   5.18, loss: 87557.8828, ppl:  78.0381, duration: 28.3254s\n",
            "2021-02-01 14:21:58,006 - INFO - joeynmt.training - Epoch 1807: total training loss 8.71\n",
            "2021-02-01 14:21:58,007 - INFO - joeynmt.training - EPOCH 1808\n",
            "2021-02-01 14:22:10,463 - INFO - joeynmt.training - Epoch 1808: total training loss 8.71\n",
            "2021-02-01 14:22:10,464 - INFO - joeynmt.training - EPOCH 1809\n",
            "2021-02-01 14:22:22,819 - INFO - joeynmt.training - Epoch 1809: total training loss 8.68\n",
            "2021-02-01 14:22:22,819 - INFO - joeynmt.training - EPOCH 1810\n",
            "2021-02-01 14:22:35,214 - INFO - joeynmt.training - Epoch 1810: total training loss 8.68\n",
            "2021-02-01 14:22:35,214 - INFO - joeynmt.training - EPOCH 1811\n",
            "2021-02-01 14:22:47,612 - INFO - joeynmt.training - Epoch 1811: total training loss 8.71\n",
            "2021-02-01 14:22:47,612 - INFO - joeynmt.training - EPOCH 1812\n",
            "2021-02-01 14:22:59,907 - INFO - joeynmt.training - Epoch 1812: total training loss 8.72\n",
            "2021-02-01 14:22:59,907 - INFO - joeynmt.training - EPOCH 1813\n",
            "2021-02-01 14:23:11,256 - INFO - joeynmt.training - Epoch 1813, Step:    27100, Batch Loss:     0.606569, Tokens per Sec:    18688, Lr: 0.000035\n",
            "2021-02-01 14:23:12,341 - INFO - joeynmt.training - Epoch 1813: total training loss 8.75\n",
            "2021-02-01 14:23:12,341 - INFO - joeynmt.training - EPOCH 1814\n",
            "2021-02-01 14:23:24,616 - INFO - joeynmt.training - Epoch 1814: total training loss 8.70\n",
            "2021-02-01 14:23:24,616 - INFO - joeynmt.training - EPOCH 1815\n",
            "2021-02-01 14:23:36,988 - INFO - joeynmt.training - Epoch 1815: total training loss 8.69\n",
            "2021-02-01 14:23:36,988 - INFO - joeynmt.training - EPOCH 1816\n",
            "2021-02-01 14:23:49,400 - INFO - joeynmt.training - Epoch 1816: total training loss 8.69\n",
            "2021-02-01 14:23:49,401 - INFO - joeynmt.training - EPOCH 1817\n",
            "2021-02-01 14:24:01,718 - INFO - joeynmt.training - Epoch 1817: total training loss 8.68\n",
            "2021-02-01 14:24:01,718 - INFO - joeynmt.training - EPOCH 1818\n",
            "2021-02-01 14:24:13,931 - INFO - joeynmt.training - Epoch 1818: total training loss 8.72\n",
            "2021-02-01 14:24:13,931 - INFO - joeynmt.training - EPOCH 1819\n",
            "2021-02-01 14:24:26,271 - INFO - joeynmt.training - Epoch 1819: total training loss 8.69\n",
            "2021-02-01 14:24:26,272 - INFO - joeynmt.training - EPOCH 1820\n",
            "2021-02-01 14:24:33,644 - INFO - joeynmt.training - Epoch 1820, Step:    27200, Batch Loss:     0.572636, Tokens per Sec:    18688, Lr: 0.000035\n",
            "2021-02-01 14:24:38,565 - INFO - joeynmt.training - Epoch 1820: total training loss 8.73\n",
            "2021-02-01 14:24:38,566 - INFO - joeynmt.training - EPOCH 1821\n",
            "2021-02-01 14:24:50,906 - INFO - joeynmt.training - Epoch 1821: total training loss 8.73\n",
            "2021-02-01 14:24:50,907 - INFO - joeynmt.training - EPOCH 1822\n",
            "2021-02-01 14:25:03,322 - INFO - joeynmt.training - Epoch 1822: total training loss 8.64\n",
            "2021-02-01 14:25:03,323 - INFO - joeynmt.training - EPOCH 1823\n",
            "2021-02-01 14:25:15,668 - INFO - joeynmt.training - Epoch 1823: total training loss 8.64\n",
            "2021-02-01 14:25:15,669 - INFO - joeynmt.training - EPOCH 1824\n",
            "2021-02-01 14:25:27,944 - INFO - joeynmt.training - Epoch 1824: total training loss 8.65\n",
            "2021-02-01 14:25:27,944 - INFO - joeynmt.training - EPOCH 1825\n",
            "2021-02-01 14:25:40,304 - INFO - joeynmt.training - Epoch 1825: total training loss 8.71\n",
            "2021-02-01 14:25:40,305 - INFO - joeynmt.training - EPOCH 1826\n",
            "2021-02-01 14:25:52,567 - INFO - joeynmt.training - Epoch 1826: total training loss 8.66\n",
            "2021-02-01 14:25:52,568 - INFO - joeynmt.training - EPOCH 1827\n",
            "2021-02-01 14:25:55,848 - INFO - joeynmt.training - Epoch 1827, Step:    27300, Batch Loss:     0.598036, Tokens per Sec:    18472, Lr: 0.000035\n",
            "2021-02-01 14:26:04,942 - INFO - joeynmt.training - Epoch 1827: total training loss 8.69\n",
            "2021-02-01 14:26:04,942 - INFO - joeynmt.training - EPOCH 1828\n",
            "2021-02-01 14:26:17,588 - INFO - joeynmt.training - Epoch 1828: total training loss 8.67\n",
            "2021-02-01 14:26:17,589 - INFO - joeynmt.training - EPOCH 1829\n",
            "2021-02-01 14:26:30,383 - INFO - joeynmt.training - Epoch 1829: total training loss 8.66\n",
            "2021-02-01 14:26:30,383 - INFO - joeynmt.training - EPOCH 1830\n",
            "2021-02-01 14:26:42,928 - INFO - joeynmt.training - Epoch 1830: total training loss 8.68\n",
            "2021-02-01 14:26:42,929 - INFO - joeynmt.training - EPOCH 1831\n",
            "2021-02-01 14:26:55,621 - INFO - joeynmt.training - Epoch 1831: total training loss 8.66\n",
            "2021-02-01 14:26:55,622 - INFO - joeynmt.training - EPOCH 1832\n",
            "2021-02-01 14:27:08,100 - INFO - joeynmt.training - Epoch 1832: total training loss 8.69\n",
            "2021-02-01 14:27:08,100 - INFO - joeynmt.training - EPOCH 1833\n",
            "2021-02-01 14:27:19,627 - INFO - joeynmt.training - Epoch 1833, Step:    27400, Batch Loss:     0.562118, Tokens per Sec:    18083, Lr: 0.000035\n",
            "2021-02-01 14:27:20,673 - INFO - joeynmt.training - Epoch 1833: total training loss 8.65\n",
            "2021-02-01 14:27:20,674 - INFO - joeynmt.training - EPOCH 1834\n",
            "2021-02-01 14:27:33,173 - INFO - joeynmt.training - Epoch 1834: total training loss 8.70\n",
            "2021-02-01 14:27:33,174 - INFO - joeynmt.training - EPOCH 1835\n",
            "2021-02-01 14:27:45,759 - INFO - joeynmt.training - Epoch 1835: total training loss 8.71\n",
            "2021-02-01 14:27:45,760 - INFO - joeynmt.training - EPOCH 1836\n",
            "2021-02-01 14:27:58,236 - INFO - joeynmt.training - Epoch 1836: total training loss 8.67\n",
            "2021-02-01 14:27:58,237 - INFO - joeynmt.training - EPOCH 1837\n",
            "2021-02-01 14:28:10,887 - INFO - joeynmt.training - Epoch 1837: total training loss 8.67\n",
            "2021-02-01 14:28:10,887 - INFO - joeynmt.training - EPOCH 1838\n",
            "2021-02-01 14:28:23,252 - INFO - joeynmt.training - Epoch 1838: total training loss 8.67\n",
            "2021-02-01 14:28:23,252 - INFO - joeynmt.training - EPOCH 1839\n",
            "2021-02-01 14:28:35,944 - INFO - joeynmt.training - Epoch 1839: total training loss 8.64\n",
            "2021-02-01 14:28:35,945 - INFO - joeynmt.training - EPOCH 1840\n",
            "2021-02-01 14:28:43,468 - INFO - joeynmt.training - Epoch 1840, Step:    27500, Batch Loss:     0.600854, Tokens per Sec:    18731, Lr: 0.000035\n",
            "2021-02-01 14:29:12,078 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:29:12,078 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - \tHypothesis: Себебі Таурат заңында Мұса арқылы берілген Таурат заңы мен ғауыр мәжілісханаларға жіберіп отырғанда, ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 14:29:12,079 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, оны жерге бас тарту ретінде берілді. Тоқтының үстінде отырған Құдай осы дүниені көрмедім. Ол қатты ашылықты Құдайды мадақтап күткен, айуанның есімі мен Құдайды мадақтап тұрды.※\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:29:12,080 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алдыңдар.※ Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:29:12,081 - INFO - joeynmt.training - Validation result (greedy) at epoch 1840, step    27500: bleu:   5.19, loss: 87611.2969, ppl:  78.2458, duration: 28.6124s\n",
            "2021-02-01 14:29:17,161 - INFO - joeynmt.training - Epoch 1840: total training loss 8.67\n",
            "2021-02-01 14:29:17,162 - INFO - joeynmt.training - EPOCH 1841\n",
            "2021-02-01 14:29:29,460 - INFO - joeynmt.training - Epoch 1841: total training loss 8.12\n",
            "2021-02-01 14:29:29,461 - INFO - joeynmt.training - EPOCH 1842\n",
            "2021-02-01 14:29:41,891 - INFO - joeynmt.training - Epoch 1842: total training loss 8.70\n",
            "2021-02-01 14:29:41,892 - INFO - joeynmt.training - EPOCH 1843\n",
            "2021-02-01 14:29:54,570 - INFO - joeynmt.training - Epoch 1843: total training loss 8.66\n",
            "2021-02-01 14:29:54,570 - INFO - joeynmt.training - EPOCH 1844\n",
            "2021-02-01 14:30:07,146 - INFO - joeynmt.training - Epoch 1844: total training loss 8.68\n",
            "2021-02-01 14:30:07,147 - INFO - joeynmt.training - EPOCH 1845\n",
            "2021-02-01 14:30:19,716 - INFO - joeynmt.training - Epoch 1845: total training loss 8.66\n",
            "2021-02-01 14:30:19,716 - INFO - joeynmt.training - EPOCH 1846\n",
            "2021-02-01 14:30:32,202 - INFO - joeynmt.training - Epoch 1846: total training loss 8.68\n",
            "2021-02-01 14:30:32,202 - INFO - joeynmt.training - EPOCH 1847\n",
            "2021-02-01 14:30:36,413 - INFO - joeynmt.training - Epoch 1847, Step:    27600, Batch Loss:     0.597659, Tokens per Sec:    18470, Lr: 0.000035\n",
            "2021-02-01 14:30:44,540 - INFO - joeynmt.training - Epoch 1847: total training loss 8.09\n",
            "2021-02-01 14:30:44,540 - INFO - joeynmt.training - EPOCH 1848\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:30:57,191 - INFO - joeynmt.training - Epoch 1848: total training loss 8.65\n",
            "2021-02-01 14:30:57,192 - INFO - joeynmt.training - EPOCH 1849\n",
            "2021-02-01 14:31:09,697 - INFO - joeynmt.training - Epoch 1849: total training loss 8.66\n",
            "2021-02-01 14:31:09,698 - INFO - joeynmt.training - EPOCH 1850\n",
            "2021-02-01 14:31:22,267 - INFO - joeynmt.training - Epoch 1850: total training loss 8.66\n",
            "2021-02-01 14:31:22,268 - INFO - joeynmt.training - EPOCH 1851\n",
            "2021-02-01 14:31:34,888 - INFO - joeynmt.training - Epoch 1851: total training loss 8.64\n",
            "2021-02-01 14:31:34,888 - INFO - joeynmt.training - EPOCH 1852\n",
            "2021-02-01 14:31:47,455 - INFO - joeynmt.training - Epoch 1852: total training loss 8.66\n",
            "2021-02-01 14:31:47,455 - INFO - joeynmt.training - EPOCH 1853\n",
            "2021-02-01 14:32:00,038 - INFO - joeynmt.training - Epoch 1853: total training loss 8.67\n",
            "2021-02-01 14:32:00,038 - INFO - joeynmt.training - EPOCH 1854\n",
            "2021-02-01 14:32:00,878 - INFO - joeynmt.training - Epoch 1854, Step:    27700, Batch Loss:     0.592631, Tokens per Sec:    19325, Lr: 0.000035\n",
            "2021-02-01 14:32:12,545 - INFO - joeynmt.training - Epoch 1854: total training loss 8.68\n",
            "2021-02-01 14:32:12,545 - INFO - joeynmt.training - EPOCH 1855\n",
            "2021-02-01 14:32:24,971 - INFO - joeynmt.training - Epoch 1855: total training loss 8.66\n",
            "2021-02-01 14:32:24,971 - INFO - joeynmt.training - EPOCH 1856\n",
            "2021-02-01 14:32:37,693 - INFO - joeynmt.training - Epoch 1856: total training loss 8.66\n",
            "2021-02-01 14:32:37,693 - INFO - joeynmt.training - EPOCH 1857\n",
            "2021-02-01 14:32:50,034 - INFO - joeynmt.training - Epoch 1857: total training loss 8.04\n",
            "2021-02-01 14:32:50,034 - INFO - joeynmt.training - EPOCH 1858\n",
            "2021-02-01 14:33:02,573 - INFO - joeynmt.training - Epoch 1858: total training loss 8.66\n",
            "2021-02-01 14:33:02,573 - INFO - joeynmt.training - EPOCH 1859\n",
            "2021-02-01 14:33:15,276 - INFO - joeynmt.training - Epoch 1859: total training loss 8.68\n",
            "2021-02-01 14:33:15,277 - INFO - joeynmt.training - EPOCH 1860\n",
            "2021-02-01 14:33:25,231 - INFO - joeynmt.training - Epoch 1860, Step:    27800, Batch Loss:     0.556036, Tokens per Sec:    18741, Lr: 0.000035\n",
            "2021-02-01 14:33:27,673 - INFO - joeynmt.training - Epoch 1860: total training loss 8.68\n",
            "2021-02-01 14:33:27,673 - INFO - joeynmt.training - EPOCH 1861\n",
            "2021-02-01 14:33:40,210 - INFO - joeynmt.training - Epoch 1861: total training loss 8.67\n",
            "2021-02-01 14:33:40,210 - INFO - joeynmt.training - EPOCH 1862\n",
            "2021-02-01 14:33:52,631 - INFO - joeynmt.training - Epoch 1862: total training loss 8.66\n",
            "2021-02-01 14:33:52,631 - INFO - joeynmt.training - EPOCH 1863\n",
            "2021-02-01 14:34:05,141 - INFO - joeynmt.training - Epoch 1863: total training loss 8.67\n",
            "2021-02-01 14:34:05,141 - INFO - joeynmt.training - EPOCH 1864\n",
            "2021-02-01 14:34:17,799 - INFO - joeynmt.training - Epoch 1864: total training loss 8.66\n",
            "2021-02-01 14:34:17,800 - INFO - joeynmt.training - EPOCH 1865\n",
            "2021-02-01 14:34:30,329 - INFO - joeynmt.training - Epoch 1865: total training loss 8.65\n",
            "2021-02-01 14:34:30,329 - INFO - joeynmt.training - EPOCH 1866\n",
            "2021-02-01 14:34:43,048 - INFO - joeynmt.training - Epoch 1866: total training loss 8.66\n",
            "2021-02-01 14:34:43,048 - INFO - joeynmt.training - EPOCH 1867\n",
            "2021-02-01 14:34:48,868 - INFO - joeynmt.training - Epoch 1867, Step:    27900, Batch Loss:     0.607798, Tokens per Sec:    18408, Lr: 0.000035\n",
            "2021-02-01 14:34:55,565 - INFO - joeynmt.training - Epoch 1867: total training loss 8.68\n",
            "2021-02-01 14:34:55,565 - INFO - joeynmt.training - EPOCH 1868\n",
            "2021-02-01 14:35:08,046 - INFO - joeynmt.training - Epoch 1868: total training loss 8.66\n",
            "2021-02-01 14:35:08,047 - INFO - joeynmt.training - EPOCH 1869\n",
            "2021-02-01 14:35:20,584 - INFO - joeynmt.training - Epoch 1869: total training loss 8.69\n",
            "2021-02-01 14:35:20,584 - INFO - joeynmt.training - EPOCH 1870\n",
            "2021-02-01 14:35:33,127 - INFO - joeynmt.training - Epoch 1870: total training loss 8.68\n",
            "2021-02-01 14:35:33,127 - INFO - joeynmt.training - EPOCH 1871\n",
            "2021-02-01 14:35:45,748 - INFO - joeynmt.training - Epoch 1871: total training loss 8.61\n",
            "2021-02-01 14:35:45,749 - INFO - joeynmt.training - EPOCH 1872\n",
            "2021-02-01 14:35:58,346 - INFO - joeynmt.training - Epoch 1872: total training loss 8.64\n",
            "2021-02-01 14:35:58,347 - INFO - joeynmt.training - EPOCH 1873\n",
            "2021-02-01 14:36:10,683 - INFO - joeynmt.training - Epoch 1873: total training loss 8.64\n",
            "2021-02-01 14:36:10,683 - INFO - joeynmt.training - EPOCH 1874\n",
            "2021-02-01 14:36:12,368 - INFO - joeynmt.training - Epoch 1874, Step:    28000, Batch Loss:     0.592039, Tokens per Sec:    18374, Lr: 0.000035\n",
            "2021-02-01 14:36:43,393 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:36:43,394 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:36:43,394 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:36:43,395 - INFO - joeynmt.training - \tHypothesis: Себебі Таурат заңында Мұса арқылы берілген Таурат заңы мен ғауыр мәжілісханаларға жіберіп отырғанда, ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:36:43,395 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:36:43,395 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:36:43,395 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:36:43,396 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны дайтты.\n",
            "2021-02-01 14:36:43,396 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:36:43,396 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:36:43,396 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:36:43,397 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, оны жерге бас тартылып жібердім. Ол Тоқтының үстінде отырған Құдай осы дүниені көрме! ! Құдайды қастерлейтін әйелдің қатты ашылып: «шайтанды тәжін» деп (яғни Құдайды мадақтап тұрды.※\n",
            "2021-02-01 14:36:43,397 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:36:43,397 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:36:43,397 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:36:43,397 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алдыңдар.※ Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:36:43,398 - INFO - joeynmt.training - Validation result (greedy) at epoch 1874, step    28000: bleu:   5.13, loss: 87590.6016, ppl:  78.1653, duration: 31.0290s\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:36:54,412 - INFO - joeynmt.training - Epoch 1874: total training loss 8.64\n",
            "2021-02-01 14:36:54,412 - INFO - joeynmt.training - EPOCH 1875\n",
            "2021-02-01 14:37:06,964 - INFO - joeynmt.training - Epoch 1875: total training loss 8.65\n",
            "2021-02-01 14:37:06,964 - INFO - joeynmt.training - EPOCH 1876\n",
            "2021-02-01 14:37:19,529 - INFO - joeynmt.training - Epoch 1876: total training loss 8.65\n",
            "2021-02-01 14:37:19,530 - INFO - joeynmt.training - EPOCH 1877\n",
            "2021-02-01 14:37:31,852 - INFO - joeynmt.training - Epoch 1877: total training loss 8.06\n",
            "2021-02-01 14:37:31,852 - INFO - joeynmt.training - EPOCH 1878\n",
            "2021-02-01 14:37:44,335 - INFO - joeynmt.training - Epoch 1878: total training loss 8.65\n",
            "2021-02-01 14:37:44,336 - INFO - joeynmt.training - EPOCH 1879\n",
            "2021-02-01 14:37:56,744 - INFO - joeynmt.training - Epoch 1879: total training loss 8.64\n",
            "2021-02-01 14:37:56,744 - INFO - joeynmt.training - EPOCH 1880\n",
            "2021-02-01 14:38:07,633 - INFO - joeynmt.training - Epoch 1880, Step:    28100, Batch Loss:     0.590335, Tokens per Sec:    18284, Lr: 0.000035\n",
            "2021-02-01 14:38:09,394 - INFO - joeynmt.training - Epoch 1880: total training loss 8.67\n",
            "2021-02-01 14:38:09,395 - INFO - joeynmt.training - EPOCH 1881\n",
            "2021-02-01 14:38:21,837 - INFO - joeynmt.training - Epoch 1881: total training loss 8.05\n",
            "2021-02-01 14:38:21,837 - INFO - joeynmt.training - EPOCH 1882\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 14:38:34,602 - INFO - joeynmt.training - Epoch 1882: total training loss 8.59\n",
            "2021-02-01 14:38:34,603 - INFO - joeynmt.training - EPOCH 1883\n",
            "2021-02-01 14:38:47,050 - INFO - joeynmt.training - Epoch 1883: total training loss 8.64\n",
            "2021-02-01 14:38:47,050 - INFO - joeynmt.training - EPOCH 1884\n",
            "2021-02-01 14:38:59,543 - INFO - joeynmt.training - Epoch 1884: total training loss 8.64\n",
            "2021-02-01 14:38:59,543 - INFO - joeynmt.training - EPOCH 1885\n",
            "2021-02-01 14:39:11,932 - INFO - joeynmt.training - Epoch 1885: total training loss 8.66\n",
            "2021-02-01 14:39:11,933 - INFO - joeynmt.training - EPOCH 1886\n",
            "2021-02-01 14:39:24,364 - INFO - joeynmt.training - Epoch 1886: total training loss 8.65\n",
            "2021-02-01 14:39:24,365 - INFO - joeynmt.training - EPOCH 1887\n",
            "2021-02-01 14:39:31,757 - INFO - joeynmt.training - Epoch 1887, Step:    28200, Batch Loss:     0.536641, Tokens per Sec:    17556, Lr: 0.000035\n",
            "2021-02-01 14:39:37,030 - INFO - joeynmt.training - Epoch 1887: total training loss 8.63\n",
            "2021-02-01 14:39:37,031 - INFO - joeynmt.training - EPOCH 1888\n",
            "2021-02-01 14:39:49,620 - INFO - joeynmt.training - Epoch 1888: total training loss 8.62\n",
            "2021-02-01 14:39:49,621 - INFO - joeynmt.training - EPOCH 1889\n",
            "2021-02-01 14:40:02,145 - INFO - joeynmt.training - Epoch 1889: total training loss 8.61\n",
            "2021-02-01 14:40:02,145 - INFO - joeynmt.training - EPOCH 1890\n",
            "2021-02-01 14:40:14,686 - INFO - joeynmt.training - Epoch 1890: total training loss 8.65\n",
            "2021-02-01 14:40:14,687 - INFO - joeynmt.training - EPOCH 1891\n",
            "2021-02-01 14:40:27,296 - INFO - joeynmt.training - Epoch 1891: total training loss 8.62\n",
            "2021-02-01 14:40:27,297 - INFO - joeynmt.training - EPOCH 1892\n",
            "2021-02-01 14:40:39,887 - INFO - joeynmt.training - Epoch 1892: total training loss 8.61\n",
            "2021-02-01 14:40:39,887 - INFO - joeynmt.training - EPOCH 1893\n",
            "2021-02-01 14:40:52,469 - INFO - joeynmt.training - Epoch 1893: total training loss 8.64\n",
            "2021-02-01 14:40:52,470 - INFO - joeynmt.training - EPOCH 1894\n",
            "2021-02-01 14:40:55,837 - INFO - joeynmt.training - Epoch 1894, Step:    28300, Batch Loss:     0.544520, Tokens per Sec:    18023, Lr: 0.000035\n",
            "2021-02-01 14:41:04,982 - INFO - joeynmt.training - Epoch 1894: total training loss 8.63\n",
            "2021-02-01 14:41:04,982 - INFO - joeynmt.training - EPOCH 1895\n",
            "2021-02-01 14:41:17,540 - INFO - joeynmt.training - Epoch 1895: total training loss 8.62\n",
            "2021-02-01 14:41:17,540 - INFO - joeynmt.training - EPOCH 1896\n",
            "2021-02-01 14:41:29,944 - INFO - joeynmt.training - Epoch 1896: total training loss 8.63\n",
            "2021-02-01 14:41:29,944 - INFO - joeynmt.training - EPOCH 1897\n",
            "2021-02-01 14:41:42,357 - INFO - joeynmt.training - Epoch 1897: total training loss 8.67\n",
            "2021-02-01 14:41:42,357 - INFO - joeynmt.training - EPOCH 1898\n",
            "2021-02-01 14:41:54,940 - INFO - joeynmt.training - Epoch 1898: total training loss 8.62\n",
            "2021-02-01 14:41:54,940 - INFO - joeynmt.training - EPOCH 1899\n",
            "2021-02-01 14:42:07,489 - INFO - joeynmt.training - Epoch 1899: total training loss 8.64\n",
            "2021-02-01 14:42:07,490 - INFO - joeynmt.training - EPOCH 1900\n",
            "2021-02-01 14:42:19,068 - INFO - joeynmt.training - Epoch 1900, Step:    28400, Batch Loss:     0.614456, Tokens per Sec:    18128, Lr: 0.000035\n",
            "2021-02-01 14:42:20,021 - INFO - joeynmt.training - Epoch 1900: total training loss 8.62\n",
            "2021-02-01 14:42:20,022 - INFO - joeynmt.training - EPOCH 1901\n",
            "2021-02-01 14:42:32,555 - INFO - joeynmt.training - Epoch 1901: total training loss 8.63\n",
            "2021-02-01 14:42:32,556 - INFO - joeynmt.training - EPOCH 1902\n",
            "2021-02-01 14:42:44,885 - INFO - joeynmt.training - Epoch 1902: total training loss 8.09\n",
            "2021-02-01 14:42:44,885 - INFO - joeynmt.training - EPOCH 1903\n",
            "2021-02-01 14:42:57,308 - INFO - joeynmt.training - Epoch 1903: total training loss 8.59\n",
            "2021-02-01 14:42:57,308 - INFO - joeynmt.training - EPOCH 1904\n",
            "2021-02-01 14:43:09,834 - INFO - joeynmt.training - Epoch 1904: total training loss 8.62\n",
            "2021-02-01 14:43:09,834 - INFO - joeynmt.training - EPOCH 1905\n",
            "2021-02-01 14:43:22,406 - INFO - joeynmt.training - Epoch 1905: total training loss 8.60\n",
            "2021-02-01 14:43:22,406 - INFO - joeynmt.training - EPOCH 1906\n",
            "2021-02-01 14:43:34,873 - INFO - joeynmt.training - Epoch 1906: total training loss 8.61\n",
            "2021-02-01 14:43:34,873 - INFO - joeynmt.training - EPOCH 1907\n",
            "2021-02-01 14:43:43,209 - INFO - joeynmt.training - Epoch 1907, Step:    28500, Batch Loss:     0.592212, Tokens per Sec:    18439, Lr: 0.000035\n",
            "2021-02-01 14:44:10,496 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:44:10,497 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:44:10,497 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:44:10,497 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 14:44:10,497 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:44:10,497 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, адам сылта келді. Тоқтының үстінде отырған Құдай көрмей қатты ашты,※ Құдайды мадақтап тілген әйелдің үстінде тұра қалып, қатты ашуландырды.\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:44:10,498 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:44:10,499 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:44:10,499 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне арналған рухани жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:44:10,499 - INFO - joeynmt.training - Validation result (greedy) at epoch 1907, step    28500: bleu:   5.15, loss: 87696.5312, ppl:  78.5784, duration: 27.2893s\n",
            "2021-02-01 14:44:14,695 - INFO - joeynmt.training - Epoch 1907: total training loss 8.59\n",
            "2021-02-01 14:44:14,695 - INFO - joeynmt.training - EPOCH 1908\n",
            "2021-02-01 14:44:27,096 - INFO - joeynmt.training - Epoch 1908: total training loss 8.62\n",
            "2021-02-01 14:44:27,096 - INFO - joeynmt.training - EPOCH 1909\n",
            "2021-02-01 14:44:39,425 - INFO - joeynmt.training - Epoch 1909: total training loss 8.04\n",
            "2021-02-01 14:44:39,426 - INFO - joeynmt.training - EPOCH 1910\n",
            "2021-02-01 14:44:51,974 - INFO - joeynmt.training - Epoch 1910: total training loss 8.61\n",
            "2021-02-01 14:44:51,975 - INFO - joeynmt.training - EPOCH 1911\n",
            "2021-02-01 14:45:04,565 - INFO - joeynmt.training - Epoch 1911: total training loss 8.62\n",
            "2021-02-01 14:45:04,565 - INFO - joeynmt.training - EPOCH 1912\n",
            "2021-02-01 14:45:17,139 - INFO - joeynmt.training - Epoch 1912: total training loss 8.61\n",
            "2021-02-01 14:45:17,140 - INFO - joeynmt.training - EPOCH 1913\n",
            "2021-02-01 14:45:29,547 - INFO - joeynmt.training - Epoch 1913: total training loss 8.61\n",
            "2021-02-01 14:45:29,547 - INFO - joeynmt.training - EPOCH 1914\n",
            "2021-02-01 14:45:34,553 - INFO - joeynmt.training - Epoch 1914, Step:    28600, Batch Loss:     0.608760, Tokens per Sec:    18124, Lr: 0.000035\n",
            "2021-02-01 14:45:42,090 - INFO - joeynmt.training - Epoch 1914: total training loss 8.62\n",
            "2021-02-01 14:45:42,091 - INFO - joeynmt.training - EPOCH 1915\n",
            "2021-02-01 14:45:54,595 - INFO - joeynmt.training - Epoch 1915: total training loss 8.65\n",
            "2021-02-01 14:45:54,596 - INFO - joeynmt.training - EPOCH 1916\n",
            "2021-02-01 14:46:07,112 - INFO - joeynmt.training - Epoch 1916: total training loss 8.61\n",
            "2021-02-01 14:46:07,112 - INFO - joeynmt.training - EPOCH 1917\n",
            "2021-02-01 14:46:19,721 - INFO - joeynmt.training - Epoch 1917: total training loss 8.56\n",
            "2021-02-01 14:46:19,721 - INFO - joeynmt.training - EPOCH 1918\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:46:32,443 - INFO - joeynmt.training - Epoch 1918: total training loss 8.58\n",
            "2021-02-01 14:46:32,444 - INFO - joeynmt.training - EPOCH 1919\n",
            "2021-02-01 14:46:44,991 - INFO - joeynmt.training - Epoch 1919: total training loss 8.63\n",
            "2021-02-01 14:46:44,992 - INFO - joeynmt.training - EPOCH 1920\n",
            "2021-02-01 14:46:57,688 - INFO - joeynmt.training - Epoch 1920: total training loss 8.62\n",
            "2021-02-01 14:46:57,688 - INFO - joeynmt.training - EPOCH 1921\n",
            "2021-02-01 14:46:58,527 - INFO - joeynmt.training - Epoch 1921, Step:    28700, Batch Loss:     0.534686, Tokens per Sec:    13864, Lr: 0.000035\n",
            "2021-02-01 14:47:10,190 - INFO - joeynmt.training - Epoch 1921: total training loss 8.62\n",
            "2021-02-01 14:47:10,191 - INFO - joeynmt.training - EPOCH 1922\n",
            "2021-02-01 14:47:22,812 - INFO - joeynmt.training - Epoch 1922: total training loss 8.61\n",
            "2021-02-01 14:47:22,813 - INFO - joeynmt.training - EPOCH 1923\n",
            "2021-02-01 14:47:35,335 - INFO - joeynmt.training - Epoch 1923: total training loss 8.60\n",
            "2021-02-01 14:47:35,335 - INFO - joeynmt.training - EPOCH 1924\n",
            "2021-02-01 14:47:47,933 - INFO - joeynmt.training - Epoch 1924: total training loss 8.63\n",
            "2021-02-01 14:47:47,933 - INFO - joeynmt.training - EPOCH 1925\n",
            "2021-02-01 14:48:00,524 - INFO - joeynmt.training - Epoch 1925: total training loss 8.60\n",
            "2021-02-01 14:48:00,525 - INFO - joeynmt.training - EPOCH 1926\n",
            "2021-02-01 14:48:13,262 - INFO - joeynmt.training - Epoch 1926: total training loss 8.62\n",
            "2021-02-01 14:48:13,262 - INFO - joeynmt.training - EPOCH 1927\n",
            "2021-02-01 14:48:22,443 - INFO - joeynmt.training - Epoch 1927, Step:    28800, Batch Loss:     0.552241, Tokens per Sec:    17804, Lr: 0.000035\n",
            "2021-02-01 14:48:25,998 - INFO - joeynmt.training - Epoch 1927: total training loss 8.62\n",
            "2021-02-01 14:48:25,999 - INFO - joeynmt.training - EPOCH 1928\n",
            "2021-02-01 14:48:38,656 - INFO - joeynmt.training - Epoch 1928: total training loss 8.60\n",
            "2021-02-01 14:48:38,657 - INFO - joeynmt.training - EPOCH 1929\n",
            "2021-02-01 14:48:51,157 - INFO - joeynmt.training - Epoch 1929: total training loss 8.61\n",
            "2021-02-01 14:48:51,157 - INFO - joeynmt.training - EPOCH 1930\n",
            "2021-02-01 14:49:03,781 - INFO - joeynmt.training - Epoch 1930: total training loss 8.61\n",
            "2021-02-01 14:49:03,782 - INFO - joeynmt.training - EPOCH 1931\n",
            "2021-02-01 14:49:16,288 - INFO - joeynmt.training - Epoch 1931: total training loss 8.61\n",
            "2021-02-01 14:49:16,288 - INFO - joeynmt.training - EPOCH 1932\n",
            "2021-02-01 14:49:28,808 - INFO - joeynmt.training - Epoch 1932: total training loss 8.58\n",
            "2021-02-01 14:49:28,809 - INFO - joeynmt.training - EPOCH 1933\n",
            "2021-02-01 14:49:41,345 - INFO - joeynmt.training - Epoch 1933: total training loss 8.58\n",
            "2021-02-01 14:49:41,345 - INFO - joeynmt.training - EPOCH 1934\n",
            "2021-02-01 14:49:46,276 - INFO - joeynmt.training - Epoch 1934, Step:    28900, Batch Loss:     0.563298, Tokens per Sec:    17585, Lr: 0.000035\n",
            "2021-02-01 14:49:53,825 - INFO - joeynmt.training - Epoch 1934: total training loss 8.62\n",
            "2021-02-01 14:49:53,825 - INFO - joeynmt.training - EPOCH 1935\n",
            "2021-02-01 14:50:06,411 - INFO - joeynmt.training - Epoch 1935: total training loss 8.59\n",
            "2021-02-01 14:50:06,412 - INFO - joeynmt.training - EPOCH 1936\n",
            "2021-02-01 14:50:18,854 - INFO - joeynmt.training - Epoch 1936: total training loss 8.61\n",
            "2021-02-01 14:50:18,854 - INFO - joeynmt.training - EPOCH 1937\n",
            "2021-02-01 14:50:31,384 - INFO - joeynmt.training - Epoch 1937: total training loss 8.60\n",
            "2021-02-01 14:50:31,385 - INFO - joeynmt.training - EPOCH 1938\n",
            "2021-02-01 14:50:43,837 - INFO - joeynmt.training - Epoch 1938: total training loss 8.58\n",
            "2021-02-01 14:50:43,837 - INFO - joeynmt.training - EPOCH 1939\n",
            "2021-02-01 14:50:56,439 - INFO - joeynmt.training - Epoch 1939: total training loss 8.56\n",
            "2021-02-01 14:50:56,440 - INFO - joeynmt.training - EPOCH 1940\n",
            "2021-02-01 14:51:08,859 - INFO - joeynmt.training - Epoch 1940: total training loss 8.61\n",
            "2021-02-01 14:51:08,860 - INFO - joeynmt.training - EPOCH 1941\n",
            "2021-02-01 14:51:09,709 - INFO - joeynmt.training - Epoch 1941, Step:    29000, Batch Loss:     0.552729, Tokens per Sec:    17883, Lr: 0.000035\n",
            "2021-02-01 14:51:39,026 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:51:39,027 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:51:39,027 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:51:39,027 - INFO - joeynmt.training - \tHypothesis: Себебі Таурат заңында Мұса арқылы берілген Таурат заңы мен ғауы қалаға апарып, мәжілісханаларға жіберіп отырды.\n",
            "2021-02-01 14:51:39,027 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:51:39,028 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:51:39,028 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:51:39,028 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құятқа деп ойлап келген еді.\n",
            "2021-02-01 14:51:39,028 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:51:39,029 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:51:39,029 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:51:39,029 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам сол рет жақсы көрдім. Ол Тоқтының үстінде отырған Құдай халқын қатты аштады! Жоқ, Құдайды қастерлейтін әйелге үйленіп, оның есімі мен қатты ашылықты болмады.※\n",
            "2021-02-01 14:51:39,029 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:51:39,030 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:51:39,030 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:51:39,030 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне сай орналасқан кісінің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:51:39,030 - INFO - joeynmt.training - Validation result (greedy) at epoch 1941, step    29000: bleu:   5.38, loss: 87597.8906, ppl:  78.1936, duration: 29.3212s\n",
            "2021-02-01 14:51:50,776 - INFO - joeynmt.training - Epoch 1941: total training loss 8.61\n",
            "2021-02-01 14:51:50,776 - INFO - joeynmt.training - EPOCH 1942\n",
            "2021-02-01 14:52:03,470 - INFO - joeynmt.training - Epoch 1942: total training loss 8.59\n",
            "2021-02-01 14:52:03,471 - INFO - joeynmt.training - EPOCH 1943\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:52:15,878 - INFO - joeynmt.training - Epoch 1943: total training loss 8.61\n",
            "2021-02-01 14:52:15,879 - INFO - joeynmt.training - EPOCH 1944\n",
            "2021-02-01 14:52:28,448 - INFO - joeynmt.training - Epoch 1944: total training loss 8.64\n",
            "2021-02-01 14:52:28,449 - INFO - joeynmt.training - EPOCH 1945\n",
            "2021-02-01 14:52:40,854 - INFO - joeynmt.training - Epoch 1945: total training loss 8.62\n",
            "2021-02-01 14:52:40,854 - INFO - joeynmt.training - EPOCH 1946\n",
            "2021-02-01 14:52:53,578 - INFO - joeynmt.training - Epoch 1946: total training loss 8.65\n",
            "2021-02-01 14:52:53,578 - INFO - joeynmt.training - EPOCH 1947\n",
            "2021-02-01 14:53:02,739 - INFO - joeynmt.training - Epoch 1947, Step:    29100, Batch Loss:     0.560201, Tokens per Sec:    18310, Lr: 0.000035\n",
            "2021-02-01 14:53:06,220 - INFO - joeynmt.training - Epoch 1947: total training loss 8.61\n",
            "2021-02-01 14:53:06,220 - INFO - joeynmt.training - EPOCH 1948\n",
            "2021-02-01 14:53:18,746 - INFO - joeynmt.training - Epoch 1948: total training loss 8.60\n",
            "2021-02-01 14:53:18,747 - INFO - joeynmt.training - EPOCH 1949\n",
            "2021-02-01 14:53:31,431 - INFO - joeynmt.training - Epoch 1949: total training loss 8.62\n",
            "2021-02-01 14:53:31,431 - INFO - joeynmt.training - EPOCH 1950\n",
            "2021-02-01 14:53:43,910 - INFO - joeynmt.training - Epoch 1950: total training loss 8.62\n",
            "2021-02-01 14:53:43,910 - INFO - joeynmt.training - EPOCH 1951\n",
            "2021-02-01 14:53:56,262 - INFO - joeynmt.training - Epoch 1951: total training loss 8.61\n",
            "2021-02-01 14:53:56,262 - INFO - joeynmt.training - EPOCH 1952\n",
            "2021-02-01 14:54:08,471 - INFO - joeynmt.training - Epoch 1952: total training loss 8.61\n",
            "2021-02-01 14:54:08,472 - INFO - joeynmt.training - EPOCH 1953\n",
            "2021-02-01 14:54:20,769 - INFO - joeynmt.training - Epoch 1953: total training loss 8.55\n",
            "2021-02-01 14:54:20,769 - INFO - joeynmt.training - EPOCH 1954\n",
            "2021-02-01 14:54:25,681 - INFO - joeynmt.training - Epoch 1954, Step:    29200, Batch Loss:     0.584708, Tokens per Sec:    18248, Lr: 0.000035\n",
            "2021-02-01 14:54:33,168 - INFO - joeynmt.training - Epoch 1954: total training loss 8.56\n",
            "2021-02-01 14:54:33,169 - INFO - joeynmt.training - EPOCH 1955\n",
            "2021-02-01 14:54:45,599 - INFO - joeynmt.training - Epoch 1955: total training loss 8.52\n",
            "2021-02-01 14:54:45,599 - INFO - joeynmt.training - EPOCH 1956\n",
            "2021-02-01 14:54:57,872 - INFO - joeynmt.training - Epoch 1956: total training loss 8.62\n",
            "2021-02-01 14:54:57,873 - INFO - joeynmt.training - EPOCH 1957\n",
            "2021-02-01 14:55:10,154 - INFO - joeynmt.training - Epoch 1957: total training loss 8.59\n",
            "2021-02-01 14:55:10,155 - INFO - joeynmt.training - EPOCH 1958\n",
            "2021-02-01 14:55:22,412 - INFO - joeynmt.training - Epoch 1958: total training loss 8.61\n",
            "2021-02-01 14:55:22,412 - INFO - joeynmt.training - EPOCH 1959\n",
            "2021-02-01 14:55:34,801 - INFO - joeynmt.training - Epoch 1959: total training loss 8.60\n",
            "2021-02-01 14:55:34,801 - INFO - joeynmt.training - EPOCH 1960\n",
            "2021-02-01 14:55:47,151 - INFO - joeynmt.training - Epoch 1960: total training loss 8.61\n",
            "2021-02-01 14:55:47,152 - INFO - joeynmt.training - EPOCH 1961\n",
            "2021-02-01 14:55:47,991 - INFO - joeynmt.training - Epoch 1961, Step:    29300, Batch Loss:     0.581044, Tokens per Sec:    17911, Lr: 0.000035\n",
            "2021-02-01 14:55:59,648 - INFO - joeynmt.training - Epoch 1961: total training loss 8.58\n",
            "2021-02-01 14:55:59,648 - INFO - joeynmt.training - EPOCH 1962\n",
            "2021-02-01 14:56:11,921 - INFO - joeynmt.training - Epoch 1962: total training loss 8.59\n",
            "2021-02-01 14:56:11,921 - INFO - joeynmt.training - EPOCH 1963\n",
            "2021-02-01 14:56:24,228 - INFO - joeynmt.training - Epoch 1963: total training loss 8.57\n",
            "2021-02-01 14:56:24,229 - INFO - joeynmt.training - EPOCH 1964\n",
            "2021-02-01 14:56:36,564 - INFO - joeynmt.training - Epoch 1964: total training loss 8.54\n",
            "2021-02-01 14:56:36,564 - INFO - joeynmt.training - EPOCH 1965\n",
            "2021-02-01 14:56:48,963 - INFO - joeynmt.training - Epoch 1965: total training loss 8.56\n",
            "2021-02-01 14:56:48,963 - INFO - joeynmt.training - EPOCH 1966\n",
            "2021-02-01 14:57:01,642 - INFO - joeynmt.training - Epoch 1966: total training loss 8.60\n",
            "2021-02-01 14:57:01,642 - INFO - joeynmt.training - EPOCH 1967\n",
            "2021-02-01 14:57:10,898 - INFO - joeynmt.training - Epoch 1967, Step:    29400, Batch Loss:     0.568381, Tokens per Sec:    18046, Lr: 0.000035\n",
            "2021-02-01 14:57:14,249 - INFO - joeynmt.training - Epoch 1967: total training loss 8.63\n",
            "2021-02-01 14:57:14,249 - INFO - joeynmt.training - EPOCH 1968\n",
            "2021-02-01 14:57:26,890 - INFO - joeynmt.training - Epoch 1968: total training loss 8.57\n",
            "2021-02-01 14:57:26,890 - INFO - joeynmt.training - EPOCH 1969\n",
            "2021-02-01 14:57:39,573 - INFO - joeynmt.training - Epoch 1969: total training loss 8.58\n",
            "2021-02-01 14:57:39,574 - INFO - joeynmt.training - EPOCH 1970\n",
            "2021-02-01 14:57:52,129 - INFO - joeynmt.training - Epoch 1970: total training loss 8.56\n",
            "2021-02-01 14:57:52,130 - INFO - joeynmt.training - EPOCH 1971\n",
            "2021-02-01 14:58:04,933 - INFO - joeynmt.training - Epoch 1971: total training loss 8.54\n",
            "2021-02-01 14:58:04,933 - INFO - joeynmt.training - EPOCH 1972\n",
            "2021-02-01 14:58:17,664 - INFO - joeynmt.training - Epoch 1972: total training loss 8.58\n",
            "2021-02-01 14:58:17,664 - INFO - joeynmt.training - EPOCH 1973\n",
            "2021-02-01 14:58:30,051 - INFO - joeynmt.training - Epoch 1973: total training loss 8.59\n",
            "2021-02-01 14:58:30,052 - INFO - joeynmt.training - EPOCH 1974\n",
            "2021-02-01 14:58:34,907 - INFO - joeynmt.training - Epoch 1974, Step:    29500, Batch Loss:     0.597594, Tokens per Sec:    18187, Lr: 0.000035\n",
            "2021-02-01 14:59:03,764 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға батыл үйретіп келді.※\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 14:59:03,765 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құятқа деп ойлап келген еді.\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - \tHypothesis: Мен бүкіл адамзаттың есінен бас тартқанда, адам сылта рет еніп, оған ілесіп тұрған кезімде бар Құдайды көрдім. Ол қатты ашты,※ Құдайды мадақтап отырған жеті жені көрмедім.\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 14:59:03,766 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 14:59:03,767 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 14:59:03,767 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жерімізден орын алдыңдар.※ Сол жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 14:59:03,767 - INFO - joeynmt.training - Validation result (greedy) at epoch 1974, step    29500: bleu:   5.21, loss: 87527.8516, ppl:  77.9216, duration: 28.8589s\n",
            "2021-02-01 14:59:11,274 - INFO - joeynmt.training - Epoch 1974: total training loss 8.57\n",
            "2021-02-01 14:59:11,274 - INFO - joeynmt.training - EPOCH 1975\n",
            "2021-02-01 14:59:23,204 - INFO - joeynmt.training - Epoch 1975: total training loss 7.99\n",
            "2021-02-01 14:59:23,204 - INFO - joeynmt.training - EPOCH 1976\n",
            "2021-02-01 14:59:35,525 - INFO - joeynmt.training - Epoch 1976: total training loss 8.54\n",
            "2021-02-01 14:59:35,525 - INFO - joeynmt.training - EPOCH 1977\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 14:59:47,916 - INFO - joeynmt.training - Epoch 1977: total training loss 8.57\n",
            "2021-02-01 14:59:47,916 - INFO - joeynmt.training - EPOCH 1978\n",
            "2021-02-01 15:00:00,325 - INFO - joeynmt.training - Epoch 1978: total training loss 8.56\n",
            "2021-02-01 15:00:00,325 - INFO - joeynmt.training - EPOCH 1979\n",
            "2021-02-01 15:00:12,784 - INFO - joeynmt.training - Epoch 1979: total training loss 8.58\n",
            "2021-02-01 15:00:12,785 - INFO - joeynmt.training - EPOCH 1980\n",
            "2021-02-01 15:00:25,014 - INFO - joeynmt.training - Epoch 1980: total training loss 8.58\n",
            "2021-02-01 15:00:25,014 - INFO - joeynmt.training - EPOCH 1981\n",
            "2021-02-01 15:00:26,658 - INFO - joeynmt.training - Epoch 1981, Step:    29600, Batch Loss:     0.564901, Tokens per Sec:    18775, Lr: 0.000035\n",
            "2021-02-01 15:00:37,330 - INFO - joeynmt.training - Epoch 1981: total training loss 8.61\n",
            "2021-02-01 15:00:37,330 - INFO - joeynmt.training - EPOCH 1982\n",
            "2021-02-01 15:00:49,693 - INFO - joeynmt.training - Epoch 1982: total training loss 8.54\n",
            "2021-02-01 15:00:49,694 - INFO - joeynmt.training - EPOCH 1983\n",
            "2021-02-01 15:01:02,047 - INFO - joeynmt.training - Epoch 1983: total training loss 8.54\n",
            "2021-02-01 15:01:02,047 - INFO - joeynmt.training - EPOCH 1984\n",
            "2021-02-01 15:01:14,164 - INFO - joeynmt.training - Epoch 1984: total training loss 8.03\n",
            "2021-02-01 15:01:14,164 - INFO - joeynmt.training - EPOCH 1985\n",
            "2021-02-01 15:01:26,427 - INFO - joeynmt.training - Epoch 1985: total training loss 8.54\n",
            "2021-02-01 15:01:26,427 - INFO - joeynmt.training - EPOCH 1986\n",
            "2021-02-01 15:01:38,724 - INFO - joeynmt.training - Epoch 1986: total training loss 8.59\n",
            "2021-02-01 15:01:38,724 - INFO - joeynmt.training - EPOCH 1987\n",
            "2021-02-01 15:01:49,238 - INFO - joeynmt.training - Epoch 1987, Step:    29700, Batch Loss:     0.592545, Tokens per Sec:    18774, Lr: 0.000035\n",
            "2021-02-01 15:01:50,846 - INFO - joeynmt.training - Epoch 1987: total training loss 8.57\n",
            "2021-02-01 15:01:50,846 - INFO - joeynmt.training - EPOCH 1988\n",
            "2021-02-01 15:02:03,190 - INFO - joeynmt.training - Epoch 1988: total training loss 8.55\n",
            "2021-02-01 15:02:03,190 - INFO - joeynmt.training - EPOCH 1989\n",
            "2021-02-01 15:02:15,501 - INFO - joeynmt.training - Epoch 1989: total training loss 8.56\n",
            "2021-02-01 15:02:15,501 - INFO - joeynmt.training - EPOCH 1990\n",
            "2021-02-01 15:02:27,810 - INFO - joeynmt.training - Epoch 1990: total training loss 8.58\n",
            "2021-02-01 15:02:27,810 - INFO - joeynmt.training - EPOCH 1991\n",
            "2021-02-01 15:02:40,152 - INFO - joeynmt.training - Epoch 1991: total training loss 8.55\n",
            "2021-02-01 15:02:40,152 - INFO - joeynmt.training - EPOCH 1992\n",
            "2021-02-01 15:02:52,497 - INFO - joeynmt.training - Epoch 1992: total training loss 8.51\n",
            "2021-02-01 15:02:52,497 - INFO - joeynmt.training - EPOCH 1993\n",
            "2021-02-01 15:03:04,553 - INFO - joeynmt.training - Epoch 1993: total training loss 8.02\n",
            "2021-02-01 15:03:04,553 - INFO - joeynmt.training - EPOCH 1994\n",
            "2021-02-01 15:03:11,912 - INFO - joeynmt.training - Epoch 1994, Step:    29800, Batch Loss:     0.577269, Tokens per Sec:    18783, Lr: 0.000035\n",
            "2021-02-01 15:03:16,823 - INFO - joeynmt.training - Epoch 1994: total training loss 8.56\n",
            "2021-02-01 15:03:16,823 - INFO - joeynmt.training - EPOCH 1995\n",
            "2021-02-01 15:03:29,096 - INFO - joeynmt.training - Epoch 1995: total training loss 8.52\n",
            "2021-02-01 15:03:29,096 - INFO - joeynmt.training - EPOCH 1996\n",
            "2021-02-01 15:03:41,473 - INFO - joeynmt.training - Epoch 1996: total training loss 8.55\n",
            "2021-02-01 15:03:41,473 - INFO - joeynmt.training - EPOCH 1997\n",
            "2021-02-01 15:03:53,901 - INFO - joeynmt.training - Epoch 1997: total training loss 8.54\n",
            "2021-02-01 15:03:53,901 - INFO - joeynmt.training - EPOCH 1998\n",
            "2021-02-01 15:04:06,249 - INFO - joeynmt.training - Epoch 1998: total training loss 8.56\n",
            "2021-02-01 15:04:06,250 - INFO - joeynmt.training - EPOCH 1999\n",
            "2021-02-01 15:04:18,492 - INFO - joeynmt.training - Epoch 1999: total training loss 8.56\n",
            "2021-02-01 15:04:18,493 - INFO - joeynmt.training - EPOCH 2000\n",
            "2021-02-01 15:04:31,037 - INFO - joeynmt.training - Epoch 2000: total training loss 8.54\n",
            "2021-02-01 15:04:31,037 - INFO - joeynmt.training - EPOCH 2001\n",
            "2021-02-01 15:04:34,317 - INFO - joeynmt.training - Epoch 2001, Step:    29900, Batch Loss:     0.556040, Tokens per Sec:    18389, Lr: 0.000035\n",
            "2021-02-01 15:04:43,529 - INFO - joeynmt.training - Epoch 2001: total training loss 8.55\n",
            "2021-02-01 15:04:43,529 - INFO - joeynmt.training - EPOCH 2002\n",
            "2021-02-01 15:04:56,106 - INFO - joeynmt.training - Epoch 2002: total training loss 8.53\n",
            "2021-02-01 15:04:56,106 - INFO - joeynmt.training - EPOCH 2003\n",
            "2021-02-01 15:05:08,743 - INFO - joeynmt.training - Epoch 2003: total training loss 8.58\n",
            "2021-02-01 15:05:08,743 - INFO - joeynmt.training - EPOCH 2004\n",
            "2021-02-01 15:05:21,398 - INFO - joeynmt.training - Epoch 2004: total training loss 8.54\n",
            "2021-02-01 15:05:21,398 - INFO - joeynmt.training - EPOCH 2005\n",
            "2021-02-01 15:05:33,964 - INFO - joeynmt.training - Epoch 2005: total training loss 8.56\n",
            "2021-02-01 15:05:33,964 - INFO - joeynmt.training - EPOCH 2006\n",
            "2021-02-01 15:05:46,480 - INFO - joeynmt.training - Epoch 2006: total training loss 8.57\n",
            "2021-02-01 15:05:46,480 - INFO - joeynmt.training - EPOCH 2007\n",
            "2021-02-01 15:05:58,028 - INFO - joeynmt.training - Epoch 2007, Step:    30000, Batch Loss:     0.611931, Tokens per Sec:    18317, Lr: 0.000035\n",
            "2021-02-01 15:06:25,782 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 15:06:25,784 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 15:06:25,784 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 15:06:25,784 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға батыл үйретіп жатты.\n",
            "2021-02-01 15:06:25,784 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 15:06:25,785 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартқанда, оны жерге бас тартылып жіберіп, Тоқтының ұят екенін көрдім. Ол Құдайды қастерлейтін әйелдің шын мәнінде тұрғанын көрмедім.\n",
            "2021-02-01 15:06:25,786 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 15:06:25,786 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 15:06:25,786 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 15:06:25,786 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте мекендерің көкке көтеріліп, Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 15:06:25,786 - INFO - joeynmt.training - Validation result (greedy) at epoch 2007, step    30000: bleu:   4.95, loss: 87625.2344, ppl:  78.3001, duration: 27.7573s\n",
            "2021-02-01 15:06:26,721 - INFO - joeynmt.training - Epoch 2007: total training loss 8.57\n",
            "2021-02-01 15:06:26,721 - INFO - joeynmt.training - EPOCH 2008\n",
            "2021-02-01 15:06:39,393 - INFO - joeynmt.training - Epoch 2008: total training loss 8.53\n",
            "2021-02-01 15:06:39,394 - INFO - joeynmt.training - EPOCH 2009\n",
            "2021-02-01 15:06:51,863 - INFO - joeynmt.training - Epoch 2009: total training loss 8.55\n",
            "2021-02-01 15:06:51,864 - INFO - joeynmt.training - EPOCH 2010\n",
            "2021-02-01 15:07:04,491 - INFO - joeynmt.training - Epoch 2010: total training loss 8.53\n",
            "2021-02-01 15:07:04,492 - INFO - joeynmt.training - EPOCH 2011\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8388608.0\n",
            "2021-02-01 15:07:16,939 - INFO - joeynmt.training - Epoch 2011: total training loss 8.52\n",
            "2021-02-01 15:07:16,940 - INFO - joeynmt.training - EPOCH 2012\n",
            "2021-02-01 15:07:29,517 - INFO - joeynmt.training - Epoch 2012: total training loss 8.49\n",
            "2021-02-01 15:07:29,518 - INFO - joeynmt.training - EPOCH 2013\n",
            "2021-02-01 15:07:42,204 - INFO - joeynmt.training - Epoch 2013: total training loss 8.54\n",
            "2021-02-01 15:07:42,205 - INFO - joeynmt.training - EPOCH 2014\n",
            "2021-02-01 15:07:49,721 - INFO - joeynmt.training - Epoch 2014, Step:    30100, Batch Loss:     0.564265, Tokens per Sec:    17730, Lr: 0.000025\n",
            "2021-02-01 15:07:54,716 - INFO - joeynmt.training - Epoch 2014: total training loss 8.53\n",
            "2021-02-01 15:07:54,716 - INFO - joeynmt.training - EPOCH 2015\n",
            "2021-02-01 15:08:07,508 - INFO - joeynmt.training - Epoch 2015: total training loss 8.53\n",
            "2021-02-01 15:08:07,508 - INFO - joeynmt.training - EPOCH 2016\n",
            "2021-02-01 15:08:20,000 - INFO - joeynmt.training - Epoch 2016: total training loss 8.56\n",
            "2021-02-01 15:08:20,001 - INFO - joeynmt.training - EPOCH 2017\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 15:08:32,583 - INFO - joeynmt.training - Epoch 2017: total training loss 8.50\n",
            "2021-02-01 15:08:32,584 - INFO - joeynmt.training - EPOCH 2018\n",
            "2021-02-01 15:08:45,242 - INFO - joeynmt.training - Epoch 2018: total training loss 8.52\n",
            "2021-02-01 15:08:45,242 - INFO - joeynmt.training - EPOCH 2019\n",
            "2021-02-01 15:08:57,677 - INFO - joeynmt.training - Epoch 2019: total training loss 8.51\n",
            "2021-02-01 15:08:57,677 - INFO - joeynmt.training - EPOCH 2020\n",
            "2021-02-01 15:09:10,393 - INFO - joeynmt.training - Epoch 2020: total training loss 8.52\n",
            "2021-02-01 15:09:10,393 - INFO - joeynmt.training - EPOCH 2021\n",
            "2021-02-01 15:09:13,781 - INFO - joeynmt.training - Epoch 2021, Step:    30200, Batch Loss:     0.559406, Tokens per Sec:    18025, Lr: 0.000025\n",
            "2021-02-01 15:09:23,058 - INFO - joeynmt.training - Epoch 2021: total training loss 8.53\n",
            "2021-02-01 15:09:23,059 - INFO - joeynmt.training - EPOCH 2022\n",
            "2021-02-01 15:09:35,818 - INFO - joeynmt.training - Epoch 2022: total training loss 8.49\n",
            "2021-02-01 15:09:35,818 - INFO - joeynmt.training - EPOCH 2023\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 15:09:48,369 - INFO - joeynmt.training - Epoch 2023: total training loss 8.51\n",
            "2021-02-01 15:09:48,370 - INFO - joeynmt.training - EPOCH 2024\n",
            "2021-02-01 15:10:00,885 - INFO - joeynmt.training - Epoch 2024: total training loss 8.49\n",
            "2021-02-01 15:10:00,885 - INFO - joeynmt.training - EPOCH 2025\n",
            "2021-02-01 15:10:13,364 - INFO - joeynmt.training - Epoch 2025: total training loss 8.52\n",
            "2021-02-01 15:10:13,364 - INFO - joeynmt.training - EPOCH 2026\n",
            "2021-02-01 15:10:25,889 - INFO - joeynmt.training - Epoch 2026: total training loss 8.52\n",
            "2021-02-01 15:10:25,890 - INFO - joeynmt.training - EPOCH 2027\n",
            "2021-02-01 15:10:37,528 - INFO - joeynmt.training - Epoch 2027, Step:    30300, Batch Loss:     0.580555, Tokens per Sec:    18368, Lr: 0.000025\n",
            "2021-02-01 15:10:38,349 - INFO - joeynmt.training - Epoch 2027: total training loss 8.51\n",
            "2021-02-01 15:10:38,349 - INFO - joeynmt.training - EPOCH 2028\n",
            "2021-02-01 15:10:50,969 - INFO - joeynmt.training - Epoch 2028: total training loss 8.54\n",
            "2021-02-01 15:10:50,970 - INFO - joeynmt.training - EPOCH 2029\n",
            "2021-02-01 15:11:03,452 - INFO - joeynmt.training - Epoch 2029: total training loss 8.50\n",
            "2021-02-01 15:11:03,453 - INFO - joeynmt.training - EPOCH 2030\n",
            "2021-02-01 15:11:16,033 - INFO - joeynmt.training - Epoch 2030: total training loss 8.50\n",
            "2021-02-01 15:11:16,034 - INFO - joeynmt.training - EPOCH 2031\n",
            "2021-02-01 15:11:28,529 - INFO - joeynmt.training - Epoch 2031: total training loss 8.53\n",
            "2021-02-01 15:11:28,529 - INFO - joeynmt.training - EPOCH 2032\n",
            "2021-02-01 15:11:41,063 - INFO - joeynmt.training - Epoch 2032: total training loss 8.53\n",
            "2021-02-01 15:11:41,063 - INFO - joeynmt.training - EPOCH 2033\n",
            "2021-02-01 15:11:53,791 - INFO - joeynmt.training - Epoch 2033: total training loss 8.51\n",
            "2021-02-01 15:11:53,791 - INFO - joeynmt.training - EPOCH 2034\n",
            "2021-02-01 15:12:01,209 - INFO - joeynmt.training - Epoch 2034, Step:    30400, Batch Loss:     0.605295, Tokens per Sec:    18087, Lr: 0.000025\n",
            "2021-02-01 15:12:06,339 - INFO - joeynmt.training - Epoch 2034: total training loss 8.54\n",
            "2021-02-01 15:12:06,339 - INFO - joeynmt.training - EPOCH 2035\n",
            "2021-02-01 15:12:18,662 - INFO - joeynmt.training - Epoch 2035: total training loss 8.53\n",
            "2021-02-01 15:12:18,663 - INFO - joeynmt.training - EPOCH 2036\n",
            "2021-02-01 15:12:31,081 - INFO - joeynmt.training - Epoch 2036: total training loss 8.53\n",
            "2021-02-01 15:12:31,081 - INFO - joeynmt.training - EPOCH 2037\n",
            "2021-02-01 15:12:43,776 - INFO - joeynmt.training - Epoch 2037: total training loss 8.50\n",
            "2021-02-01 15:12:43,776 - INFO - joeynmt.training - EPOCH 2038\n",
            "2021-02-01 15:12:56,361 - INFO - joeynmt.training - Epoch 2038: total training loss 8.49\n",
            "2021-02-01 15:12:56,361 - INFO - joeynmt.training - EPOCH 2039\n",
            "2021-02-01 15:13:09,026 - INFO - joeynmt.training - Epoch 2039: total training loss 8.51\n",
            "2021-02-01 15:13:09,026 - INFO - joeynmt.training - EPOCH 2040\n",
            "2021-02-01 15:13:21,843 - INFO - joeynmt.training - Epoch 2040: total training loss 8.51\n",
            "2021-02-01 15:13:21,843 - INFO - joeynmt.training - EPOCH 2041\n",
            "2021-02-01 15:13:25,188 - INFO - joeynmt.training - Epoch 2041, Step:    30500, Batch Loss:     0.561952, Tokens per Sec:    18733, Lr: 0.000025\n",
            "2021-02-01 15:13:54,811 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 15:13:54,812 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 15:13:54,812 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 15:13:54,812 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға жіберді.※\n",
            "2021-02-01 15:13:54,812 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 15:13:54,812 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі дереу ояу үшін ұстап тұрып, Исаны тігіп алды.\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еттін, оны көрдім. Ол Тияны отырған Құдайды мадақтап тұрған, ешқандай ауыртпалық көрмегендерді деп ойлау үшін үйге кірді. Олар қатты ашулана көріңіз!\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 15:13:54,813 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 15:13:54,814 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 15:13:54,814 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтермеген кезімде орнына батылдық, Оның оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 15:13:54,814 - INFO - joeynmt.training - Validation result (greedy) at epoch 2041, step    30500: bleu:   5.22, loss: 87622.7656, ppl:  78.2905, duration: 29.6249s\n",
            "2021-02-01 15:14:03,982 - INFO - joeynmt.training - Epoch 2041: total training loss 8.56\n",
            "2021-02-01 15:14:03,982 - INFO - joeynmt.training - EPOCH 2042\n",
            "2021-02-01 15:14:16,695 - INFO - joeynmt.training - Epoch 2042: total training loss 8.51\n",
            "2021-02-01 15:14:16,695 - INFO - joeynmt.training - EPOCH 2043\n",
            "2021-02-01 15:14:29,512 - INFO - joeynmt.training - Epoch 2043: total training loss 8.53\n",
            "2021-02-01 15:14:29,512 - INFO - joeynmt.training - EPOCH 2044\n",
            "2021-02-01 15:14:41,821 - INFO - joeynmt.training - Epoch 2044: total training loss 7.95\n",
            "2021-02-01 15:14:41,821 - INFO - joeynmt.training - EPOCH 2045\n",
            "2021-02-01 15:14:54,424 - INFO - joeynmt.training - Epoch 2045: total training loss 8.51\n",
            "2021-02-01 15:14:54,424 - INFO - joeynmt.training - EPOCH 2046\n",
            "2021-02-01 15:15:07,009 - INFO - joeynmt.training - Epoch 2046: total training loss 8.50\n",
            "2021-02-01 15:15:07,009 - INFO - joeynmt.training - EPOCH 2047\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2097152.0\n",
            "2021-02-01 15:15:19,395 - INFO - joeynmt.training - Epoch 2047, Step:    30600, Batch Loss:     0.570237, Tokens per Sec:    18126, Lr: 0.000025\n",
            "2021-02-01 15:15:19,596 - INFO - joeynmt.training - Epoch 2047: total training loss 8.47\n",
            "2021-02-01 15:15:19,597 - INFO - joeynmt.training - EPOCH 2048\n",
            "2021-02-01 15:15:32,207 - INFO - joeynmt.training - Epoch 2048: total training loss 8.49\n",
            "2021-02-01 15:15:32,208 - INFO - joeynmt.training - EPOCH 2049\n",
            "2021-02-01 15:15:44,761 - INFO - joeynmt.training - Epoch 2049: total training loss 8.51\n",
            "2021-02-01 15:15:44,761 - INFO - joeynmt.training - EPOCH 2050\n",
            "2021-02-01 15:15:57,133 - INFO - joeynmt.training - Epoch 2050: total training loss 7.96\n",
            "2021-02-01 15:15:57,134 - INFO - joeynmt.training - EPOCH 2051\n",
            "2021-02-01 15:16:09,712 - INFO - joeynmt.training - Epoch 2051: total training loss 8.52\n",
            "2021-02-01 15:16:09,712 - INFO - joeynmt.training - EPOCH 2052\n",
            "2021-02-01 15:16:22,283 - INFO - joeynmt.training - Epoch 2052: total training loss 8.44\n",
            "2021-02-01 15:16:22,284 - INFO - joeynmt.training - EPOCH 2053\n",
            "2021-02-01 15:16:34,747 - INFO - joeynmt.training - Epoch 2053: total training loss 8.50\n",
            "2021-02-01 15:16:34,747 - INFO - joeynmt.training - EPOCH 2054\n",
            "2021-02-01 15:16:43,869 - INFO - joeynmt.training - Epoch 2054, Step:    30700, Batch Loss:     0.523563, Tokens per Sec:    18494, Lr: 0.000025\n",
            "2021-02-01 15:16:47,193 - INFO - joeynmt.training - Epoch 2054: total training loss 8.50\n",
            "2021-02-01 15:16:47,194 - INFO - joeynmt.training - EPOCH 2055\n",
            "2021-02-01 15:16:59,631 - INFO - joeynmt.training - Epoch 2055: total training loss 8.51\n",
            "2021-02-01 15:16:59,631 - INFO - joeynmt.training - EPOCH 2056\n",
            "2021-02-01 15:17:11,939 - INFO - joeynmt.training - Epoch 2056: total training loss 7.93\n",
            "2021-02-01 15:17:11,940 - INFO - joeynmt.training - EPOCH 2057\n",
            "2021-02-01 15:17:24,638 - INFO - joeynmt.training - Epoch 2057: total training loss 8.56\n",
            "2021-02-01 15:17:24,639 - INFO - joeynmt.training - EPOCH 2058\n",
            "2021-02-01 15:17:37,033 - INFO - joeynmt.training - Epoch 2058: total training loss 7.94\n",
            "2021-02-01 15:17:37,034 - INFO - joeynmt.training - EPOCH 2059\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
            "2021-02-01 15:17:49,733 - INFO - joeynmt.training - Epoch 2059: total training loss 8.47\n",
            "2021-02-01 15:17:49,733 - INFO - joeynmt.training - EPOCH 2060\n",
            "2021-02-01 15:18:02,121 - INFO - joeynmt.training - Epoch 2060: total training loss 8.52\n",
            "2021-02-01 15:18:02,121 - INFO - joeynmt.training - EPOCH 2061\n",
            "2021-02-01 15:18:08,812 - INFO - joeynmt.training - Epoch 2061, Step:    30800, Batch Loss:     0.577193, Tokens per Sec:    18684, Lr: 0.000025\n",
            "2021-02-01 15:18:14,580 - INFO - joeynmt.training - Epoch 2061: total training loss 8.54\n",
            "2021-02-01 15:18:14,580 - INFO - joeynmt.training - EPOCH 2062\n",
            "2021-02-01 15:18:26,967 - INFO - joeynmt.training - Epoch 2062: total training loss 7.91\n",
            "2021-02-01 15:18:26,967 - INFO - joeynmt.training - EPOCH 2063\n",
            "2021-02-01 15:18:39,650 - INFO - joeynmt.training - Epoch 2063: total training loss 8.51\n",
            "2021-02-01 15:18:39,650 - INFO - joeynmt.training - EPOCH 2064\n",
            "2021-02-01 15:18:52,155 - INFO - joeynmt.training - Epoch 2064: total training loss 8.54\n",
            "2021-02-01 15:18:52,155 - INFO - joeynmt.training - EPOCH 2065\n",
            "2021-02-01 15:19:04,507 - INFO - joeynmt.training - Epoch 2065: total training loss 8.49\n",
            "2021-02-01 15:19:04,508 - INFO - joeynmt.training - EPOCH 2066\n",
            "2021-02-01 15:19:17,052 - INFO - joeynmt.training - Epoch 2066: total training loss 8.44\n",
            "2021-02-01 15:19:17,053 - INFO - joeynmt.training - EPOCH 2067\n",
            "2021-02-01 15:19:29,871 - INFO - joeynmt.training - Epoch 2067: total training loss 8.49\n",
            "2021-02-01 15:19:29,871 - INFO - joeynmt.training - EPOCH 2068\n",
            "2021-02-01 15:19:33,224 - INFO - joeynmt.training - Epoch 2068, Step:    30900, Batch Loss:     0.500164, Tokens per Sec:    17456, Lr: 0.000025\n",
            "2021-02-01 15:19:42,271 - INFO - joeynmt.training - Epoch 2068: total training loss 8.49\n",
            "2021-02-01 15:19:42,271 - INFO - joeynmt.training - EPOCH 2069\n",
            "2021-02-01 15:19:54,931 - INFO - joeynmt.training - Epoch 2069: total training loss 8.49\n",
            "2021-02-01 15:19:54,931 - INFO - joeynmt.training - EPOCH 2070\n",
            "2021-02-01 15:20:07,521 - INFO - joeynmt.training - Epoch 2070: total training loss 8.51\n",
            "2021-02-01 15:20:07,521 - INFO - joeynmt.training - EPOCH 2071\n",
            "2021-02-01 15:20:20,119 - INFO - joeynmt.training - Epoch 2071: total training loss 8.49\n",
            "2021-02-01 15:20:20,119 - INFO - joeynmt.training - EPOCH 2072\n",
            "2021-02-01 15:20:32,577 - INFO - joeynmt.training - Epoch 2072: total training loss 8.51\n",
            "2021-02-01 15:20:32,577 - INFO - joeynmt.training - EPOCH 2073\n",
            "2021-02-01 15:20:44,946 - INFO - joeynmt.training - Epoch 2073: total training loss 7.91\n",
            "2021-02-01 15:20:44,946 - INFO - joeynmt.training - EPOCH 2074\n",
            "2021-02-01 15:20:57,422 - INFO - joeynmt.training - Epoch 2074, Step:    31000, Batch Loss:     0.571321, Tokens per Sec:    18057, Lr: 0.000025\n",
            "2021-02-01 15:21:27,966 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 15:21:27,967 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 15:21:27,967 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 15:21:27,967 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға батыл үйретіп келді.※\n",
            "2021-02-01 15:21:27,968 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 15:21:27,968 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 15:21:27,968 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 15:21:27,968 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құятқа деп ойлап келген еді.\n",
            "2021-02-01 15:21:27,968 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 15:21:27,969 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 15:21:27,969 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 15:21:27,969 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еттсын. Сонда әлемді көрме; мұны көрген Құдай оның қатты дауыспен※ пайғамбар қатты ашылып: «Тік Иені Құдайды мадақтап тұра ал! Жер бетінде маған, алтын шашаттандырды.\n",
            "2021-02-01 15:21:27,969 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 15:21:27,970 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 15:21:27,970 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 15:21:27,970 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх көкте мекендерің көкке көтеріліп, Құдайдың оң жағындағы құрметті орнына отырыңдар.※\n",
            "2021-02-01 15:21:27,970 - INFO - joeynmt.training - Validation result (greedy) at epoch 2074, step    31000: bleu:   5.24, loss: 87560.8828, ppl:  78.0498, duration: 30.5474s\n",
            "2021-02-01 15:21:28,170 - INFO - joeynmt.training - Epoch 2074: total training loss 8.50\n",
            "2021-02-01 15:21:28,171 - INFO - joeynmt.training - EPOCH 2075\n",
            "2021-02-01 15:21:40,726 - INFO - joeynmt.training - Epoch 2075: total training loss 8.46\n",
            "2021-02-01 15:21:40,726 - INFO - joeynmt.training - EPOCH 2076\n",
            "2021-02-01 15:21:53,374 - INFO - joeynmt.training - Epoch 2076: total training loss 8.49\n",
            "2021-02-01 15:21:53,374 - INFO - joeynmt.training - EPOCH 2077\n",
            "2021-02-01 15:22:05,857 - INFO - joeynmt.training - Epoch 2077: total training loss 8.52\n",
            "2021-02-01 15:22:05,858 - INFO - joeynmt.training - EPOCH 2078\n",
            "2021-02-01 15:22:18,559 - INFO - joeynmt.training - Epoch 2078: total training loss 8.50\n",
            "2021-02-01 15:22:18,559 - INFO - joeynmt.training - EPOCH 2079\n",
            "2021-02-01 15:22:30,882 - INFO - joeynmt.training - Epoch 2079: total training loss 7.95\n",
            "2021-02-01 15:22:30,882 - INFO - joeynmt.training - EPOCH 2080\n",
            "2021-02-01 15:22:43,517 - INFO - joeynmt.training - Epoch 2080: total training loss 8.54\n",
            "2021-02-01 15:22:43,518 - INFO - joeynmt.training - EPOCH 2081\n",
            "2021-02-01 15:22:52,736 - INFO - joeynmt.training - Epoch 2081, Step:    31100, Batch Loss:     0.558574, Tokens per Sec:    18733, Lr: 0.000025\n",
            "2021-02-01 15:22:55,914 - INFO - joeynmt.training - Epoch 2081: total training loss 7.94\n",
            "2021-02-01 15:22:55,915 - INFO - joeynmt.training - EPOCH 2082\n",
            "2021-02-01 15:23:08,254 - INFO - joeynmt.training - Epoch 2082: total training loss 7.94\n",
            "2021-02-01 15:23:08,255 - INFO - joeynmt.training - EPOCH 2083\n",
            "2021-02-01 15:23:20,789 - INFO - joeynmt.training - Epoch 2083: total training loss 8.51\n",
            "2021-02-01 15:23:20,789 - INFO - joeynmt.training - EPOCH 2084\n",
            "2021-02-01 15:23:33,541 - INFO - joeynmt.training - Epoch 2084: total training loss 8.51\n",
            "2021-02-01 15:23:33,542 - INFO - joeynmt.training - EPOCH 2085\n",
            "2021-02-01 15:23:46,255 - INFO - joeynmt.training - Epoch 2085: total training loss 8.45\n",
            "2021-02-01 15:23:46,255 - INFO - joeynmt.training - EPOCH 2086\n",
            "2021-02-01 15:23:58,772 - INFO - joeynmt.training - Epoch 2086: total training loss 8.49\n",
            "2021-02-01 15:23:58,772 - INFO - joeynmt.training - EPOCH 2087\n",
            "2021-02-01 15:24:11,255 - INFO - joeynmt.training - Epoch 2087: total training loss 8.49\n",
            "2021-02-01 15:24:11,255 - INFO - joeynmt.training - EPOCH 2088\n",
            "2021-02-01 15:24:17,860 - INFO - joeynmt.training - Epoch 2088, Step:    31200, Batch Loss:     0.549058, Tokens per Sec:    17970, Lr: 0.000025\n",
            "2021-02-01 15:24:23,630 - INFO - joeynmt.training - Epoch 2088: total training loss 8.49\n",
            "2021-02-01 15:24:23,631 - INFO - joeynmt.training - EPOCH 2089\n",
            "2021-02-01 15:24:36,205 - INFO - joeynmt.training - Epoch 2089: total training loss 8.52\n",
            "2021-02-01 15:24:36,206 - INFO - joeynmt.training - EPOCH 2090\n",
            "2021-02-01 15:24:48,922 - INFO - joeynmt.training - Epoch 2090: total training loss 8.46\n",
            "2021-02-01 15:24:48,923 - INFO - joeynmt.training - EPOCH 2091\n",
            "2021-02-01 15:25:01,589 - INFO - joeynmt.training - Epoch 2091: total training loss 8.50\n",
            "2021-02-01 15:25:01,589 - INFO - joeynmt.training - EPOCH 2092\n",
            "2021-02-01 15:25:14,175 - INFO - joeynmt.training - Epoch 2092: total training loss 8.49\n",
            "2021-02-01 15:25:14,176 - INFO - joeynmt.training - EPOCH 2093\n",
            "2021-02-01 15:25:26,696 - INFO - joeynmt.training - Epoch 2093: total training loss 8.50\n",
            "2021-02-01 15:25:26,696 - INFO - joeynmt.training - EPOCH 2094\n",
            "2021-02-01 15:25:39,101 - INFO - joeynmt.training - Epoch 2094: total training loss 8.47\n",
            "2021-02-01 15:25:39,101 - INFO - joeynmt.training - EPOCH 2095\n",
            "2021-02-01 15:25:41,603 - INFO - joeynmt.training - Epoch 2095, Step:    31300, Batch Loss:     0.592327, Tokens per Sec:    17595, Lr: 0.000025\n",
            "2021-02-01 15:25:51,611 - INFO - joeynmt.training - Epoch 2095: total training loss 8.49\n",
            "2021-02-01 15:25:51,612 - INFO - joeynmt.training - EPOCH 2096\n",
            "2021-02-01 15:26:04,278 - INFO - joeynmt.training - Epoch 2096: total training loss 8.44\n",
            "2021-02-01 15:26:04,278 - INFO - joeynmt.training - EPOCH 2097\n",
            "2021-02-01 15:26:16,659 - INFO - joeynmt.training - Epoch 2097: total training loss 8.47\n",
            "2021-02-01 15:26:16,659 - INFO - joeynmt.training - EPOCH 2098\n",
            "2021-02-01 15:26:29,282 - INFO - joeynmt.training - Epoch 2098: total training loss 8.45\n",
            "2021-02-01 15:26:29,282 - INFO - joeynmt.training - EPOCH 2099\n",
            "2021-02-01 15:26:42,083 - INFO - joeynmt.training - Epoch 2099: total training loss 8.47\n",
            "2021-02-01 15:26:42,084 - INFO - joeynmt.training - EPOCH 2100\n",
            "2021-02-01 15:26:54,410 - INFO - joeynmt.training - Epoch 2100: total training loss 7.90\n",
            "2021-02-01 15:26:54,410 - INFO - joeynmt.training - EPOCH 2101\n",
            "2021-02-01 15:27:05,957 - INFO - joeynmt.training - Epoch 2101, Step:    31400, Batch Loss:     0.583294, Tokens per Sec:    18026, Lr: 0.000025\n",
            "2021-02-01 15:27:07,001 - INFO - joeynmt.training - Epoch 2101: total training loss 8.49\n",
            "2021-02-01 15:27:07,002 - INFO - joeynmt.training - EPOCH 2102\n",
            "2021-02-01 15:27:19,524 - INFO - joeynmt.training - Epoch 2102: total training loss 8.51\n",
            "2021-02-01 15:27:19,525 - INFO - joeynmt.training - EPOCH 2103\n",
            "2021-02-01 15:27:32,286 - INFO - joeynmt.training - Epoch 2103: total training loss 8.43\n",
            "2021-02-01 15:27:32,287 - INFO - joeynmt.training - EPOCH 2104\n",
            "2021-02-01 15:27:44,946 - INFO - joeynmt.training - Epoch 2104: total training loss 8.48\n",
            "2021-02-01 15:27:44,947 - INFO - joeynmt.training - EPOCH 2105\n",
            "2021-02-01 15:27:57,442 - INFO - joeynmt.training - Epoch 2105: total training loss 8.46\n",
            "2021-02-01 15:27:57,443 - INFO - joeynmt.training - EPOCH 2106\n",
            "2021-02-01 15:28:10,063 - INFO - joeynmt.training - Epoch 2106: total training loss 8.48\n",
            "2021-02-01 15:28:10,063 - INFO - joeynmt.training - EPOCH 2107\n",
            "2021-02-01 15:28:22,680 - INFO - joeynmt.training - Epoch 2107: total training loss 8.43\n",
            "2021-02-01 15:28:22,681 - INFO - joeynmt.training - EPOCH 2108\n",
            "2021-02-01 15:28:30,084 - INFO - joeynmt.training - Epoch 2108, Step:    31500, Batch Loss:     0.546841, Tokens per Sec:    17540, Lr: 0.000025\n",
            "2021-02-01 15:28:58,215 - INFO - joeynmt.training - Example #0\n",
            "2021-02-01 15:28:58,216 - INFO - joeynmt.training - \tSource:     Анткени Муса пайгамбардын мыйзамында жазылган бул эрежелер илгертен бери дем алыш сайын ар бир шаардагы синагогаларда окулуп келе жатат.\n",
            "2021-02-01 15:28:58,216 - INFO - joeynmt.training - \tReference:  Себебі атам заманнан бері Мұса (арқылы берілген Таурат заңының осы ережелері) барлық қалаларда уағыздалып, мәжілісханаларда сенбі сайын оқылып келеді.※\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - \tHypothesis: Мұса пайғамбар арқылы берілген Таурат заңында Мұса пайғамбар арқылы берілген ғауы қалаға мәжілісханаларға жіберіп отырды. Олар сол жерде ол мәжілісханаларға батыл үйретіп келді.※\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - Example #1\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - \tSource:     Жүрөгү түшкөн алар Аны арбак экен деп ойлошту.\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - \tReference:  Шәкірттері қатты абыржып қорқып, елес көріп тұрмыз деп ойлады.\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - \tHypothesis: Жындардың біреуі Оны байлап қалып, «Оны құятқа деп ойлап қалды.\n",
            "2021-02-01 15:28:58,217 - INFO - joeynmt.training - Example #2\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tSource:     Анан бүт дүйнөмдү Кудайдын Руху бийлеп алды да периште мени ээн талаага алып барды. Талаадан бүткөн боюн Кудайды сөккөн сөздөр баскан, жети баштуу, он мүйүздүү кыпкызыл жырткычты минип олтурган бир аялды көрдүм.\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tReference:  Сонда мен киелі Рухқа ерекше бөленіп, періште мені айдалаға апарды.※ Сол жерде бір ал қызыл айуанның※ үстінде отырған әйелді көрдім. Айуанның жеті басы мен он мүйізі болды. Оның тұла бойында Құдайды қорлайтын атаулары жазылған екен.\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tHypothesis: Алайда мен бүкіл адамзаттың есінен бас тартып, адам суға мәжбүр еттсын. Сонда әлемді жаратқан Құдай оның қатты ашылып, Құдайды мадақтап тұрған, мұны да Құдайды мадақтап тілейтін әйелдер ! Олар қатты ашылықты болмады.\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - Example #3\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tSource:     Демек, Машаяк менен тирилген болсоңор, анда асманда турганга умтулгула. Ал жерде, Кудайдын оң жагындагы ардактуу орунда Машаяк олтурат.\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tReference:  Сонымен, Мәсіх өлімнен қайта тірілгенде сендер Онымен бірге рухани тірілдіңдер. Сондықтан көкке※ лайықты қасиеттерді иемденуге ұмтылыңдар! Өйткені Мәсіх көкте, Құдайдың оң жағында отыр.\n",
            "2021-02-01 15:28:58,218 - INFO - joeynmt.training - \tHypothesis: Ал Мәсіх Исамен бірге болыңдар: Мәсіх дене көкке көтерме, Оның оң жағындағы жеріміздің Иесіне сай орналасқан кісінің оң жағындағы құрметті орнында отыр.※\n",
            "2021-02-01 15:28:58,219 - INFO - joeynmt.training - Validation result (greedy) at epoch 2108, step    31500: bleu:   5.33, loss: 87556.0781, ppl:  78.0311, duration: 28.1335s\n",
            "2021-02-01 15:29:03,352 - INFO - joeynmt.training - Epoch 2108: total training loss 8.47\n",
            "2021-02-01 15:29:03,353 - INFO - joeynmt.training - EPOCH 2109\n",
            "2021-02-01 15:29:15,886 - INFO - joeynmt.training - Epoch 2109: total training loss 8.52\n",
            "2021-02-01 15:29:15,887 - INFO - joeynmt.training - EPOCH 2110\n",
            "2021-02-01 15:29:28,478 - INFO - joeynmt.training - Epoch 2110: total training loss 8.46\n",
            "2021-02-01 15:29:28,478 - INFO - joeynmt.training - EPOCH 2111\n",
            "2021-02-01 15:29:41,010 - INFO - joeynmt.training - Epoch 2111: total training loss 8.49\n",
            "2021-02-01 15:29:41,011 - INFO - joeynmt.training - EPOCH 2112\n",
            "2021-02-01 15:29:53,618 - INFO - joeynmt.training - Epoch 2112: total training loss 8.50\n",
            "2021-02-01 15:29:53,618 - INFO - joeynmt.training - EPOCH 2113\n",
            "2021-02-01 15:30:05,993 - INFO - joeynmt.training - Epoch 2113: total training loss 8.46\n",
            "2021-02-01 15:30:05,993 - INFO - joeynmt.training - EPOCH 2114\n",
            "2021-02-01 15:30:18,597 - INFO - joeynmt.training - Epoch 2114: total training loss 8.50\n",
            "2021-02-01 15:30:18,597 - INFO - joeynmt.training - EPOCH 2115\n",
            "2021-02-01 15:30:21,949 - INFO - joeynmt.training - Epoch 2115, Step:    31600, Batch Loss:     0.588299, Tokens per Sec:    18672, Lr: 0.000025\n",
            "2021-02-01 15:30:31,045 - INFO - joeynmt.training - Epoch 2115: total training loss 8.47\n",
            "2021-02-01 15:30:31,046 - INFO - joeynmt.training - EPOCH 2116\n",
            "2021-02-01 15:30:43,796 - INFO - joeynmt.training - Epoch 2116: total training loss 8.51\n",
            "2021-02-01 15:30:43,796 - INFO - joeynmt.training - EPOCH 2117\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 15:30:56,261 - INFO - joeynmt.training - Epoch 2117: total training loss 8.47\n",
            "2021-02-01 15:30:56,262 - INFO - joeynmt.training - EPOCH 2118\n",
            "2021-02-01 15:31:08,589 - INFO - joeynmt.training - Epoch 2118: total training loss 7.92\n",
            "2021-02-01 15:31:08,590 - INFO - joeynmt.training - EPOCH 2119\n",
            "2021-02-01 15:31:21,131 - INFO - joeynmt.training - Epoch 2119: total training loss 8.45\n",
            "2021-02-01 15:31:21,131 - INFO - joeynmt.training - EPOCH 2120\n",
            "2021-02-01 15:31:33,594 - INFO - joeynmt.training - Epoch 2120: total training loss 8.47\n",
            "2021-02-01 15:31:33,595 - INFO - joeynmt.training - EPOCH 2121\n",
            "2021-02-01 15:31:45,992 - INFO - joeynmt.training - Epoch 2121, Step:    31700, Batch Loss:     0.556952, Tokens per Sec:    18444, Lr: 0.000025\n",
            "2021-02-01 15:31:45,993 - INFO - joeynmt.training - Epoch 2121: total training loss 8.51\n",
            "2021-02-01 15:31:45,993 - INFO - joeynmt.training - EPOCH 2122\n",
            "2021-02-01 15:31:58,662 - INFO - joeynmt.training - Epoch 2122: total training loss 8.42\n",
            "2021-02-01 15:31:58,663 - INFO - joeynmt.training - EPOCH 2123\n",
            "2021-02-01 15:32:11,200 - INFO - joeynmt.training - Epoch 2123: total training loss 8.47\n",
            "2021-02-01 15:32:11,200 - INFO - joeynmt.training - EPOCH 2124\n",
            "2021-02-01 15:32:23,758 - INFO - joeynmt.training - Epoch 2124: total training loss 8.48\n",
            "2021-02-01 15:32:23,758 - INFO - joeynmt.training - EPOCH 2125\n",
            "2021-02-01 15:32:36,287 - INFO - joeynmt.training - Epoch 2125: total training loss 8.47\n",
            "2021-02-01 15:32:36,287 - INFO - joeynmt.training - EPOCH 2126\n",
            "2021-02-01 15:32:48,936 - INFO - joeynmt.training - Epoch 2126: total training loss 8.39\n",
            "2021-02-01 15:32:48,937 - INFO - joeynmt.training - EPOCH 2127\n",
            "2021-02-01 15:33:01,518 - INFO - joeynmt.training - Epoch 2127: total training loss 8.44\n",
            "2021-02-01 15:33:01,519 - INFO - joeynmt.training - EPOCH 2128\n",
            "2021-02-01 15:33:09,838 - INFO - joeynmt.training - Epoch 2128, Step:    31800, Batch Loss:     0.556223, Tokens per Sec:    17744, Lr: 0.000025\n",
            "2021-02-01 15:33:14,426 - INFO - joeynmt.training - Epoch 2128: total training loss 8.44\n",
            "2021-02-01 15:33:14,427 - INFO - joeynmt.training - EPOCH 2129\n",
            "2021-02-01 15:33:27,280 - INFO - joeynmt.training - Epoch 2129: total training loss 8.46\n",
            "2021-02-01 15:33:27,281 - INFO - joeynmt.training - EPOCH 2130\n",
            "2021-02-01 15:33:40,093 - INFO - joeynmt.training - Epoch 2130: total training loss 8.46\n",
            "2021-02-01 15:33:40,094 - INFO - joeynmt.training - EPOCH 2131\n",
            "2021-02-01 15:33:52,723 - INFO - joeynmt.training - Epoch 2131: total training loss 8.45\n",
            "2021-02-01 15:33:52,724 - INFO - joeynmt.training - EPOCH 2132\n",
            "2021-02-01 15:34:05,315 - INFO - joeynmt.training - Epoch 2132: total training loss 8.50\n",
            "2021-02-01 15:34:05,315 - INFO - joeynmt.training - EPOCH 2133\n",
            "2021-02-01 15:34:18,040 - INFO - joeynmt.training - Epoch 2133: total training loss 8.50\n",
            "2021-02-01 15:34:18,040 - INFO - joeynmt.training - EPOCH 2134\n",
            "2021-02-01 15:34:30,663 - INFO - joeynmt.training - Epoch 2134: total training loss 8.46\n",
            "2021-02-01 15:34:30,664 - INFO - joeynmt.training - EPOCH 2135\n",
            "2021-02-01 15:34:34,850 - INFO - joeynmt.training - Epoch 2135, Step:    31900, Batch Loss:     0.569579, Tokens per Sec:    17949, Lr: 0.000025\n",
            "2021-02-01 15:34:43,001 - INFO - joeynmt.training - Epoch 2135: total training loss 8.49\n",
            "2021-02-01 15:34:43,002 - INFO - joeynmt.training - EPOCH 2136\n",
            "2021-02-01 15:34:55,412 - INFO - joeynmt.training - Epoch 2136: total training loss 8.46\n",
            "2021-02-01 15:34:55,412 - INFO - joeynmt.training - EPOCH 2137\n",
            "2021-02-01 15:35:07,846 - INFO - joeynmt.training - Epoch 2137: total training loss 8.46\n",
            "2021-02-01 15:35:07,846 - INFO - joeynmt.training - EPOCH 2138\n",
            "2021-02-01 15:35:19,973 - INFO - joeynmt.training - Epoch 2138: total training loss 8.45\n",
            "2021-02-01 15:35:19,973 - INFO - joeynmt.training - EPOCH 2139\n",
            "2021-02-01 15:35:32,121 - INFO - joeynmt.training - Epoch 2139: total training loss 8.44\n",
            "2021-02-01 15:35:32,121 - INFO - joeynmt.training - EPOCH 2140\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4194304.0\n",
            "2021-02-01 15:35:44,344 - INFO - joeynmt.training - Epoch 2140: total training loss 8.50\n",
            "2021-02-01 15:35:44,345 - INFO - joeynmt.training - EPOCH 2141\n",
            "2021-02-01 15:35:56,535 - INFO - joeynmt.training - Epoch 2141, Step:    32000, Batch Loss:     0.531398, Tokens per Sec:    18646, Lr: 0.000025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPu6szb5EE8Z"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzewWkThEGKa"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}