{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc3JolYCJnWp"
   },
   "outputs": [],
   "source": [
    "# mount the drive onto here\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# select the source and target language code\n",
    "source_language = \"uz\"\n",
    "target_language = \"ru\" \n",
    "\n",
    "# this is for bilingual\n",
    "experiment_name = \"bilingual_baseline\" \n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = experiment_name\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"/content/drive/My Drive/experiments/$src-$tgt-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/experiments/%s-%s-%s\" % (source_language, target_language, experiment_name)\n",
    "os.environ['data_path'] = \"/content/drive/My Drive/TIL Corpus/bitext\"\n",
    "os.environ['dev_path'] = \"/content/drive/My Drive/TIL Corpus/dev_set\"\n",
    "os.environ['test_bible'] = \"/content/drive/My Drive/TIL Corpus/test_set/bible\"\n",
    "os.environ['test_ted'] = \"/content/drive/My Drive/TIL Corpus/test_set/ted_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icWcAjWpKZJ4",
    "outputId": "dbee9566-15cd-4e9c-8d52-2bf32a968141"
   },
   "outputs": [],
   "source": [
    "# check if the drive link ia good\n",
    "!echo \"$gdrive_path\"\n",
    "!echo \"$data_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the bitext data files for a language pair given above\n",
    "!find \"$data_path\" -name \"$src-$tgt.*\" >> data.txt\n",
    "!find \"$data_path\" -name \"$tgt-$src.*\" >> data.txt\n",
    "\n",
    "# this prints how many files it has discovered\n",
    "! wc -l data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch4ym_fDL5NH"
   },
   "outputs": [],
   "source": [
    "# read the files and merge them\n",
    "paths = open('data.txt', 'r').readlines()\n",
    "source = []\n",
    "target = []\n",
    "for i in range(0, len(paths), 2):\n",
    "  \n",
    "  if paths[i].strip().endswith(source_language):\n",
    "    source += open(paths[i].strip(), 'r').readlines()\n",
    "    target += open(paths[i+1].strip(), 'r').readlines()\n",
    "  else:\n",
    "    source += open(paths[i+1].strip(), 'r').readlines()\n",
    "    target += open(paths[i].strip(), 'r').readlines()\n",
    "\n",
    "assert len(source) == len(target)\n",
    "\n",
    "!rm data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0456OY390bay",
    "outputId": "8affc3aa-6470-42b4-97af-288b4e8dabb1"
   },
   "outputs": [],
   "source": [
    "# load the universal test sets from bible\n",
    "!find \"$test_bible\" -name \"$src-$tgt.*\" >> test_files.txt\n",
    "!find \"$test_bible\" -name \"$tgt-$src.*\" >> test_files.txt\n",
    "! wc -l test_files.txt\n",
    "\n",
    "paths = open('test_files.txt', 'r').readlines()\n",
    "test_source = []\n",
    "test_target = []\n",
    "for i in range(0, len(paths), 2):\n",
    "  \n",
    "  if paths[i].strip().endswith(source_language):\n",
    "    test_source += open(paths[i].strip(), 'r').readlines()\n",
    "    test_target += open(paths[i+1].strip(), 'r').readlines()\n",
    "  else:\n",
    "    test_source += open(paths[i+1].strip(), 'r').readlines()\n",
    "    test_target += open(paths[i].strip(), 'r').readlines()\n",
    "\n",
    "assert len(test_source) == len(test_target)\n",
    "!rm test_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPJFLPXVLWX1",
    "outputId": "51a7956f-3d50-4bef-9866-a9c93a3aa0f5"
   },
   "outputs": [],
   "source": [
    "# load the universal test sets from ted\n",
    "!find \"$test_ted\" -name \"$src-$tgt.*\" >> test_files.txt\n",
    "!find \"$test_ted\" -name \"$tgt-$src.*\" >> test_files.txt\n",
    "! wc -l test_files.txt\n",
    "\n",
    "paths = open('test_files.txt', 'r').readlines()\n",
    "test_source2 = []\n",
    "test_target2 = []\n",
    "for i in range(0, len(paths), 2):\n",
    "  \n",
    "  if paths[i].strip().endswith(source_language):\n",
    "    test_source2 += open(paths[i].strip(), 'r').readlines()\n",
    "    test_target2 += open(paths[i+1].strip(), 'r').readlines()\n",
    "  else:\n",
    "    test_source2 += open(paths[i+1].strip(), 'r').readlines()\n",
    "    test_target2 += open(paths[i].strip(), 'r').readlines()\n",
    "\n",
    "assert len(test_source2) == len(test_target2)\n",
    "!rm test_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4k4tKsW4Vm7c",
    "outputId": "8dac0adb-301d-4235-dbb5-7231c7cde830"
   },
   "outputs": [],
   "source": [
    "# load the universal dev sets\n",
    "!find \"$dev_path\" -name \"dev.$src-$tgt.*\" >> dev_files.txt\n",
    "!find \"$dev_path\" -name \"dev.$tgt-$src.*\" >> dev_files.txt\n",
    "! wc -l dev_files.txt\n",
    "\n",
    "paths = open('dev_files.txt', 'r').readlines()\n",
    "dev_source = []\n",
    "dev_target = []\n",
    "for i in range(0, len(paths), 2):\n",
    "  \n",
    "  if paths[i].strip().endswith(source_language):\n",
    "    dev_source += open(paths[i].strip(), 'r').readlines()\n",
    "    dev_target += open(paths[i+1].strip(), 'r').readlines()\n",
    "  else:\n",
    "    dev_source += open(paths[i+1].strip(), 'r').readlines()\n",
    "    dev_target += open(paths[i].strip(), 'r').readlines()\n",
    "\n",
    "assert len(dev_source) == len(dev_target)\n",
    "\n",
    "!rm dev_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h6gFuVBAvRW"
   },
   "outputs": [],
   "source": [
    "# deduplicate the data\n",
    "clean_source = []\n",
    "clean_target = []\n",
    "\n",
    "merged_test = [(test_source[i], test_target[i]) for i in range(0, len(test_source))] \n",
    "merged_test2 = [(test_source2[i], test_target2[i]) for i in range(0, len(test_source2))] \n",
    "merged_dev = [(dev_source[i], dev_target[i]) for i in range(0, len(dev_source))] \n",
    "\n",
    "for s, t in zip(source, target):\n",
    "  # IMPORTANT for uzbek only. Uncomment otherwise!\n",
    "  _t = t.replace('ѓ', 'gʻ')\n",
    "  _s = s.replace('ѓ', 'gʻ')\n",
    "\n",
    "  # if the sentences is a single letter (plus a newline), then discard it\n",
    "  if len(s) > 2 and len(t) > 2:\n",
    "    if (_s, _t) not in merged_test and (s, t) not in merged_test2 and (s, t) not in merged_dev:\n",
    "        clean_source.append(s)\n",
    "        clean_target.append(t)\n",
    "\n",
    "dev_source = []\n",
    "dev_target = []\n",
    "\n",
    "for (s, t) in merged_dev:\n",
    "  if len(s) > 2 and len(t) > 2:\n",
    "    if (s, t) not in merged_test and (s, t) not in merged_test2:\n",
    "        dev_source.append(s)\n",
    "        dev_target.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsAVK--VA2vo",
    "outputId": "c9dd4352-b41f-4856-d24d-705cf9efcaae"
   },
   "outputs": [],
   "source": [
    "# check the new length of the data (before deduplication)\n",
    "print(f\"Train set: {len(clean_source)} sentences\")\n",
    "print(f\"Dev set: {len(dev_source)} sentences\")\n",
    "print(f\"Test set (bible): {len(test_source)} sentences\")\n",
    "print(f\"Test set (ted talks): {len(test_source2)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0v5wV2KvpDsv",
    "outputId": "cb3d3667-e592-455a-9715-c88a10b73715"
   },
   "outputs": [],
   "source": [
    "# load the data into a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(zip(clean_source, clean_target), columns=['source_sentence', 'target_sentence'])\n",
    "df_dev = pd.DataFrame(zip(dev_source, dev_target), columns=['source_sentence', 'target_sentence'])\n",
    "df_bible = pd.DataFrame(zip(test_source, test_target), columns=['source_sentence', 'target_sentence'])\n",
    "df_ted = pd.DataFrame(zip(test_source2, test_target2), columns=['source_sentence', 'target_sentence'])\n",
    "\n",
    "\n",
    "# drop duplicate translations\n",
    "df = df.drop_duplicates()\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "df_bible = df_bible.drop_duplicates()\n",
    "df_ted = df_ted.drop_duplicates()\n",
    "\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df.sample(frac=1, random_state=99).reset_index(drop=True)\n",
    "print(df_pp.head(3))\n",
    "\n",
    "print(f\"Train set (after dedup): {len(df_pp)} sentences\")\n",
    "print(f\"Dev set: {len(df_dev)} sentences\")\n",
    "print(f\"Test set (bible): {len(df_bible)} sentences\")\n",
    "print(f\"Test set (ted talks): {len(df_ted)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGfvQer44oeY",
    "outputId": "0563e9ad-601d-4757-cf95-8edb17fcd09f"
   },
   "outputs": [],
   "source": [
    "# This section does the split between train/dev/test for the parallel corpora then saves them as separate files\n",
    "import csv\n",
    "\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_pp.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "  \n",
    "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_bible.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "\n",
    "with open(\"test2.\"+source_language, \"w\") as src_file, open(\"test2.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_ted.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "\n",
    "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
    "! head train.*\n",
    "! head dev.*\n",
    "! head test.*\n",
    "! head test2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7O-girJE5mHi",
    "outputId": "f7af5546-69dd-49c8-88c7-511e5568facc"
   },
   "outputs": [],
   "source": [
    "# Install JoeyNMT\n",
    "! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! pip install joeynmt ; cd joeynmt;\n",
    "! pip install torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5i0vjqE7Cnl",
    "outputId": "318c331a-8502-4b51-885d-9c06b74d6203"
   },
   "outputs": [],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "\n",
    "os.environ[\"bpe_size\"] = 32000 # should be 4000 for small datasets (<10,000 samples)\n",
    "\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s $bpe_size -o bpe.codes.$bpe_size --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$src < test2.$src > test2.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.$tgt < test2.$tgt > test2.bpe.$tgt\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp train.* \"$data_path\"\n",
    "! cp test.* \"$data_path\"\n",
    "! cp test2.* \"$data_path\"\n",
    "! cp dev.* \"$data_path\"\n",
    "! cp bpe.codes.$bpe_size \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp test2.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.$bpe_size \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE Test language Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybck4tim8aPz"
   },
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"  # change this to data/{name}/test2.bpe so that you can test it on Ted Talks\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 128\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # sacrebleu options\n",
    "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
    "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.998] \n",
    "    scheduling: \"plateau\"           \n",
    "    patience: 5                     \n",
    "    learning_rate_factor: 0.5       \n",
    "    learning_rate_warmup: 4000     \n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003          \n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 8\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 100                     \n",
    "    validation_freq: 1000          \n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
    "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    fp16: True\n",
    "    max_output_length: 128\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             \n",
    "        embeddings:\n",
    "            embedding_dim: 512   \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         \n",
    "        ff_size: 2048            \n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             \n",
    "        embeddings:\n",
    "            embedding_dim: 512    \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         \n",
    "        ff_size: 2048           \n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlPz-83Neg98",
    "outputId": "156db278-1f1e-4f62-c7d7-a08af570bbd0"
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes to install but it will speed up your training a lot!\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKsFbsiiCAcu",
    "outputId": "45e7a78c-219f-439a-8376-99287c6282b2"
   },
   "outputs": [],
   "source": [
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPu6szb5EE8Z"
   },
   "outputs": [],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzewWkThEGKa",
    "outputId": "7024419c-2b00-4f7e-c30e-31d9bc772d78"
   },
   "outputs": [],
   "source": [
    "# Test our model\n",
    "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "reproduce_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
