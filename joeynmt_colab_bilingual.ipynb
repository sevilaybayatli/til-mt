{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea7rPVLHJlne"
   },
   "outputs": [],
   "source": [
    "# mount the drive onto here\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gsutil\n",
    "\n",
    "# select the source and target language code\n",
    "source_language = \"uz\"\n",
    "target_language = \"tr\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data from the corpus\n",
    "base_url = \"gs://til-corpus/corpus\"\n",
    "train_path = f\"{base_url}/train/{source_language}-{target_language}\"\n",
    "dev_path = f\"{base_url}/dev/{source_language}-{target_language}\"\n",
    "\n",
    "!mkdir -p \"/content/data/train\"\n",
    "!mkdir -p \"/content/data/dev\"\n",
    "\n",
    "# download the train file\n",
    "!gsutil -m cp -r \"$train_path\" \"/content/data/train\"\n",
    "\n",
    "# download the dev file\n",
    "!gsutil -m cp -r \"$dev_path\" \"/content/data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc3JolYCJnWp"
   },
   "outputs": [],
   "source": [
    "# this is for bilingual\n",
    "experiment_name = \"bilingual_baseline\" \n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = experiment_name\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"/content/drive/My Drive/experiments/$src-$tgt-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/experiments/%s-%s-%s\" % (source_language, target_language, experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icWcAjWpKZJ4"
   },
   "outputs": [],
   "source": [
    "# check if the drive link ia good\n",
    "!echo \"$gdrive_path\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch4ym_fDL5NH"
   },
   "outputs": [],
   "source": [
    "source_path = f\"/content/data/train/{source_language}-{target_language}/{source_language}-{target_language}.{source_language}\" \n",
    "target_path = f\"/content/data/train/{source_language}-{target_language}/{source_language}-{target_language}.{target_language}\" \n",
    "\n",
    "source = open(source_path, \"r\").readlines()\n",
    "target = open(target_path, \"r\").readlines()\n",
    "\n",
    "assert len(source) == len(target)\n",
    "\n",
    "print(f\"Found a total of training {len(source)} samples!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0456OY390bay"
   },
   "outputs": [],
   "source": [
    "dev_source_path = f\"/content/data/dev/{source_language}-{target_language}.{source_language}\" \n",
    "dev_target_path = f\"/content/data/dev/{source_language}-{target_language}.{target_language}\" \n",
    "\n",
    "dev_source = open(dev_source_path, \"r\").readlines()\n",
    "dev_target = open(dev_target_path, \"r\").readlines()\n",
    "\n",
    "assert len(dev_source) == len(dev_target)\n",
    "\n",
    "print(f\"Found a total of dev {len(dev_source)} samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsAVK--VA2vo"
   },
   "outputs": [],
   "source": [
    "# check the new length of the data (before deduplication)\n",
    "print(f\"Train set: {len(source)} sentences\")\n",
    "print(f\"Dev set: {len(dev_source)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0v5wV2KvpDsv"
   },
   "outputs": [],
   "source": [
    "# load the data into a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df_dev = pd.DataFrame(zip(dev_source, dev_target), columns=['source_sentence', 'target_sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGfvQer44oeY"
   },
   "outputs": [],
   "source": [
    "# This section does the split between train/dev/test for the parallel corpora then saves them as separate files\n",
    "import csv\n",
    "\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_pp.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in df_dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"])\n",
    "    trg_file.write(row[\"target_sentence\"])\n",
    "  \n",
    "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRuR4Gs3nYgg"
   },
   "outputs": [],
   "source": [
    "! pip install sacremoses\n",
    "\n",
    "source_file = \"train.\" + source_language\n",
    "target_file = \"train.\" + target_language\n",
    "\n",
    "dev_source_file = \"dev.\" + source_language\n",
    "dev_target_file = \"dev.\" + target_language\n",
    "\n",
    "\n",
    "tok_source_file = source_file+\".tok\"\n",
    "tok_target_file = target_file+\".tok\"\n",
    "\n",
    "dev_tok_source_file = dev_source_file+\".tok\"\n",
    "dev_tok_target_file = dev_target_file+\".tok\"\n",
    "\n",
    "# Tokenize the source\n",
    "! sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
    "# Tokenize the target\n",
    "! sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\"\n",
    "\n",
    "# Tokenize the source\n",
    "! sacremoses -l \"$source_language\" tokenize < \"$dev_source_file\" > \"$dev_tok_source_file\"\n",
    "# Tokenize the target\n",
    "! sacremoses -l \"$target_language\" tokenize < \"$dev_target_file\" > \"$dev_tok_target_file\"\n",
    "\n",
    "\n",
    "# Let's take a look what tokenization did to the text.\n",
    "! head \"$source_file\"*\n",
    "! head \"$target_file\"*\n",
    "\n",
    "# Let's take a look what tokenization did to the text.\n",
    "! head \"$dev_source_file\"*\n",
    "! head \"$dev_target_file\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O-girJE5mHi"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/joeynmt/joeynmt.git\n",
    "!cd joeynmt; pip3 install .\n",
    "!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5i0vjqE7Cnl"
   },
   "outputs": [],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src.tok train.$tgt.tok -s 32000 -o bpe.codes.32000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$src < train.$src.tok > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$tgt < train.$tgt.tok > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$src < dev.$src.tok > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$tgt < dev.$tgt.tok > dev.bpe.$tgt\n",
    "\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp train.* \"$data_path\"\n",
    "! cp dev.* \"$data_path\"\n",
    "! cp bpe.codes.32000 \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.32000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE dev language sentences\"\n",
    "! tail -n 5 dev.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybck4tim8aPz"
   },
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 128\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # sacrebleu options\n",
    "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
    "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.998] \n",
    "    scheduling: \"plateau\"           \n",
    "    patience: 7                     \n",
    "    learning_rate_factor: 0.5       \n",
    "    learning_rate_warmup: 4000     \n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003          \n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 4096\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 8\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 3000                     \n",
    "    validation_freq: 500          \n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    fp16: False\n",
    "    max_output_length: 128\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             \n",
    "        embeddings:\n",
    "            embedding_dim: 512   \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         \n",
    "        ff_size: 2048            \n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             \n",
    "        embeddings:\n",
    "            embedding_dim: 512    \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         \n",
    "        ff_size: 2048           \n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlPz-83Neg98"
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes to install but it will speed up your training a lot!\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKsFbsiiCAcu"
   },
   "outputs": [],
   "source": [
    "!cd joeynmt; python -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPu6szb5EE8Z"
   },
   "outputs": [],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "til_bilingual.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
