{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "til_bilingual.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7rPVLHJlne"
      },
      "source": [
        "# mount the drive onto here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc3JolYCJnWp"
      },
      "source": [
        "import os\n",
        "# select the source and target language code\n",
        "source_language = \"uz\"\n",
        "target_language = \"tr\" \n",
        "\n",
        "# this is for bilingual\n",
        "experiment_name = \"bilingual_baseline\" \n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = experiment_name\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/jim/experiments/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/jim/experiments/%s-%s-%s\" % (source_language, target_language, experiment_name)\n",
        "os.environ['data_path'] = \"/content/drive/My Drive/corpus/train\"\n",
        "\n",
        "os.environ['dev_path'] = \"/content/drive/My Drive/corpus/dev\"\n",
        "os.environ['test_bible'] = \"/content/drive/My Drive/corpus/test/bible\"\n",
        "#os.environ['test_ted'] = \"/content/drive/My Drive/corpus/test_set/ted\"\n",
        "#os.environ['test_wmt'] = \"/content/drive/My Drive/corpus/test_set/x-wmt\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icWcAjWpKZJ4"
      },
      "source": [
        "# check if the drive link ia good\n",
        "!echo \"$gdrive_path\"\n",
        "!echo \"$data_path\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4ym_fDL5NH"
      },
      "source": [
        "source_path = f\"{os.environ['data_path']}/{source_language}-{target_language}/{source_language}-{target_language}.{source_language}\" \n",
        "target_path = f\"{os.environ['data_path']}/{source_language}-{target_language}/{source_language}-{target_language}.{target_language}\" \n",
        "\n",
        "source = open(source_path, \"r\").readlines()\n",
        "target = open(target_path, \"r\").readlines()\n",
        "\n",
        "assert len(source) == len(target)\n",
        "\n",
        "print(f\"Found a total of training {len(source)} samples!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0456OY390bay"
      },
      "source": [
        "dev_source_path = f\"{os.environ['dev_path']}/{source_language}-{target_language}/{source_language}-{target_language}.{source_language}\" \n",
        "dev_target_path = f\"{os.environ['dev_path']}/{source_language}-{target_language}/{source_language}-{target_language}.{target_language}\" \n",
        "\n",
        "dev_source = open(dev_source_path, \"r\").readlines()\n",
        "dev_target = open(dev_target_path, \"r\").readlines()\n",
        "\n",
        "assert len(dev_source) == len(dev_target)\n",
        "\n",
        "print(f\"Found a total of dev {len(dev_source)} samples!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJFLPXVLWX1"
      },
      "source": [
        "test_source_path = f\"{os.environ['test_bible']}/{source_language}-{target_language}/{source_language}-{target_language}.{source_language}\" \n",
        "test_target_path = f\"{os.environ['test_bible']}/{source_language}-{target_language}/{source_language}-{target_language}.{target_language}\" \n",
        "\n",
        "test_source = open(test_source_path, \"r\").readlines()\n",
        "test_target = open(test_target_path, \"r\").readlines()\n",
        "\n",
        "assert len(test_source) == len(test_target)\n",
        "\n",
        "print(f\"Found a total of test (Bible) {len(test_source)} samples!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsAVK--VA2vo"
      },
      "source": [
        "# check the new length of the data (before deduplication)\n",
        "print(f\"Train set: {len(source)} sentences\")\n",
        "print(f\"Dev set: {len(dev_source)} sentences\")\n",
        "print(f\"Test set (bible): {len(test_source)} sentences\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v5wV2KvpDsv"
      },
      "source": [
        "# load the data into a pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "df_dev = pd.DataFrame(zip(dev_source, dev_target), columns=['source_sentence', 'target_sentence'])\n",
        "df_bible = pd.DataFrame(zip(test_source, test_target), columns=['source_sentence', 'target_sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfvQer44oeY"
      },
      "source": [
        "# This section does the split between train/dev/test for the parallel corpora then saves them as separate files\n",
        "import csv\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_pp.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "  \n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in df_bible.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"])\n",
        "    trg_file.write(row[\"target_sentence\"])\n",
        "\n",
        "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRuR4Gs3nYgg"
      },
      "source": [
        "! pip install sacremoses\n",
        "\n",
        "source_file = \"train.\" + source_language\n",
        "target_file = \"train.\" + target_language\n",
        "\n",
        "dev_source_file = \"dev.\" + source_language\n",
        "dev_target_file = \"dev.\" + target_language\n",
        "\n",
        "test_source_file = \"test.\" + source_language\n",
        "test_target_file = \"test.\" + target_language\n",
        "\n",
        "tok_source_file = source_file+\".tok\"\n",
        "tok_target_file = target_file+\".tok\"\n",
        "\n",
        "dev_tok_source_file = dev_source_file+\".tok\"\n",
        "dev_tok_target_file = dev_target_file+\".tok\"\n",
        "\n",
        "test_tok_source_file = test_source_file+\".tok\"\n",
        "test_tok_target_file = test_target_file+\".tok\"\n",
        "\n",
        "# Tokenize the source\n",
        "! sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
        "# Tokenize the target\n",
        "! sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\"\n",
        "\n",
        "# Tokenize the source\n",
        "! sacremoses -l \"$source_language\" tokenize < \"$dev_source_file\" > \"$dev_tok_source_file\"\n",
        "# Tokenize the target\n",
        "! sacremoses -l \"$target_language\" tokenize < \"$dev_target_file\" > \"$dev_tok_target_file\"\n",
        "\n",
        "# Tokenize the source\n",
        "! sacremoses -l \"$source_language\" tokenize < \"$test_source_file\" > \"$test_tok_source_file\"\n",
        "# Tokenize the target\n",
        "! sacremoses -l \"$target_language\" tokenize < \"$test_target_file\" > \"$test_tok_target_file\"\n",
        "\n",
        "\n",
        "\n",
        "# Let's take a look what tokenization did to the text.\n",
        "! head \"$source_file\"*\n",
        "! head \"$target_file\"*\n",
        "\n",
        "# Let's take a look what tokenization did to the text.\n",
        "! head \"$dev_source_file\"*\n",
        "! head \"$dev_target_file\"*\n",
        "\n",
        "# Let's take a look what tokenization did to the text.\n",
        "! head \"$test_source_file\"*\n",
        "! head \"$test_target_file\"*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-girJE5mHi"
      },
      "source": [
        "!git clone https://github.com/joeynmt/joeynmt.git\n",
        "!cd joeynmt; pip3 install .\n",
        "!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5i0vjqE7Cnl"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src.tok train.$tgt.tok -s 32000 -o bpe.codes.32000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$src < train.$src.tok > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$tgt < train.$tgt.tok > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$src < dev.$src.tok > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$tgt < dev.$tgt.tok > dev.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$src < test.$src.tok > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.32000 --vocabulary vocab.$tgt < test.$tgt.tok > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p \"$data_path\"\n",
        "! cp train.* \"$data_path\"\n",
        "! cp test.* \"$data_path\"\n",
        "! cp dev.* \"$data_path\"\n",
        "! cp bpe.codes.32000 \"$data_path\"\n",
        "! ls \"$data_path\"\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.32000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Test language Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybck4tim8aPz"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"  # change this to data/{name}/test2.bpe so that you can test it on Ted Talks\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 128\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.998] \n",
        "    scheduling: \"plateau\"           \n",
        "    patience: 7                     \n",
        "    learning_rate_factor: 0.5       \n",
        "    learning_rate_warmup: 4000     \n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003          \n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 4096\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 8\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 3000                     \n",
        "    validation_freq: 500          \n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
        "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    fp16: False\n",
        "    max_output_length: 128\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8             \n",
        "        embeddings:\n",
        "            embedding_dim: 512   \n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512         \n",
        "        ff_size: 2048            \n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8             \n",
        "        embeddings:\n",
        "            embedding_dim: 512    \n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512         \n",
        "        ff_size: 2048           \n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlPz-83Neg98"
      },
      "source": [
        "# This may take a few minutes to install but it will speed up your training a lot!\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKsFbsiiCAcu"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt train configs/transformer_$src$tgt.yaml\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPu6szb5EE8Z"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzewWkThEGKa"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}